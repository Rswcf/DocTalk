{"factual_nvidia_01": [{"chunk_id": "ddc05d6e-79f6-4ed4-a29b-876d9e76fd48", "text": "We did not experience any significant impact or expense toour business; however, if the conflict is further extended, it could impact future product development, operations, and revenue or create other uncertainty for ourbusiness.35Fiscal Year 2024 SummaryYear EndedJan 28, 2024Jan 29, 2023Change($ in millions, except per share data)Revenue$60,922$26,974Up 126%Gross margin72.7 %56.9 %Up 15.8 ptsOperating expenses$11,329$11,132Up 2%Operating income$32,972$4,224Up 681%Net income$29,760$4,368Up 581%Net income per diluted share$11.93$1.74Up 586%We specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Revenue for fiscal year 2024 was $60.9 billion, up 126% from a year ago.Data Center revenue for fiscal year 2024 was up 217%.Strong demand was driven by enterprise software and consumer internet applications, and multipleindustry verticals including automotive, financial services, and healthcare.Customers across industry verticals access NVIDIA AI infrastructure both through thecloud and on-premises.Data Center compute revenue was up 244% in the fiscal year.Networking revenue was up 133% in the fiscal year.Gaming revenue for fiscal year 2024 was up 15%.The increase reflects higher sell-in to partners following the normalization of channel inventory levels andgrowing demand.Professional Visualization revenue for fiscal year 2024 was up 1%.Automotive revenue for the fiscal year 2024 was up 21%.The increase primarily reflected growth in self-driving platforms.Gross margin increased in fiscal year 2024, primarily driven by Data Center revenue growth and lower net inventory provisions as a percentage of revenue.Operating expenses increased for fiscal year 2024, driven by growth in employees and compensation increases.Fiscal year 2023 also included a $1.4 billionacquisition termination charge related to the proposed Arm transaction.Market Platform HighlightsData Center revenue for fiscal year 2024 was $47.5 billion, up 217% from fiscal year 2023.In Data Center, we launched AI inference platforms that combine ourfull-stack inference software with NVIDIA Ada, NVIDIA Hopper and NVIDIA Grace Hopper processors optimized for generative AI, LLMs and other AI workloads.We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.", "page_start": 35, "page_end": 36, "section_title": "NVIDIA CORPORATION", "score": 0.7429445}, {"chunk_id": "823438dd-9ce8-4ae9-924a-70c6bbf5121f", "text": "Form 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38The following table sets forth, for the periods indicated, certain items in our Consolidated Statements of Income expressed as a percentage of revenue.Year EndedJan 28, 2024Jan 29, 2023Revenue100.0 %100.0 %Cost of revenue27.343.1Gross profit72.756.9Operating expensesResearch and development14.227.2Sales, general and administrative4.49.1Acquisition termination cost—5.0Total operating expenses18.641.3Operating income54.115.6Interest income1.41.0Interest expense(0.4)(1.0)0.4(0.1)Other, net1.4(0.1)Other income (expense), netIncome before income tax55.515.5Income tax expense (benefit)6.6(0.7)48.9 %16.2 %Net incomeReportable SegmentsRevenue by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$47,405$15,068$32,337215 %13,51711,9061,611Graphics14 %$60,922$26,974$33,948Total126 %Operating Income by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$32,016$5,083$26,933530 %Graphics5,8464,5521,29428 %(4,890)(5,411)521All Other(10)%$32,972$4,224$28,748Total681 %Compute & Networking revenue – The year-on-year increase was due to higher Data Center revenue.Compute grew 266% due to higher shipments of theNVIDIA Hopper GPU computing platform for the training and inference of LLMs, recommendation engines and generative AI applications.Networking was up133% due to higher shipments of InfiniBand.Graphics revenue – The year-on-year increase was led by growth in Gaming of 15% driven by higher sell-in to partners following the normalization of channelinventory levels.Reportable segment operating income – The year-on-year increase in Compute & Networking and Graphics operating income was driven by higher revenue.39All Other operating loss - The year-on-year decrease was due to the $1.4 billion Arm acquisition termination cost in fiscal year 2023, partially offset by a $839million increase in stock-based compensation expense in fiscal year 2024.Concentration of Revenue", "page_start": 38, "page_end": 40, "section_title": "NVIDIA CORPORATION", "score": 0.73281425}, {"chunk_id": "8d7f8306-8d55-40f5-b383-f4914775e7ee", "text": "(10)Restructuring costs and other—(54)—Acquisition termination cost—(1,353)—10(2)—Other$(4,890)$(5,411)$(3,049)TotalRevenue by geographic areas is designated based upon the billing location of the customer.End customer location may be different than our customer’s billinglocation.Revenue by geographic areas was as follows:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Revenue:(In millions)United States$26,966$8,292$4,349Taiwan13,4056,9868,544China (including Hong Kong)10,3065,7857,11110,2455,9116,910Other countries$60,922$26,974$26,914Total revenueRevenue from sales to customers outside of the United States accounted for 56%, 69%, and 84% of total revenue for fiscal years 2024, 2023, and 2022,respectively.The increase in revenue to the United States for fiscal year 2024 was primarily due to higher U.S.-based Compute & Networking segment demand.Sales to one customer represented 13% of total revenue for fiscal year 2024, which was attributable to the Compute & Networking segment.No customerrepresented 10% or more of total revenue for fiscal years 2023 and 2022.The following table summarizes information pertaining to our revenue by each of the specialized markets we serve:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Revenue:(In millions)Data Center$47,525$15,005$10,613Gaming10,4479,06712,462Professional Visualization1,5531,5442,111Automotive1,0919035663064551,162OEM and Other$60,922$26,974$26,914Total revenue79NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)The following table presents summarized information for long-lived assets by country.Long-lived assets consist of property and equipment and exclude otherassets, operating lease assets, goodwill, and intangible assets.Jan 28, 2024Jan 29, 2023Long-lived assets:(In millions)United States$2,595$2,587Taiwan773702Israel325283Other countries221235$3,914$3,807Total long-lived assets80NVIDIA Corporation and SubsidiariesSchedule II – Valuation and Qualifying AccountsBalance atBeginning ofBalance atDescriptionPeriodAdditionsDeductionsEnd of Period(In millions)Fiscal year 2024Allowance for doubtful accounts$4$— (1)$— (1)$4Sales return allowance$26$213 (2)$", "page_start": 79, "page_end": 81, "section_title": "NVIDIA CORPORATION", "score": 0.68944114}, {"chunk_id": "f2207e14-4c6c-421b-849b-76359737cca1", "text": "Germany, India, Israel, and Taiwan for fiscal years 2005 through 2023.Note 15 - Shareholders’ EquityCapital Return ProgramIn August 2023, our Board of Directors approved an increase to our share repurchase program of an additional $25.0 billion, without expiration.During fiscalyear 2024, we repurchased 21 million shares of our common stock for $9.7 billion.As of January 28, 2024, we were authorized, subject to certain specifications,to repurchase additional shares of our common stock up to $22.5 billion.From January 29, 2024 through February 16, 2024, we repurchased 2.8 million sharesfor $1.9 billion pursuant to a Rule 10b5-1 trading plan.Our share repurchase program aims to offset dilution from shares issued to employees.We may pursueadditional share repurchases as we weigh market factors and other investment opportunities.During fiscal years 2024, 2023, and 2022, we paid $395 million, $398 million, and $399 million in cash dividends to our shareholders, respectively.Our cashdividend program and the payment of future cash dividends under that program are subject to our Board of Directors' continuing determination that the dividendprogram and the declaration of dividends thereunder are in the best interests of our shareholders.In fiscal year 2022, we retired our existing 349 million treasury shares.These shares assumed the status of authorized and unissued shares upon retirement.The excess of repurchase price over par value was allocated between additional paid-in capital and retained earnings, resulting in a reduction in additional paidin capital by $20 million and retained earnings by $12.0 billion.Any future repurchased shares will assume the status of authorized and unissued shares.77NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Note 16 - Employee Retirement PlansWe provide tax-qualified defined contribution plans to eligible employees in the U.S.and certain other countries.Our contribution expense for fiscal years 2024,2023, and 2022 was $255 million, $227 million, and $168 million, respectively.Note 17 - Segment InformationOur Chief Executive Officer, who is considered to be our chief operating decision maker, or CODM, reviews financial information presented on an operatingsegment basis for purposes of making decisions and assessing financial performance.The Compute & Networking segment includes our Data Center accelerated computing platform; networking; automotive artificial intelligence, or AI, Cockpit,autonomous driving development agreements, and autonomous vehicle solutions; electric vehicle computing platforms; Jetson for robotics and other embeddedplatforms; NVIDIA AI Enterprise and other software; and DGX Cloud.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions forgaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across our", "page_start": 77, "page_end": 78, "section_title": "NVIDIA CORPORATION", "score": 0.68072414}, {"chunk_id": "c69190e8-be56-40d7-9e3f-a1b1a8224354", "text": "Estimates for customer program accrualsinclude a combination of historical attainment and claim rates and may be adjusted based on relevant internal and external factors.License and Development ArrangementsRevenue from License and Development Arrangements is recognized over the period in which the development services are performed.Each fiscal reportingperiod, we measure progress to completion based on actual cost incurred to date as a percentage of the estimated total cost required to complete each project.Estimated total cost for each project includes a forecast of internal engineer personnel time expected to be incurred and other third-party costs as applicable.Contracts with Multiple Performance ObligationsOur contracts may contain more than one performance obligation.Judgement is required in determining whether each performance obligation within a customercontract is distinct.Except for License and Development Arrangements, NVIDIA products and services function on a standalone basis and do not require asignificant amount of integration or interdependency.Therefore, multiple performance obligations contained within a customer contract are considered distinctand are not combined for revenue recognition purposes.We allocate the total transaction price to each distinct performance obligation in a multiple performance obligations arrangement on a relative standalone sellingprice basis.In certain cases, we can establish standalone selling price based on directly observable prices of products or services sold separately in comparablecircumstances to similar customers.If standalone selling price is not directly observable, such as when we do not sell a product or service separately, wedetermine standalone selling price based on market data and other observable inputs.Change in Accounting EstimateIn February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of a majority of the server, storage, and network equipment from three years to a range of four to five years, and assembly and testequipment from five years to seven years.The estimated effect of this change for fiscal year 2024 was a benefit of $33 million and $102 million for cost ofrevenue and operating expenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or$0.05 per both basic and diluted share.Results of OperationsA discussion regarding our financial condition and results of operations for fiscal year 2024 compared to fiscal year 2023 is presented below.A discussionregarding our financial condition and results of operations for fiscal year 2023 compared to fiscal year 2022 can be found under Item 7 in our Annual Report onForm 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38", "page_start": 38, "page_end": 38, "section_title": "NVIDIA CORPORATION", "score": 0.6799737}, {"chunk_id": "16846335-f2af-4650-bf60-d4b5485698d4", "text": "Principles of ConsolidationOur consolidated financial statements include the accounts of NVIDIA Corporation and our wholly-owned subsidiaries.All intercompany balances andtransactions have been eliminated in consolidation.Use of EstimatesThe preparation of financial statements in conformity with U.S.GAAP requires management to make estimates and assumptions that affect the reportedamounts of assets and liabilities and disclosures of contingent assets and liabilities at the date of the financial statements and the reported amounts of revenueand expenses during the reporting period.Actual results could differ materially from our estimates.On an on-going basis, we evaluate our estimates, includingthose related to revenue recognition, cash equivalents and marketable securities, accounts receivable, inventories and product purchase commitments, incometaxes, goodwill, stock-based compensation, litigation, investigation and settlement costs, restructuring and other charges, property, plant, and equipment, andother contingencies.These estimates are based on historical facts and various other assumptions that we believe are reasonable.In February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of most of our server, storage, and network equipment from three to four or five years, and our assembly and test equipment from five toseven years.The effect of this change for the fiscal year ended January 28, 2024 was a benefit of $33 million and $102 million for cost of revenue and operatingexpenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or $0.05 per both basic anddiluted share.Revenue RecognitionWe derive our revenue from product sales, including hardware and systems, license and development arrangements, software licensing, and cloud services.Wedetermine revenue recognition through the following steps: (1) identification of the contract with a customer; (2) identification of the performance obligations inthe contract; (3) determination of the transaction price; (4) allocation of the transaction price to the performance obligations in the contract (where revenue isallocated on a relative standalone selling price basis by maximizing the use of observable inputs to determine the standalone selling price for each performanceobligation); and (5) recognition of revenue when, or as, we satisfy a performance obligation.Product Sales RevenueRevenue from product sales is recognized upon transfer of control of products to customers in an amount that reflects the consideration we expect to receive inexchange for those products.Certain products are sold with support or an extended warranty for the incorporated system, hardware, and/or software.Supportand extended warranty revenue are recognized ratably over the service period, or as services are performed.Revenue is recognized net of allowances forreturns, customer programs and any taxes collected from customers.For products sold with a right of return, we record a reduction to revenue by establishing a sales return allowance for estimated product returns at the timerevenue is recognized, based primarily on historical return rates.", "page_start": 55, "page_end": 55, "section_title": "NVIDIA CORPORATION", "score": 0.6742786}, {"chunk_id": "0dd7fcd2-abcb-4469-bfdc-d6d5c6cb8eae", "text": "*$100 invested on 1/27/19 in stock and in indices, including reinvestment of dividends.Source: FactSet financial data and analytics.1/27/20191/26/20201/31/20211/30/20221/29/20231/28/2024NVIDIA Corporation$100.00$157.02$326.26$574.15$512.40$1,536.28S&P 500$100.00$126.17$144.83$175.25$163.63$199.83Nasdaq 100$100.00$136.15$194.20$218.68$185.67$268.13Item 6.[Reserved]33Item 7.Management's Discussion and Analysis of Financial Condition and Results of OperationsThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with “Item 1A.Risk Factors”, ourConsolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described elsewhere in this Annual Report onForm 10-K, before deciding to purchase, hold or sell shares of our common stock.OverviewOur Company and Our BusinessesNVIDIA pioneered accelerated computing to help solve the most challenging computational problems.Since our original focus on PC graphics, we haveexpanded to several other large and important computationally intensive fields.NVIDIA has leveraged its GPU architecture to create platforms for acceleratedcomputing, AI solutions, scientific computing, data science, AV, robotics, metaverse and 3D internet applications.Our two operating segments are \"Compute & Networking\" and \"Graphics.\" Refer to Note 17 of the Notes to the Consolidated Financial Statements in Part IV,Item 15 of this Annual Report on Form 10-K for additional information.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Recent Developments, Future Objectives and ChallengesDemand and Supply, Product Transitions, and New Products and Business ModelsDemand for our data center systems and products surged in fiscal year 2024.Entering fiscal year 2025, we are gathering customer demand indications acrossseveral product transitions.We have demand visibility for our new data center products ramping later in fiscal year 2025.We have increased our supply andcapacity purchases with existing suppliers, added new vendors and entered into prepaid manufacturing and capacity agreements.These increased purchasevolumes, the number of suppliers, and the integration of new vendors into our supply chain may create more complexity and execution risk.Our purchasecommitments and obligations for inventory and manufacturing capacity at the end of fiscal year 2024 were impacted by shortening lead times for certaincomponents.We may continue to enter into new supplier and capacity arrangements.Supply of Hopper architecture products is improving, and demand remainsvery strong.We expect our next-generation products to be supply-constrained based upon demand indications.", "page_start": 33, "page_end": 34, "section_title": "NVIDIA CORPORATION", "score": 0.66801167}, {"chunk_id": "4f638c9d-79d6-4b84-9677-86448558b897", "text": "44,30115,35617,475Operating expensesResearch and development8,6757,3395,268Sales, general and administrative2,6542,4402,166—1,353—Acquisition termination cost11,32911,1327,434Total operating expensesOperating income32,9724,22410,041Interest income86626729Interest expense(257)(262)(236)237(48)107Other, net846(43)(100)Other income (expense), netIncome before income tax33,8184,1819,941Income tax expense (benefit)4,058(187)189$29,760$4,368$9,752Net incomeNet income per share:$12.05$1.76$3.91Basic$11.93$1.74$3.85DilutedWeighted average shares used in per share computation:2,4692,4872,496Basic2,4942,5072,535DilutedSee accompanying notes to the consolidated financial statements.50NVIDIA Corporation and SubsidiariesConsolidated Statements of Comprehensive Income(In millions)Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Net income$29,760$4,368$9,752Other comprehensive income (loss), net of taxAvailable-for-sale securities:Net change in unrealized gain (loss)80(31)(16)Reclassification adjustments for net realized gain included in net income—1—80(30)(16)Net change in unrealized gain (loss)Cash flow hedges:Net change in unrealized gain (loss)3847(43)(48)(49)29Reclassification adjustments for net realized gain (loss) included in net income(10)(2)(14)Net change in unrealized loss70(32)(30)Other comprehensive income (loss), net of tax$29,830$4,336$9,722Total comprehensive incomeSee accompanying notes to the consolidated financial statements.51NVIDIA Corporation and SubsidiariesConsolidated Balance Sheets(In millions, except par value)Jan 28, 2024Jan 29, 2023AssetsCurrent assets:Cash and cash equivalents$7,280$3,389Marketable securities18,7049,907Accounts receivable, net9,9993,827Inventories5,2825,1593,080791Prepaid expenses and other current assetsTotal current assets44,34523,073Property and equipment, net3,9143,807Operating lease assets1,3461,038Goodwill4,4304,372Intangible assets, net1,1121,676Deferred income tax assets6,0813,3964,5003,820Other assets$65,728$41,182Total assetsLiabilities and Shareholders' EquityCurrent liabilities:Accounts payable$2,699$1,193Accrued and other current liabilities6,682", "page_start": 50, "page_end": 52, "section_title": "NVIDIA CORPORATION", "score": 0.65318716}], "factual_nvidia_02": [{"chunk_id": "4d665fa7-dd83-464c-908e-2634e7e09406", "text": "10or generate enough renewable energy to match 100% of our global electricity usage for our offices and data centers.In fiscal year 2023, we increased thepercentage of our total electricity use matched by renewable energy purchases to 44%.By fiscal year 2026, we aim to engage manufacturing supplierscomprising at least 67% of NVIDIA’s scope 3 category 1 GHG emissions with goal of effecting supplier adoption of science-based targets.Whether it is creation of technology to power next-generation laptops or designs to support high-performance supercomputers, improving energy efficiency isimportant in our research, development, and design processes.GPU-accelerated computing is inherently more energy efficient than traditional computing formany workloads because it is optimized for throughput, performance per watt, and certain AI workloads.The energy efficiency of our products is evidenced byour continued strong presence on the Green500 list of the most energy-efficient systems.We powered 24 of the top 30 most energy efficient systems, includingthe top supercomputer, on the Green500 list.We plan to build Earth-2, a digital twin of the Earth on NVIDIA AI and NVIDIA Omniverse platforms.Earth-2 will enable scientists, companies, and policy makersto do ultra-high-resolution predictions of the impact of climate change and explore mitigation and adaptation strategies.Human Capital ManagementWe believe that our employees are our greatest assets, and they play a key role in creating long-term value for our stakeholders.As of the end of fiscal year2024, we had approximately 29,600 employees in 36 countries, 22,200 were engaged in research and development and 7,400 were engaged in sales,marketing, operations, and administrative positions.The Compensation Committee of our Board of Directors assists in the oversight of policies and strategiesrelating to human capital management.To be competitive and execute our business strategy successfully, we must recruit, develop, and retain talented employees, including qualified executives,scientists, engineers, and technical and non-technical staff.RecruitmentAs the demand for global technical talent continues to be competitive, we have grown our technical workforce and have been successful in attracting top talentto NVIDIA.We have attracted talent globally through our strong employer brand and differentiated hiring strategies for college, professional, and leadershiptalent.Our workforce is 83% technical and 49% hold advanced degrees.Additionally, we have increased focus on diversity recruiting, resulting in an increase inglobal female hiring in each channel.Our own employees help to surface top talent, with over 40% of our new hires in fiscal year 2024 coming from employeereferrals.Development and RetentionTo support employee development, we provide opportunities to learn on-the-job through training courses, targeted development programs, mentoring and peercoaching and ongoing feedback.We have a library of live and on-demand learning experiences that include workshops, panel discussions, and speaker forums.We create learning paths focused on our most common development needs and constantly upgrade our offerings to ensure that our employees are exposed tothe most current content and technologies available.", "page_start": 10, "page_end": 11, "section_title": "NVIDIA CORPORATION", "score": 0.6954882}, {"chunk_id": "0dd7fcd2-abcb-4469-bfdc-d6d5c6cb8eae", "text": "*$100 invested on 1/27/19 in stock and in indices, including reinvestment of dividends.Source: FactSet financial data and analytics.1/27/20191/26/20201/31/20211/30/20221/29/20231/28/2024NVIDIA Corporation$100.00$157.02$326.26$574.15$512.40$1,536.28S&P 500$100.00$126.17$144.83$175.25$163.63$199.83Nasdaq 100$100.00$136.15$194.20$218.68$185.67$268.13Item 6.[Reserved]33Item 7.Management's Discussion and Analysis of Financial Condition and Results of OperationsThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with “Item 1A.Risk Factors”, ourConsolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described elsewhere in this Annual Report onForm 10-K, before deciding to purchase, hold or sell shares of our common stock.OverviewOur Company and Our BusinessesNVIDIA pioneered accelerated computing to help solve the most challenging computational problems.Since our original focus on PC graphics, we haveexpanded to several other large and important computationally intensive fields.NVIDIA has leveraged its GPU architecture to create platforms for acceleratedcomputing, AI solutions, scientific computing, data science, AV, robotics, metaverse and 3D internet applications.Our two operating segments are \"Compute & Networking\" and \"Graphics.\" Refer to Note 17 of the Notes to the Consolidated Financial Statements in Part IV,Item 15 of this Annual Report on Form 10-K for additional information.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Recent Developments, Future Objectives and ChallengesDemand and Supply, Product Transitions, and New Products and Business ModelsDemand for our data center systems and products surged in fiscal year 2024.Entering fiscal year 2025, we are gathering customer demand indications acrossseveral product transitions.We have demand visibility for our new data center products ramping later in fiscal year 2025.We have increased our supply andcapacity purchases with existing suppliers, added new vendors and entered into prepaid manufacturing and capacity agreements.These increased purchasevolumes, the number of suppliers, and the integration of new vendors into our supply chain may create more complexity and execution risk.Our purchasecommitments and obligations for inventory and manufacturing capacity at the end of fiscal year 2024 were impacted by shortening lead times for certaincomponents.We may continue to enter into new supplier and capacity arrangements.Supply of Hopper architecture products is improving, and demand remainsvery strong.We expect our next-generation products to be supply-constrained based upon demand indications.", "page_start": 33, "page_end": 34, "section_title": "NVIDIA CORPORATION", "score": 0.66407466}, {"chunk_id": "f2207e14-4c6c-421b-849b-76359737cca1", "text": "Germany, India, Israel, and Taiwan for fiscal years 2005 through 2023.Note 15 - Shareholders’ EquityCapital Return ProgramIn August 2023, our Board of Directors approved an increase to our share repurchase program of an additional $25.0 billion, without expiration.During fiscalyear 2024, we repurchased 21 million shares of our common stock for $9.7 billion.As of January 28, 2024, we were authorized, subject to certain specifications,to repurchase additional shares of our common stock up to $22.5 billion.From January 29, 2024 through February 16, 2024, we repurchased 2.8 million sharesfor $1.9 billion pursuant to a Rule 10b5-1 trading plan.Our share repurchase program aims to offset dilution from shares issued to employees.We may pursueadditional share repurchases as we weigh market factors and other investment opportunities.During fiscal years 2024, 2023, and 2022, we paid $395 million, $398 million, and $399 million in cash dividends to our shareholders, respectively.Our cashdividend program and the payment of future cash dividends under that program are subject to our Board of Directors' continuing determination that the dividendprogram and the declaration of dividends thereunder are in the best interests of our shareholders.In fiscal year 2022, we retired our existing 349 million treasury shares.These shares assumed the status of authorized and unissued shares upon retirement.The excess of repurchase price over par value was allocated between additional paid-in capital and retained earnings, resulting in a reduction in additional paidin capital by $20 million and retained earnings by $12.0 billion.Any future repurchased shares will assume the status of authorized and unissued shares.77NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Note 16 - Employee Retirement PlansWe provide tax-qualified defined contribution plans to eligible employees in the U.S.and certain other countries.Our contribution expense for fiscal years 2024,2023, and 2022 was $255 million, $227 million, and $168 million, respectively.Note 17 - Segment InformationOur Chief Executive Officer, who is considered to be our chief operating decision maker, or CODM, reviews financial information presented on an operatingsegment basis for purposes of making decisions and assessing financial performance.The Compute & Networking segment includes our Data Center accelerated computing platform; networking; automotive artificial intelligence, or AI, Cockpit,autonomous driving development agreements, and autonomous vehicle solutions; electric vehicle computing platforms; Jetson for robotics and other embeddedplatforms; NVIDIA AI Enterprise and other software; and DGX Cloud.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions forgaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across our", "page_start": 77, "page_end": 78, "section_title": "NVIDIA CORPORATION", "score": 0.6637027}, {"chunk_id": "823438dd-9ce8-4ae9-924a-70c6bbf5121f", "text": "Form 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38The following table sets forth, for the periods indicated, certain items in our Consolidated Statements of Income expressed as a percentage of revenue.Year EndedJan 28, 2024Jan 29, 2023Revenue100.0 %100.0 %Cost of revenue27.343.1Gross profit72.756.9Operating expensesResearch and development14.227.2Sales, general and administrative4.49.1Acquisition termination cost—5.0Total operating expenses18.641.3Operating income54.115.6Interest income1.41.0Interest expense(0.4)(1.0)0.4(0.1)Other, net1.4(0.1)Other income (expense), netIncome before income tax55.515.5Income tax expense (benefit)6.6(0.7)48.9 %16.2 %Net incomeReportable SegmentsRevenue by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$47,405$15,068$32,337215 %13,51711,9061,611Graphics14 %$60,922$26,974$33,948Total126 %Operating Income by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$32,016$5,083$26,933530 %Graphics5,8464,5521,29428 %(4,890)(5,411)521All Other(10)%$32,972$4,224$28,748Total681 %Compute & Networking revenue – The year-on-year increase was due to higher Data Center revenue.Compute grew 266% due to higher shipments of theNVIDIA Hopper GPU computing platform for the training and inference of LLMs, recommendation engines and generative AI applications.Networking was up133% due to higher shipments of InfiniBand.Graphics revenue – The year-on-year increase was led by growth in Gaming of 15% driven by higher sell-in to partners following the normalization of channelinventory levels.Reportable segment operating income – The year-on-year increase in Compute & Networking and Graphics operating income was driven by higher revenue.39All Other operating loss - The year-on-year decrease was due to the $1.4 billion Arm acquisition termination cost in fiscal year 2023, partially offset by a $839million increase in stock-based compensation expense in fiscal year 2024.Concentration of Revenue", "page_start": 38, "page_end": 40, "section_title": "NVIDIA CORPORATION", "score": 0.6611511}, {"chunk_id": "ddc05d6e-79f6-4ed4-a29b-876d9e76fd48", "text": "We did not experience any significant impact or expense toour business; however, if the conflict is further extended, it could impact future product development, operations, and revenue or create other uncertainty for ourbusiness.35Fiscal Year 2024 SummaryYear EndedJan 28, 2024Jan 29, 2023Change($ in millions, except per share data)Revenue$60,922$26,974Up 126%Gross margin72.7 %56.9 %Up 15.8 ptsOperating expenses$11,329$11,132Up 2%Operating income$32,972$4,224Up 681%Net income$29,760$4,368Up 581%Net income per diluted share$11.93$1.74Up 586%We specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Revenue for fiscal year 2024 was $60.9 billion, up 126% from a year ago.Data Center revenue for fiscal year 2024 was up 217%.Strong demand was driven by enterprise software and consumer internet applications, and multipleindustry verticals including automotive, financial services, and healthcare.Customers across industry verticals access NVIDIA AI infrastructure both through thecloud and on-premises.Data Center compute revenue was up 244% in the fiscal year.Networking revenue was up 133% in the fiscal year.Gaming revenue for fiscal year 2024 was up 15%.The increase reflects higher sell-in to partners following the normalization of channel inventory levels andgrowing demand.Professional Visualization revenue for fiscal year 2024 was up 1%.Automotive revenue for the fiscal year 2024 was up 21%.The increase primarily reflected growth in self-driving platforms.Gross margin increased in fiscal year 2024, primarily driven by Data Center revenue growth and lower net inventory provisions as a percentage of revenue.Operating expenses increased for fiscal year 2024, driven by growth in employees and compensation increases.Fiscal year 2023 also included a $1.4 billionacquisition termination charge related to the proposed Arm transaction.Market Platform HighlightsData Center revenue for fiscal year 2024 was $47.5 billion, up 217% from fiscal year 2023.In Data Center, we launched AI inference platforms that combine ourfull-stack inference software with NVIDIA Ada, NVIDIA Hopper and NVIDIA Grace Hopper processors optimized for generative AI, LLMs and other AI workloads.We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.", "page_start": 35, "page_end": 36, "section_title": "NVIDIA CORPORATION", "score": 0.65786386}, {"chunk_id": "f48f39e2-3a78-42d2-b292-53056dc3d79a", "text": "When recruiting for new talent or developing our current employees, we strive to build a diverse talent pipeline that includes those underrepresented in thetechnology field, including women, Black/African American, and Hispanic/Latino candidates.To this end, we have been:•Partnering with institutions and professional organizations serving historically underrepresented communities;•Embedding dedicated recruiting teams to business areas to shepherd underrepresented candidates through the interview process and find internalopportunities;•Supporting the development of women employees through programs aimed at building a pipeline of future leaders;•Providing peer support and executive sponsors for our internal community resource groups;•Providing training and education to managers and peers on fostering supportive environments and recruiting for diversity;•Track equity and parity in retention, promotions, pay, and employee engagement scores; and•Measuring year over year progress and providing leadership visibility on diversity efforts.As of the end of fiscal year 2024, our global workforce was 79% male, 20% female, and 1% not declared, with 6% of our workforce in the United Statescomposed of Black or African American and Hispanic or Latino employees.Flexible Working EnvironmentWe support a flexible work environment, understanding that many employees want the ability to work from home under certain conditions.This flexibilitysupports diverse hiring, retention, and employee engagement, which we believe makes NVIDIA a great place to work.During fiscal year 2025, we will continue to have a flexible work environment and maintain our company wide 2-days off a quarter for employees to rest andrecharge.Information About Our Executive OfficersThe following sets forth certain information regarding our executive officers, their ages, and positions as of February 16, 2024:NameAgePositionJen-Hsun Huang60President and Chief Executive OfficerColette M.Kress56Executive Vice President and Chief Financial OfficerAjay K.Puri69Executive Vice President, Worldwide Field OperationsDebora Shoquist69Executive Vice President, OperationsTimothy S.Teter57Executive Vice President and General CounselJen-Hsun Huang co-founded NVIDIA in 1993 and has served as our President, Chief Executive Officer, and a member of the Board of Directors since ourinception.From 1985 to 1993, Mr.Huang was employed at LSI Logic Corporation, a computer chip manufacturer, where he held a variety of positions includingas Director of Coreware, the business unit responsible for LSI's SOC.From 1983 to 1985, Mr.Huang was a microprocessor designer for AMD, a semiconductorcompany.Mr.Huang holds a B.S.E.E.degree from Oregon State University and an M.S.E.E.degree from Stanford University.Colette M.Kress joined NVIDIA in 2013 as Executive Vice President and Chief Financial Officer.Prior to NVIDIA, Ms.Kress most recently served as SeniorVice President and Chief Financial Officer of the Business Technology and Operations Finance organization at Cisco Systems, Inc., a networking equipmentcompany, since 2010.", "page_start": 12, "page_end": 12, "section_title": "NVIDIA CORPORATION", "score": 0.6533391}, {"chunk_id": "4f638c9d-79d6-4b84-9677-86448558b897", "text": "44,30115,35617,475Operating expensesResearch and development8,6757,3395,268Sales, general and administrative2,6542,4402,166—1,353—Acquisition termination cost11,32911,1327,434Total operating expensesOperating income32,9724,22410,041Interest income86626729Interest expense(257)(262)(236)237(48)107Other, net846(43)(100)Other income (expense), netIncome before income tax33,8184,1819,941Income tax expense (benefit)4,058(187)189$29,760$4,368$9,752Net incomeNet income per share:$12.05$1.76$3.91Basic$11.93$1.74$3.85DilutedWeighted average shares used in per share computation:2,4692,4872,496Basic2,4942,5072,535DilutedSee accompanying notes to the consolidated financial statements.50NVIDIA Corporation and SubsidiariesConsolidated Statements of Comprehensive Income(In millions)Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Net income$29,760$4,368$9,752Other comprehensive income (loss), net of taxAvailable-for-sale securities:Net change in unrealized gain (loss)80(31)(16)Reclassification adjustments for net realized gain included in net income—1—80(30)(16)Net change in unrealized gain (loss)Cash flow hedges:Net change in unrealized gain (loss)3847(43)(48)(49)29Reclassification adjustments for net realized gain (loss) included in net income(10)(2)(14)Net change in unrealized loss70(32)(30)Other comprehensive income (loss), net of tax$29,830$4,336$9,722Total comprehensive incomeSee accompanying notes to the consolidated financial statements.51NVIDIA Corporation and SubsidiariesConsolidated Balance Sheets(In millions, except par value)Jan 28, 2024Jan 29, 2023AssetsCurrent assets:Cash and cash equivalents$7,280$3,389Marketable securities18,7049,907Accounts receivable, net9,9993,827Inventories5,2825,1593,080791Prepaid expenses and other current assetsTotal current assets44,34523,073Property and equipment, net3,9143,807Operating lease assets1,3461,038Goodwill4,4304,372Intangible assets, net1,1121,676Deferred income tax assets6,0813,3964,5003,820Other assets$65,728$41,182Total assetsLiabilities and Shareholders' EquityCurrent liabilities:Accounts payable$2,699$1,193Accrued and other current liabilities6,682", "page_start": 50, "page_end": 52, "section_title": "NVIDIA CORPORATION", "score": 0.65025777}, {"chunk_id": "0e7320ce-f31a-4f98-a6d0-4908cc48de5f", "text": "Intangible assets, net1,1121,676Deferred income tax assets6,0813,3964,5003,820Other assets$65,728$41,182Total assetsLiabilities and Shareholders' EquityCurrent liabilities:Accounts payable$2,699$1,193Accrued and other current liabilities6,6824,1201,2501,250Short-term debtTotal current liabilities10,6316,563Long-term debt8,4599,703Long-term operating lease liabilities1,1199022,5411,913Other long-term liabilitiesTotal liabilities22,75019,081Commitments and contingencies - see Note 13Shareholders’ equity:Preferred stock, $0.001 par value; 2 shares authorized; none issued——Common stock, $0.001 par value; 8,000 shares authorized; 2,464 shares issued and outstanding as ofJanuary 28, 2024; 2,466 shares issued and outstanding as of January 29, 202322Additional paid-in capital13,13211,971Accumulated other comprehensive income (loss)27(43)29,81710,171Retained earnings42,97822,101Total shareholders' equity$65,728$41,182Total liabilities and shareholders' equitySee accompanying notes to the consolidated financial statements.52NVIDIA Corporation and SubsidiariesConsolidated Statements of Shareholders' EquityAccumulatedCommon StockAdditionalOtherTotalOutstandingPaid-inTreasuryComprehensiveRetainedShareholders'SharesAmountCapitalStockIncome (Loss)EarningsEquity(In millions, except per share data)Balances, Jan 31, 20212,479$3$8,719$(10,756)$19$18,908$16,893Net income—————9,7529,752Other comprehensive loss————(30)—(30)Issuance of common stock from stock plans35—281———281Tax withholding related to vesting of restricted stock units(8)—(614)(1,290)——(1,904)Cash dividends declared and paid ($0.16 per common share)—————(399)(399)Fair value of partially vested equity awards assumed in connection withacquisitions——18———18Stock-based compensation——2,001———2,001——(20)12,046—(12,026)—Retirement of Treasury StockBalances, Jan 30, 20222,506310,385—(11)16,23526,612Net income—————4,3684,368Other comprehensive loss————(32)—(32)", "page_start": 52, "page_end": 53, "section_title": "NVIDIA CORPORATION", "score": 0.6291738}], "factual_attention_01": [{"chunk_id": "914cfb0b-3d4a-4988-8392-851b64c13e57", "text": "The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.Weused beam search with a beam size of 4 and length penalty α = 0.6 [38].These hyperparameterswere chosen after experimentation on the development set.We set the maximum output length duringinference to input length + 50, but terminate early when possible [38].Table 2 summarizes our results and compares our translation quality and training costs to other modelarchitectures from the literature.We estimate the number of floating point operations used to train amodel by multiplying the training time, the number of GPUs used, and an estimate of the sustainedsingle-precision floating-point capacity of each GPU 5.6.2Model VariationsTo evaluate the importance of different components of the Transformer, we varied our base modelin different ways, measuring the change in performance on English-to-German translation on the5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the basemodel.All metrics are on the English-to-German translation development set, newstest2013.Listedperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared toper-word perplexities.trainPPLBLEUparamsNdmodeldffhdkdvPdropϵls×106steps(dev)(dev)base65122048864640.10.1100K4.9225.86515125125.2924.941281285.0025.5(A)1632324.9125.83216165.0125.4165.1625.158(B)325.0125.46026.1123.73645.1925.35084.8825.580(C)25632325.7524.52810241281284.6626.016810245.1225.45340964.7526.2900.05.7724.60.24.9525.5(D)0.04.6725.30.25.4725.7(E)positional embedding instead of sinusoids4.9225.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.", "page_start": 8, "page_end": 9, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.5208893}, {"chunk_id": "6ecb21e7-c073-4d90-9d8a-41fdfda06afb", "text": "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,textual entailment and learning task-independent sentence representations [4, 27, 28, 22].End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering andlanguage modeling tasks [34].To the best of our knowledge, however, the Transformer is the first transduction model relyingentirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.In the following sections, we will describe the Transformer, motivateself-attention and discuss its advantages over models such as [17, 18] and [9].3Model ArchitectureMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequenceof continuous representations z = (z1, ..., zn).Given z, the decoder then generates an outputsequence (y1, ..., ym) of symbols one element at a time.At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next.2Figure 1: The Transformer - model architecture.The Transformer follows this overall architecture using stacked self-attention and point-wise, fullyconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,respectively.3.1Encoder and Decoder StacksThe encoder is composed of a stack of N = 6 identical layers.Each layer has twoEncoder:sub-layers.The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network.We employ a residual connection [11] around each ofthe two sub-layers, followed by layer normalization [1].That is, the output of each sub-layer isLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layeritself.To facilitate these residual connections, all sub-layers in the model, as well as the embeddinglayers, produce outputs of dimension dmodel = 512.The decoder is also composed of a stack of N = 6 identical layers.In addition to the twoDecoder:sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-headattention over the output of the encoder stack.Similar to the encoder, we employ residual connectionsaround each of the sub-layers, followed by layer normalization.We also modify the self-attentionsub-layer in the decoder stack to prevent positions from attending to subsequent positions.Thismasking, combined with fact that the output embeddings are offset by one position, ensures that the", "page_start": 2, "page_end": 3, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.5031439}, {"chunk_id": "160dd6c9-8965-4dcd-8d74-e0dd157c4fd1", "text": "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step number.We usedwarmup_steps = 4000.5.4RegularizationWe employ three types of regularization during training:7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on theEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.BLEUTraining Cost (FLOPs)ModelEN-DEEN-FREN-DEEN-FRByteNet [18]23.751.0 · 1020Deep-Att + PosUnk [39]39.22.3 · 10191.4 · 1020GNMT + RL [38]24.639.929.6 · 10181.5 · 1020ConvS2S [9]25.1640.462.0 · 10191.2 · 1020MoE [32]26.0340.568.0 · 1020Deep-Att + PosUnk Ensemble [39]40.41.8 · 10201.1 · 1021GNMT + RL Ensemble [38]26.3041.167.7 · 10191.2 · 102141.29ConvS2S Ensemble [9]26.363.3 · 1018Transformer (base model)27.338.12.3 · 101928.441.8Transformer (big)Residual DropoutWe apply dropout [33] to the output of each sub-layer, before it is added to thesub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and thepositional encodings in both the encoder and decoder stacks.For the base model, we use a rate ofPdrop = 0.1.During training, we employed label smoothing of value ϵls = 0.1 [36].ThisLabel Smoothinghurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6Results6.1Machine TranslationOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0BLEU, establishing a new state-of-the-art BLEU score of 28.4.The configuration of this model islisted in the bottom line of Table 3.Training took 3.5 days on 8 P100 GPUs.Even our base modelsurpasses all previously published models and ensembles, at a fraction of the training cost of any ofthe competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,outperforming all of the previously published single models, at less than 1/4 the training cost of theprevious state-of-the-art model.The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We", "page_start": 7, "page_end": 8, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.47342017}, {"chunk_id": "8d9a2784-c6bd-4675-a20b-80dd0948329f", "text": "25.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,keeping the amount of computation constant, as described in Section 3.2.2.While single-headattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality.Thissuggests that determining compatibility is not easy and that a more sophisticated compatibilityfunction than dot product may be beneficial.We further observe in rows (C) and (D) that, as expected,bigger models are better, and dropout is very helpful in avoiding over-fitting.In row (E) we replace oursinusoidal positional encoding with learned positional embeddings [9], and observe nearly identicalresults to the base model.6.3English Constituency ParsingTo evaluate if the Transformer can generalize to other tasks we performed experiments on Englishconstituency parsing.This task presents specific challenges: the output is subject to strong structuralconstraints and is significantly longer than the input.Furthermore, RNN sequence-to-sequencemodels have not been able to attain state-of-the-art results in small-data regimes [37].We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of thePenn Treebank [25], about 40K training sentences.We also trained it in a semi-supervised setting,using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences[37].We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokensfor the semi-supervised setting.We performed only a small number of experiments to select the dropout, both attention and residual(section 5.4), learning rates and beam size on the Section 22 development set, all other parametersremained unchanged from the English-to-German base translation model.During inference, we9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23of WSJ)ParserTrainingWSJ 23 F1Vinyals & Kaiser el al.(2014) [37]WSJ only, discriminative88.3Petrov et al.(2006) [29]WSJ only, discriminative90.4Zhu et al.(2013) [40]WSJ only, discriminative90.4Dyer et al.(2016) [8]WSJ only, discriminative91.7Transformer (4 layers)WSJ only, discriminative91.3Zhu et al.(2013) [40]semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised", "page_start": 9, "page_end": 10, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4629204}, {"chunk_id": "58f1c685-b5d0-4ba4-8711-0ba5768f729c", "text": "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,we found it beneficial to linearly project the queries, keys and values h times with different, learnedlinear projections to dk, dk and dv dimensions, respectively.On each of these projected versions ofqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional4To illustrate why the dot products get large, assume that the components of q and k are independent randomvariables with mean 0 and variance 1.Then their dot product, q · k = Pdki=1 qiki, has mean 0 and variance dk.4output values.These are concatenated and once again projected, resulting in the final values, asdepicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representationsubspaces at different positions.With a single attention head, averaging inhibits this.MultiHead(Q, K, V ) = Concat(head1, ..., headh)W Owhere headi = Attention(QW Qi , KW Ki , V W Vi )Where the projections are parameter matrices W Q∈Rdmodel×dk, W V∈Rdmodel×dv∈Rdmodel×dk, W Kiiiand W O ∈Rhdv×dmodel.In this work we employ h = 8 parallel attention layers, or heads.For each of these we usedk = dv = dmodel/h = 64.Due to the reduced dimension of each head, the total computational costis similar to that of single-head attention with full dimensionality.3.2.3Applications of Attention in our ModelThe Transformer uses multi-head attention in three different ways:• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,and the memory keys and values come from the output of the encoder.This allows everyposition in the decoder to attend over all positions in the input sequence.This mimics thetypical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38, 2, 9].• The encoder contains self-attention layers.In a self-attention layer all of the keys, valuesand queries come from the same place, in this case, the output of the previous layer in theencoder.Each position in the encoder can attend to all positions in the previous layer of theencoder.• Similarly, self-attention layers in the decoder allow each position in the decoder to attend toall positions in the decoder up to and including that position.We need to prevent leftwardinformation flow in the decoder to preserve the auto-regressive property.We implement thisinside of scaled dot-product attention by masking out (setting to −∞) all values in the inputof the softmax which correspond to illegal connections.See Figure 2.3.3Position-wise Feed-Forward NetworksIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fullyconnected feed-forward network, which is applied to each position separately and identically.This", "page_start": 4, "page_end": 5, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4500365}, {"chunk_id": "0b373c99-8c0e-44bf-8fc7-f6b6b009590d", "text": "Provided proper attribution is provided, Google hereby grants permission toreproduce the tables and figures in this paper solely for use in journalistic orscholarly works.Ashish Vaswani∗Noam Shazeer∗Niki Parmar∗Jakob Uszkoreit∗Google BrainGoogle BrainGoogle ResearchGoogle Researchavaswani@google.comnoam@google.comnikip@google.comusz@google.comLlion Jones∗Aidan N.Gomez∗†Łukasz Kaiser∗Google ResearchUniversity of TorontoGoogle Brainlukaszkaiser@google.comllion@google.comaidan@cs.toronto.eduIllia Polosukhin∗‡illia.polosukhin@gmail.comAbstractThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder.The bestperforming models also connect the encoder and decoder through an attentionmechanism.We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely.Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring significantlyless time to train.Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU.On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.8 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature.We show that the Transformer generalizes well toother tasks by applying it successfully to English constituency parsing both withlarge and limited training data.∗Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and startedthe effort to evaluate this idea.Ashish, with Illia, designed and implemented the first Transformer models andhas been crucially involved in every aspect of this work.Noam proposed scaled dot-product attention, multi-headattention and the parameter-free position representation and became the other person involved in nearly everydetail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase andtensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, andefficient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of andimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks", "page_start": 1, "page_end": 2, "section_title": "", "score": 0.44448712}, {"chunk_id": "5151352b-ae46-44f3-83b0-8b8748547941", "text": "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networksin particular, have been firmly established as state of the art approaches in sequence modeling andtransduction problems such as language modeling and machine translation [35, 2, 5].Numerousefforts have since continued to push the boundaries of recurrent language models and encoder-decoderarchitectures [38, 24, 15].Recurrent models typically factor computation along the symbol positions of the input and outputsequences.Aligning the positions to steps in computation time, they generate a sequence of hiddenstates ht, as a function of the previous hidden state ht−1 and the input for position t.This inherentlysequential nature precludes parallelization within training examples, which becomes critical at longersequence lengths, as memory constraints limit batching across examples.Recent work has achievedsignificant improvements in computational efficiency through factorization tricks [21] and conditionalcomputation [32], while also improving model performance in case of the latter.The fundamentalconstraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance inthe input or output sequences [2, 19].In all but a few cases [27], however, such attention mechanismsare used in conjunction with a recurrent network.In this work we propose the Transformer, a model architecture eschewing recurrence and insteadrelying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for significantly more parallelization and can reach a new state of the art intranslation quality after being trained for as little as twelve hours on eight P100 GPUs.2BackgroundThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic buildingblock, computing hidden representations in parallel for all input and output positions.In these models,the number of operations required to relate signals from two arbitrary input or output positions growsin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.This makesit more difficult to learn dependencies between distant positions [12].In the Transformer this isreduced to a constant number of operations, albeit at the cost of reduced effective resolution dueto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,", "page_start": 1, "page_end": 2, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.43512803}, {"chunk_id": "d18429f8-020c-433c-b74a-eb6e9b76b836", "text": "inside of scaled dot-product attention by masking out (setting to −∞) all values in the inputof the softmax which correspond to illegal connections.See Figure 2.3.3Position-wise Feed-Forward NetworksIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fullyconnected feed-forward network, which is applied to each position separately and identically.Thisconsists of two linear transformations with a ReLU activation in between.FFN(x) = max(0, xW1 + b1)W2 + b2(2)While the linear transformations are the same across different positions, they use different parametersfrom layer to layer.Another way of describing this is as two convolutions with kernel size 1.The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionalitydff = 2048.3.4Embeddings and SoftmaxSimilarly to other sequence transduction models, we use learned embeddings to convert the inputtokens and output tokens to vectors of dimension dmodel.We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.Inour model, we share the same weight matrix between the two embedding layers and the pre-softmaxlinear transformation, similar to [30].In the embedding layers, we multiply those weights by √dmodel.5Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operationsfor different layer types.n is the sequence length, d is the representation dimension, k is the kernelsize of convolutions and r the size of the neighborhood in restricted self-attention.Layer TypeComplexity per LayerSequentialMaximum Path LengthOperationsO(n2 · d)O(1)O(1)Self-AttentionO(n · d2)O(n)O(n)RecurrentO(k · n · d2)O(1)O(logk(n))ConvolutionalO(r · n · d)O(1)O(n/r)Self-Attention (restricted)3.5Positional EncodingSince our model contains no recurrence and no convolution, in order for the model to make use of theorder of the sequence, we must inject some information about the relative or absolute position of thetokens in the sequence.To this end, we add \"positional encodings\" to the input embeddings at thebottoms of the encoder and decoder stacks.The positional encodings have the same dimension dmodelas the embeddings, so that the two can be summed.There are many choices of positional encodings,learned and fixed [9].In this work, we use sine and cosine functions of different frequencies:PE(pos,2i) = sin(pos/100002i/dmodel)PE(pos,2i+1) = cos(pos/100002i/dmodel)where pos is the position and i is the dimension.That is, each dimension of the positional encodingcorresponds to a sinusoid.The wavelengths form a geometric progression from 2π to 10000 · 2π.We", "page_start": 5, "page_end": 6, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4303187}], "factual_attention_02": [{"chunk_id": "6ecb21e7-c073-4d90-9d8a-41fdfda06afb", "text": "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,textual entailment and learning task-independent sentence representations [4, 27, 28, 22].End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering andlanguage modeling tasks [34].To the best of our knowledge, however, the Transformer is the first transduction model relyingentirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.In the following sections, we will describe the Transformer, motivateself-attention and discuss its advantages over models such as [17, 18] and [9].3Model ArchitectureMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequenceof continuous representations z = (z1, ..., zn).Given z, the decoder then generates an outputsequence (y1, ..., ym) of symbols one element at a time.At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next.2Figure 1: The Transformer - model architecture.The Transformer follows this overall architecture using stacked self-attention and point-wise, fullyconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,respectively.3.1Encoder and Decoder StacksThe encoder is composed of a stack of N = 6 identical layers.Each layer has twoEncoder:sub-layers.The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network.We employ a residual connection [11] around each ofthe two sub-layers, followed by layer normalization [1].That is, the output of each sub-layer isLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layeritself.To facilitate these residual connections, all sub-layers in the model, as well as the embeddinglayers, produce outputs of dimension dmodel = 512.The decoder is also composed of a stack of N = 6 identical layers.In addition to the twoDecoder:sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-headattention over the output of the encoder stack.Similar to the encoder, we employ residual connectionsaround each of the sub-layers, followed by layer normalization.We also modify the self-attentionsub-layer in the decoder stack to prevent positions from attending to subsequent positions.Thismasking, combined with fact that the output embeddings are offset by one position, ensures that the", "page_start": 2, "page_end": 3, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.61888146}, {"chunk_id": "58f1c685-b5d0-4ba4-8711-0ba5768f729c", "text": "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,we found it beneficial to linearly project the queries, keys and values h times with different, learnedlinear projections to dk, dk and dv dimensions, respectively.On each of these projected versions ofqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional4To illustrate why the dot products get large, assume that the components of q and k are independent randomvariables with mean 0 and variance 1.Then their dot product, q · k = Pdki=1 qiki, has mean 0 and variance dk.4output values.These are concatenated and once again projected, resulting in the final values, asdepicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representationsubspaces at different positions.With a single attention head, averaging inhibits this.MultiHead(Q, K, V ) = Concat(head1, ..., headh)W Owhere headi = Attention(QW Qi , KW Ki , V W Vi )Where the projections are parameter matrices W Q∈Rdmodel×dk, W V∈Rdmodel×dv∈Rdmodel×dk, W Kiiiand W O ∈Rhdv×dmodel.In this work we employ h = 8 parallel attention layers, or heads.For each of these we usedk = dv = dmodel/h = 64.Due to the reduced dimension of each head, the total computational costis similar to that of single-head attention with full dimensionality.3.2.3Applications of Attention in our ModelThe Transformer uses multi-head attention in three different ways:• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,and the memory keys and values come from the output of the encoder.This allows everyposition in the decoder to attend over all positions in the input sequence.This mimics thetypical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38, 2, 9].• The encoder contains self-attention layers.In a self-attention layer all of the keys, valuesand queries come from the same place, in this case, the output of the previous layer in theencoder.Each position in the encoder can attend to all positions in the previous layer of theencoder.• Similarly, self-attention layers in the decoder allow each position in the decoder to attend toall positions in the decoder up to and including that position.We need to prevent leftwardinformation flow in the decoder to preserve the auto-regressive property.We implement thisinside of scaled dot-product attention by masking out (setting to −∞) all values in the inputof the softmax which correspond to illegal connections.See Figure 2.3.3Position-wise Feed-Forward NetworksIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fullyconnected feed-forward network, which is applied to each position separately and identically.This", "page_start": 4, "page_end": 5, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.54827225}, {"chunk_id": "0b373c99-8c0e-44bf-8fc7-f6b6b009590d", "text": "Provided proper attribution is provided, Google hereby grants permission toreproduce the tables and figures in this paper solely for use in journalistic orscholarly works.Ashish Vaswani∗Noam Shazeer∗Niki Parmar∗Jakob Uszkoreit∗Google BrainGoogle BrainGoogle ResearchGoogle Researchavaswani@google.comnoam@google.comnikip@google.comusz@google.comLlion Jones∗Aidan N.Gomez∗†Łukasz Kaiser∗Google ResearchUniversity of TorontoGoogle Brainlukaszkaiser@google.comllion@google.comaidan@cs.toronto.eduIllia Polosukhin∗‡illia.polosukhin@gmail.comAbstractThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder.The bestperforming models also connect the encoder and decoder through an attentionmechanism.We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely.Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring significantlyless time to train.Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU.On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.8 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature.We show that the Transformer generalizes well toother tasks by applying it successfully to English constituency parsing both withlarge and limited training data.∗Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and startedthe effort to evaluate this idea.Ashish, with Illia, designed and implemented the first Transformer models andhas been crucially involved in every aspect of this work.Noam proposed scaled dot-product attention, multi-headattention and the parameter-free position representation and became the other person involved in nearly everydetail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase andtensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, andefficient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of andimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks", "page_start": 1, "page_end": 2, "section_title": "", "score": 0.54058474}, {"chunk_id": "5151352b-ae46-44f3-83b0-8b8748547941", "text": "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networksin particular, have been firmly established as state of the art approaches in sequence modeling andtransduction problems such as language modeling and machine translation [35, 2, 5].Numerousefforts have since continued to push the boundaries of recurrent language models and encoder-decoderarchitectures [38, 24, 15].Recurrent models typically factor computation along the symbol positions of the input and outputsequences.Aligning the positions to steps in computation time, they generate a sequence of hiddenstates ht, as a function of the previous hidden state ht−1 and the input for position t.This inherentlysequential nature precludes parallelization within training examples, which becomes critical at longersequence lengths, as memory constraints limit batching across examples.Recent work has achievedsignificant improvements in computational efficiency through factorization tricks [21] and conditionalcomputation [32], while also improving model performance in case of the latter.The fundamentalconstraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance inthe input or output sequences [2, 19].In all but a few cases [27], however, such attention mechanismsare used in conjunction with a recurrent network.In this work we propose the Transformer, a model architecture eschewing recurrence and insteadrelying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for significantly more parallelization and can reach a new state of the art intranslation quality after being trained for as little as twelve hours on eight P100 GPUs.2BackgroundThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic buildingblock, computing hidden representations in parallel for all input and output positions.In these models,the number of operations required to relate signals from two arbitrary input or output positions growsin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.This makesit more difficult to learn dependencies between distant positions [12].In the Transformer this isreduced to a constant number of operations, albeit at the cost of reduced effective resolution dueto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,", "page_start": 1, "page_end": 2, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.5223297}, {"chunk_id": "914cfb0b-3d4a-4988-8392-851b64c13e57", "text": "The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.Weused beam search with a beam size of 4 and length penalty α = 0.6 [38].These hyperparameterswere chosen after experimentation on the development set.We set the maximum output length duringinference to input length + 50, but terminate early when possible [38].Table 2 summarizes our results and compares our translation quality and training costs to other modelarchitectures from the literature.We estimate the number of floating point operations used to train amodel by multiplying the training time, the number of GPUs used, and an estimate of the sustainedsingle-precision floating-point capacity of each GPU 5.6.2Model VariationsTo evaluate the importance of different components of the Transformer, we varied our base modelin different ways, measuring the change in performance on English-to-German translation on the5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the basemodel.All metrics are on the English-to-German translation development set, newstest2013.Listedperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared toper-word perplexities.trainPPLBLEUparamsNdmodeldffhdkdvPdropϵls×106steps(dev)(dev)base65122048864640.10.1100K4.9225.86515125125.2924.941281285.0025.5(A)1632324.9125.83216165.0125.4165.1625.158(B)325.0125.46026.1123.73645.1925.35084.8825.580(C)25632325.7524.52810241281284.6626.016810245.1225.45340964.7526.2900.05.7724.60.24.9525.5(D)0.04.6725.30.25.4725.7(E)positional embedding instead of sinusoids4.9225.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.", "page_start": 8, "page_end": 9, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.5128164}, {"chunk_id": "160dd6c9-8965-4dcd-8d74-e0dd157c4fd1", "text": "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step number.We usedwarmup_steps = 4000.5.4RegularizationWe employ three types of regularization during training:7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on theEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.BLEUTraining Cost (FLOPs)ModelEN-DEEN-FREN-DEEN-FRByteNet [18]23.751.0 · 1020Deep-Att + PosUnk [39]39.22.3 · 10191.4 · 1020GNMT + RL [38]24.639.929.6 · 10181.5 · 1020ConvS2S [9]25.1640.462.0 · 10191.2 · 1020MoE [32]26.0340.568.0 · 1020Deep-Att + PosUnk Ensemble [39]40.41.8 · 10201.1 · 1021GNMT + RL Ensemble [38]26.3041.167.7 · 10191.2 · 102141.29ConvS2S Ensemble [9]26.363.3 · 1018Transformer (base model)27.338.12.3 · 101928.441.8Transformer (big)Residual DropoutWe apply dropout [33] to the output of each sub-layer, before it is added to thesub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and thepositional encodings in both the encoder and decoder stacks.For the base model, we use a rate ofPdrop = 0.1.During training, we employed label smoothing of value ϵls = 0.1 [36].ThisLabel Smoothinghurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6Results6.1Machine TranslationOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0BLEU, establishing a new state-of-the-art BLEU score of 28.4.The configuration of this model islisted in the bottom line of Table 3.Training took 3.5 days on 8 P100 GPUs.Even our base modelsurpasses all previously published models and ensembles, at a fraction of the training cost of any ofthe competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,outperforming all of the previously published single models, at less than 1/4 the training cost of theprevious state-of-the-art model.The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We", "page_start": 7, "page_end": 8, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4925608}, {"chunk_id": "8d9a2784-c6bd-4675-a20b-80dd0948329f", "text": "25.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,keeping the amount of computation constant, as described in Section 3.2.2.While single-headattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality.Thissuggests that determining compatibility is not easy and that a more sophisticated compatibilityfunction than dot product may be beneficial.We further observe in rows (C) and (D) that, as expected,bigger models are better, and dropout is very helpful in avoiding over-fitting.In row (E) we replace oursinusoidal positional encoding with learned positional embeddings [9], and observe nearly identicalresults to the base model.6.3English Constituency ParsingTo evaluate if the Transformer can generalize to other tasks we performed experiments on Englishconstituency parsing.This task presents specific challenges: the output is subject to strong structuralconstraints and is significantly longer than the input.Furthermore, RNN sequence-to-sequencemodels have not been able to attain state-of-the-art results in small-data regimes [37].We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of thePenn Treebank [25], about 40K training sentences.We also trained it in a semi-supervised setting,using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences[37].We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokensfor the semi-supervised setting.We performed only a small number of experiments to select the dropout, both attention and residual(section 5.4), learning rates and beam size on the Section 22 development set, all other parametersremained unchanged from the English-to-German base translation model.During inference, we9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23of WSJ)ParserTrainingWSJ 23 F1Vinyals & Kaiser el al.(2014) [37]WSJ only, discriminative88.3Petrov et al.(2006) [29]WSJ only, discriminative90.4Zhu et al.(2013) [40]WSJ only, discriminative90.4Dyer et al.(2016) [8]WSJ only, discriminative91.7Transformer (4 layers)WSJ only, discriminative91.3Zhu et al.(2013) [40]semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised", "page_start": 9, "page_end": 10, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.49045426}, {"chunk_id": "bb77d3cc-59f9-4b97-a21b-132b30215259", "text": "semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised92.1Transformer (4 layers)semi-supervised92.7Luong et al.(2015) [23]multi-task93.0Dyer et al.(2016) [8]generative93.3increased the maximum output length to input length + 300.We used a beam size of 21 and α = 0.3for both WSJ only and the semi-supervised setting.Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of theRecurrent Neural Network Grammar [8].In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.7ConclusionIn this work, we presented the Transformer, the first sequence transduction model based entirely onattention, replacing the recurrent layers most commonly used in encoder-decoder architectures withmulti-headed self-attention.For translation tasks, the Transformer can be trained significantly faster than architectures basedon recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014English-to-French translation tasks, we achieve a new state of the art.In the former task our bestmodel outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.Weplan to extend the Transformer to problems involving input and output modalities other than text andto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputssuch as images, audio and video.Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.AcknowledgementsWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitfulcomments, corrections and inspiration.References[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprintarXiv:1607.06450, 2016.[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointlylearning to align and translate.CoRR, abs/1409.0473, 2014.[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V.Le.Massive exploration of neuralmachine translation architectures.CoRR, abs/1703.03906, 2017.[4] Jianpeng Cheng, Li Dong, and Mirella Lapata.Long short-term memory-networks for machinereading.arXiv preprint arXiv:1601.06733, 2016.10[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,", "page_start": 10, "page_end": 11, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.489423}], "factual_nda_01": [{"chunk_id": "94d954fb-950b-4dcf-8049-c72deb8243c9", "text": "Date: 201[ ]Parties:[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]and[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]1.Each of the parties to this Agreement intends to disclose information (the ConfidentialInformation) to the other party for the purpose of [insert details e.g.discussing thepossibility of the parties entering into a joint venture] (the Purpose).2.Each party to this Agreement is referred to as ‘the Recipient’ when it receives or usesthe Confidential Information disclosed by the other party.3.The Recipient undertakes not to use the Confidential Information disclosed by the otherparty for any purpose except the Purpose, without first obtaining the written agreementof the other party.4.The Recipient undertakes to keep the Confidential Information disclosed by the otherparty secure and not to disclose it to any third party [except to its employees [andprofessional advisers] who need to know the same for the Purpose, who know theyowe a duty of confidence to the other party and who are bound by obligationsequivalent to those in clause 3 above and this clause 4.5.The undertakings in clauses 3 and 4 above apply to all of the information disclosed byeach of the parties to the other, regardless of the way or form in which it is disclosed orrecorded but they do not apply to:a) any information which is or in future comes into the public domain (unless as a result ofthe breach of this Agreement); orb) any information which is already known to the Recipient and which was not subject toany obligation of confidence before it was disclosed to the Recipient by the other party.6.Nothing in this Agreement will prevent the Recipient from making any disclosure of theConfidential Information required by law or by any competent authority.7.The Recipient will, on request from the other party, return all copies and records of theConfidential Information disclosed by the other party to the Recipient and will not retainany copies or records of the Confidential Information disclosed by the other party.8.Neither this Agreement nor the supply of any information grants the Recipient anylicence, interest or right in respect of any intellectual property rights of the other partyexcept the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.", "page_start": 1, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.54260147}, {"chunk_id": "6c64d139-fd3d-400d-b64b-3ddf0dea3473", "text": "except the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.The English Courts will have non-exclusive jurisdiction to deal with any dispute whichhas arisen or may arise out of, or in connection with, this Agreement.Signed [by [insert name]] OR [on behalf of][insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________PositionSigned [by [insert name]] OR [on behalf of] [insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________Position", "page_start": 2, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.5420172}], "factual_nda_02": [{"chunk_id": "94d954fb-950b-4dcf-8049-c72deb8243c9", "text": "Date: 201[ ]Parties:[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]and[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]1.Each of the parties to this Agreement intends to disclose information (the ConfidentialInformation) to the other party for the purpose of [insert details e.g.discussing thepossibility of the parties entering into a joint venture] (the Purpose).2.Each party to this Agreement is referred to as ‘the Recipient’ when it receives or usesthe Confidential Information disclosed by the other party.3.The Recipient undertakes not to use the Confidential Information disclosed by the otherparty for any purpose except the Purpose, without first obtaining the written agreementof the other party.4.The Recipient undertakes to keep the Confidential Information disclosed by the otherparty secure and not to disclose it to any third party [except to its employees [andprofessional advisers] who need to know the same for the Purpose, who know theyowe a duty of confidence to the other party and who are bound by obligationsequivalent to those in clause 3 above and this clause 4.5.The undertakings in clauses 3 and 4 above apply to all of the information disclosed byeach of the parties to the other, regardless of the way or form in which it is disclosed orrecorded but they do not apply to:a) any information which is or in future comes into the public domain (unless as a result ofthe breach of this Agreement); orb) any information which is already known to the Recipient and which was not subject toany obligation of confidence before it was disclosed to the Recipient by the other party.6.Nothing in this Agreement will prevent the Recipient from making any disclosure of theConfidential Information required by law or by any competent authority.7.The Recipient will, on request from the other party, return all copies and records of theConfidential Information disclosed by the other party to the Recipient and will not retainany copies or records of the Confidential Information disclosed by the other party.8.Neither this Agreement nor the supply of any information grants the Recipient anylicence, interest or right in respect of any intellectual property rights of the other partyexcept the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.", "page_start": 1, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.511741}, {"chunk_id": "6c64d139-fd3d-400d-b64b-3ddf0dea3473", "text": "except the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.The English Courts will have non-exclusive jurisdiction to deal with any dispute whichhas arisen or may arise out of, or in connection with, this Agreement.Signed [by [insert name]] OR [on behalf of][insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________PositionSigned [by [insert name]] OR [on behalf of] [insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________Position", "page_start": 2, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.44147572}], "comparative_nvidia_01": [{"chunk_id": "ddc05d6e-79f6-4ed4-a29b-876d9e76fd48", "text": "We did not experience any significant impact or expense toour business; however, if the conflict is further extended, it could impact future product development, operations, and revenue or create other uncertainty for ourbusiness.35Fiscal Year 2024 SummaryYear EndedJan 28, 2024Jan 29, 2023Change($ in millions, except per share data)Revenue$60,922$26,974Up 126%Gross margin72.7 %56.9 %Up 15.8 ptsOperating expenses$11,329$11,132Up 2%Operating income$32,972$4,224Up 681%Net income$29,760$4,368Up 581%Net income per diluted share$11.93$1.74Up 586%We specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Revenue for fiscal year 2024 was $60.9 billion, up 126% from a year ago.Data Center revenue for fiscal year 2024 was up 217%.Strong demand was driven by enterprise software and consumer internet applications, and multipleindustry verticals including automotive, financial services, and healthcare.Customers across industry verticals access NVIDIA AI infrastructure both through thecloud and on-premises.Data Center compute revenue was up 244% in the fiscal year.Networking revenue was up 133% in the fiscal year.Gaming revenue for fiscal year 2024 was up 15%.The increase reflects higher sell-in to partners following the normalization of channel inventory levels andgrowing demand.Professional Visualization revenue for fiscal year 2024 was up 1%.Automotive revenue for the fiscal year 2024 was up 21%.The increase primarily reflected growth in self-driving platforms.Gross margin increased in fiscal year 2024, primarily driven by Data Center revenue growth and lower net inventory provisions as a percentage of revenue.Operating expenses increased for fiscal year 2024, driven by growth in employees and compensation increases.Fiscal year 2023 also included a $1.4 billionacquisition termination charge related to the proposed Arm transaction.Market Platform HighlightsData Center revenue for fiscal year 2024 was $47.5 billion, up 217% from fiscal year 2023.In Data Center, we launched AI inference platforms that combine ourfull-stack inference software with NVIDIA Ada, NVIDIA Hopper and NVIDIA Grace Hopper processors optimized for generative AI, LLMs and other AI workloads.We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.", "page_start": 35, "page_end": 36, "section_title": "NVIDIA CORPORATION", "score": 0.7445093}, {"chunk_id": "823438dd-9ce8-4ae9-924a-70c6bbf5121f", "text": "Form 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38The following table sets forth, for the periods indicated, certain items in our Consolidated Statements of Income expressed as a percentage of revenue.Year EndedJan 28, 2024Jan 29, 2023Revenue100.0 %100.0 %Cost of revenue27.343.1Gross profit72.756.9Operating expensesResearch and development14.227.2Sales, general and administrative4.49.1Acquisition termination cost—5.0Total operating expenses18.641.3Operating income54.115.6Interest income1.41.0Interest expense(0.4)(1.0)0.4(0.1)Other, net1.4(0.1)Other income (expense), netIncome before income tax55.515.5Income tax expense (benefit)6.6(0.7)48.9 %16.2 %Net incomeReportable SegmentsRevenue by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$47,405$15,068$32,337215 %13,51711,9061,611Graphics14 %$60,922$26,974$33,948Total126 %Operating Income by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$32,016$5,083$26,933530 %Graphics5,8464,5521,29428 %(4,890)(5,411)521All Other(10)%$32,972$4,224$28,748Total681 %Compute & Networking revenue – The year-on-year increase was due to higher Data Center revenue.Compute grew 266% due to higher shipments of theNVIDIA Hopper GPU computing platform for the training and inference of LLMs, recommendation engines and generative AI applications.Networking was up133% due to higher shipments of InfiniBand.Graphics revenue – The year-on-year increase was led by growth in Gaming of 15% driven by higher sell-in to partners following the normalization of channelinventory levels.Reportable segment operating income – The year-on-year increase in Compute & Networking and Graphics operating income was driven by higher revenue.39All Other operating loss - The year-on-year decrease was due to the $1.4 billion Arm acquisition termination cost in fiscal year 2023, partially offset by a $839million increase in stock-based compensation expense in fiscal year 2024.Concentration of Revenue", "page_start": 38, "page_end": 40, "section_title": "NVIDIA CORPORATION", "score": 0.73951524}, {"chunk_id": "f2207e14-4c6c-421b-849b-76359737cca1", "text": "Germany, India, Israel, and Taiwan for fiscal years 2005 through 2023.Note 15 - Shareholders’ EquityCapital Return ProgramIn August 2023, our Board of Directors approved an increase to our share repurchase program of an additional $25.0 billion, without expiration.During fiscalyear 2024, we repurchased 21 million shares of our common stock for $9.7 billion.As of January 28, 2024, we were authorized, subject to certain specifications,to repurchase additional shares of our common stock up to $22.5 billion.From January 29, 2024 through February 16, 2024, we repurchased 2.8 million sharesfor $1.9 billion pursuant to a Rule 10b5-1 trading plan.Our share repurchase program aims to offset dilution from shares issued to employees.We may pursueadditional share repurchases as we weigh market factors and other investment opportunities.During fiscal years 2024, 2023, and 2022, we paid $395 million, $398 million, and $399 million in cash dividends to our shareholders, respectively.Our cashdividend program and the payment of future cash dividends under that program are subject to our Board of Directors' continuing determination that the dividendprogram and the declaration of dividends thereunder are in the best interests of our shareholders.In fiscal year 2022, we retired our existing 349 million treasury shares.These shares assumed the status of authorized and unissued shares upon retirement.The excess of repurchase price over par value was allocated between additional paid-in capital and retained earnings, resulting in a reduction in additional paidin capital by $20 million and retained earnings by $12.0 billion.Any future repurchased shares will assume the status of authorized and unissued shares.77NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Note 16 - Employee Retirement PlansWe provide tax-qualified defined contribution plans to eligible employees in the U.S.and certain other countries.Our contribution expense for fiscal years 2024,2023, and 2022 was $255 million, $227 million, and $168 million, respectively.Note 17 - Segment InformationOur Chief Executive Officer, who is considered to be our chief operating decision maker, or CODM, reviews financial information presented on an operatingsegment basis for purposes of making decisions and assessing financial performance.The Compute & Networking segment includes our Data Center accelerated computing platform; networking; automotive artificial intelligence, or AI, Cockpit,autonomous driving development agreements, and autonomous vehicle solutions; electric vehicle computing platforms; Jetson for robotics and other embeddedplatforms; NVIDIA AI Enterprise and other software; and DGX Cloud.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions forgaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across our", "page_start": 77, "page_end": 78, "section_title": "NVIDIA CORPORATION", "score": 0.6924764}, {"chunk_id": "8d7f8306-8d55-40f5-b383-f4914775e7ee", "text": "(10)Restructuring costs and other—(54)—Acquisition termination cost—(1,353)—10(2)—Other$(4,890)$(5,411)$(3,049)TotalRevenue by geographic areas is designated based upon the billing location of the customer.End customer location may be different than our customer’s billinglocation.Revenue by geographic areas was as follows:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Revenue:(In millions)United States$26,966$8,292$4,349Taiwan13,4056,9868,544China (including Hong Kong)10,3065,7857,11110,2455,9116,910Other countries$60,922$26,974$26,914Total revenueRevenue from sales to customers outside of the United States accounted for 56%, 69%, and 84% of total revenue for fiscal years 2024, 2023, and 2022,respectively.The increase in revenue to the United States for fiscal year 2024 was primarily due to higher U.S.-based Compute & Networking segment demand.Sales to one customer represented 13% of total revenue for fiscal year 2024, which was attributable to the Compute & Networking segment.No customerrepresented 10% or more of total revenue for fiscal years 2023 and 2022.The following table summarizes information pertaining to our revenue by each of the specialized markets we serve:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Revenue:(In millions)Data Center$47,525$15,005$10,613Gaming10,4479,06712,462Professional Visualization1,5531,5442,111Automotive1,0919035663064551,162OEM and Other$60,922$26,974$26,914Total revenue79NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)The following table presents summarized information for long-lived assets by country.Long-lived assets consist of property and equipment and exclude otherassets, operating lease assets, goodwill, and intangible assets.Jan 28, 2024Jan 29, 2023Long-lived assets:(In millions)United States$2,595$2,587Taiwan773702Israel325283Other countries221235$3,914$3,807Total long-lived assets80NVIDIA Corporation and SubsidiariesSchedule II – Valuation and Qualifying AccountsBalance atBeginning ofBalance atDescriptionPeriodAdditionsDeductionsEnd of Period(In millions)Fiscal year 2024Allowance for doubtful accounts$4$— (1)$— (1)$4Sales return allowance$26$213 (2)$", "page_start": 79, "page_end": 81, "section_title": "NVIDIA CORPORATION", "score": 0.6734464}, {"chunk_id": "a39adb5c-8c62-4daa-b471-d64ac0bed3a6", "text": "gaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across ourunified architecture and therefore allocated between our two segments.The “All Other” category includes the expenses that our CODM does not assign to either Compute & Networking or Graphics for purposes of making operatingdecisions or assessing financial performance.The expenses include stock-based compensation expense, corporate infrastructure and support costs, acquisitionrelated and other costs, intellectual property related, or IP-related costs, acquisition termination cost, and other non-recurring charges and benefits that ourCODM deems to be enterprise in nature.Our CODM does not review any information regarding total assets on a reportable segment basis.Depreciation and amortization expense directly attributable toeach reportable segment is included in operating results for each segment.However, our CODM does not evaluate depreciation and amortization expense byoperating segment and, therefore, it is not separately presented.There is no intersegment revenue.The accounting policies for segment reporting are the sameas for our consolidated financial statements.The table below presents details of our reportable segments and the “All Other” category.Compute &NetworkingGraphicsAll OtherConsolidated(In millions)Year Ended Jan 28, 2024:Revenue$47,405$13,517$—$60,922Operating income (loss)$32,016$5,846$(4,890)$32,972Year Ended Jan 29, 2023:Revenue$15,068$11,906$—$26,974Operating income (loss)$5,083$4,552$(5,411)$4,224Year Ended Jan 30, 2022:Revenue$11,046$15,868$—$26,914Operating income (loss)$4,598$8,492$(3,049)$10,04178NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022(In millions)Reconciling items included in \"All Other\" category:Stock-based compensation expense$(3,549)$(2,710)$(2,004)Unallocated cost of revenue and operating expenses(728)(595)(399)Acquisition-related and other costs(583)(674)(636)IP-related and legal settlement costs(40)(23)(10)Restructuring costs and other—(54)—Acquisition termination cost—(1,353)—10(2)—Other$(4,890)$(5,411)$(3,049)TotalRevenue by geographic areas is designated based upon the billing location of the customer.", "page_start": 78, "page_end": 79, "section_title": "NVIDIA CORPORATION", "score": 0.66141075}, {"chunk_id": "66e77043-a370-4255-b50b-931d8a75e255", "text": "We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.In the fourth quarter of fiscal year 2024, largecloud providers represented more than half of our Data Center revenue, supporting both internal workloads and external customers.We announced NVIDIASpectrum-X, an accelerated networking platform for AI.Gaming revenue for fiscal year 2024 was $10.4 billion, up 15% from fiscal year 2023.In Gaming, we launched the GeForce RTX 4060 and 4070 GPUs based onthe NVIDIA Ada Lovelace architecture.We announced NVIDIA Avatar Cloud Engine for Games, a custom AI model foundry service using AI-powered naturallanguage interactions to transform games and launched DLSS 3.5 Ray Reconstruction.Additionally, we released TensorRT-LLM for Windows and launchedGeForce RTX 40-Series SUPER GPUs.Gaming reached a milestone of 500 AI-powered RTX games and applications utilizing NVIDIA DLSS, ray tracing andother NVIDIA RTX technologies.Professional Visualization revenue for fiscal year 2024 was $1.6 billion, up 1% from fiscal year 2023.In Professional Visualization, we announced new GPUsbased on the NVIDIA RTX Ada Lovelace architecture, and announced NVIDIA Omniverse Cloud, a fully managed service running in Microsoft Azure, for thedevelopment and deployment of industrial metaverse applications.Automotive revenue for fiscal year 2024 was $1.1 billion, up 21% from fiscal year 2023.In Automotive, we announced a partnership with MediaTek, which willdevelop mainstream automotive systems on chips for global OEMs integrating a new NVIDIA GPU chiplet IP for AI and graphics.We furthered our collaborationwith Foxconn to develop next-generation36electric vehicles, and announced further adoption of NVIDIA DRIVE platform with BYD, XPENG, GWM, Li Auto, ZEEKR and Xiaomi.Critical Accounting EstimatesOur consolidated financial statements are prepared in accordance with accounting principles generally accepted in the United States, or U.S.GAAP.Thepreparation of these financial statements requires us to make estimates and judgments that affect the reported amounts of assets, liabilities, revenue, cost ofrevenue, expenses and related disclosure of contingencies.Critical accounting estimates are those estimates that involve a significant level of estimationuncertainty and could have a material impact on our financial condition or results of operations.We have critical accounting estimates in the areas of inventories,revenue recognition, and income taxes.Refer to Note 1 of the Notes to the Consolidated Financial Statements in Part IV, Item 15 of this Annual Report on Form10-K for a summary of significant accounting policies.InventoriesWe charge cost of sales for inventory provisions to write-down our inventory to the lower of cost or net realizable value or for obsolete or excess inventory, and", "page_start": 36, "page_end": 37, "section_title": "NVIDIA CORPORATION", "score": 0.6500416}, {"chunk_id": "c69190e8-be56-40d7-9e3f-a1b1a8224354", "text": "Estimates for customer program accrualsinclude a combination of historical attainment and claim rates and may be adjusted based on relevant internal and external factors.License and Development ArrangementsRevenue from License and Development Arrangements is recognized over the period in which the development services are performed.Each fiscal reportingperiod, we measure progress to completion based on actual cost incurred to date as a percentage of the estimated total cost required to complete each project.Estimated total cost for each project includes a forecast of internal engineer personnel time expected to be incurred and other third-party costs as applicable.Contracts with Multiple Performance ObligationsOur contracts may contain more than one performance obligation.Judgement is required in determining whether each performance obligation within a customercontract is distinct.Except for License and Development Arrangements, NVIDIA products and services function on a standalone basis and do not require asignificant amount of integration or interdependency.Therefore, multiple performance obligations contained within a customer contract are considered distinctand are not combined for revenue recognition purposes.We allocate the total transaction price to each distinct performance obligation in a multiple performance obligations arrangement on a relative standalone sellingprice basis.In certain cases, we can establish standalone selling price based on directly observable prices of products or services sold separately in comparablecircumstances to similar customers.If standalone selling price is not directly observable, such as when we do not sell a product or service separately, wedetermine standalone selling price based on market data and other observable inputs.Change in Accounting EstimateIn February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of a majority of the server, storage, and network equipment from three years to a range of four to five years, and assembly and testequipment from five years to seven years.The estimated effect of this change for fiscal year 2024 was a benefit of $33 million and $102 million for cost ofrevenue and operating expenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or$0.05 per both basic and diluted share.Results of OperationsA discussion regarding our financial condition and results of operations for fiscal year 2024 compared to fiscal year 2023 is presented below.A discussionregarding our financial condition and results of operations for fiscal year 2023 compared to fiscal year 2022 can be found under Item 7 in our Annual Report onForm 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38", "page_start": 38, "page_end": 38, "section_title": "NVIDIA CORPORATION", "score": 0.6424212}, {"chunk_id": "e01eee79-fb51-4750-97d0-f13e01619fcb", "text": "Reportable segment operating income – The year-on-year increase in Compute & Networking and Graphics operating income was driven by higher revenue.39All Other operating loss - The year-on-year decrease was due to the $1.4 billion Arm acquisition termination cost in fiscal year 2023, partially offset by a $839million increase in stock-based compensation expense in fiscal year 2024.Concentration of RevenueRevenue by geographic region is designated based on the billing location even if the revenue may be attributable to end customers, such as enterprises andgamers in a different location.Revenue from sales to customers outside of the United States accounted for 56% and 69% of total revenue for fiscal years 2024and 2023, respectively.Our direct and indirect customers include public cloud, consumer internet companies, enterprises, startups, public sector entities, OEMs, ODMs, systemintegrators, AIB, and distributors.Sales to one customer, Customer A, represented 13% of total revenue for fiscal year 2024, which was attributable to the Compute & Networking segment.One indirect customer which primarily purchases our products through system integrators and distributors, including through Customer A, is estimated to haverepresented approximately 19% of total revenue for fiscal year 2024, attributable to the Compute & Networking segment.Our estimated Compute & Networking demand is expected to remain concentrated.There were no customers with 10% or more of total revenue for fiscal years 2023 and 2022.Gross Profit and Gross MarginGross profit consists of total revenue, net of allowances, less cost of revenue.Cost of revenue consists primarily of the cost of semiconductors, including waferfabrication, assembly, testing and packaging, board and device costs, manufacturing support costs, including labor and overhead associated with suchpurchases, final test yield fallout, inventory and warranty provisions, memory and component costs, tariffs, and shipping costs.Cost of revenue also includesacquisition-related costs, development costs for license and service arrangements, IP-related costs, and stock-based compensation related to personnelassociated with manufacturing operations.Our overall gross margin increased to 72.7% in fiscal year 2024 from 56.9% in fiscal year 2023.The year over year increase was primarily due to strong DataCenter revenue growth of 217% and lower net inventory provisions as a percentage of revenue.Provisions for inventory and excess inventory purchase obligations totaled $2.2 billion for both fiscal years 2024 and 2023.Sales of previously reservedinventory or settlements of excess inventory purchase obligations resulted in a provision release of $540 million and $137 million for fiscal years 2024 and 2023,respectively.The net effect on our gross margin was an unfavorable impact of 2.7% and 7.5% in fiscal years 2024 and 2023, respectively.Operating ExpensesYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Research and development expenses$8,675$7,339$1,33618 %% of net revenue14.2 %27.2 %Sales, general and administrative expenses2,654", "page_start": 39, "page_end": 40, "section_title": "NVIDIA CORPORATION", "score": 0.64118224}], "comparative_nvidia_02": [{"chunk_id": "823438dd-9ce8-4ae9-924a-70c6bbf5121f", "text": "Form 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38The following table sets forth, for the periods indicated, certain items in our Consolidated Statements of Income expressed as a percentage of revenue.Year EndedJan 28, 2024Jan 29, 2023Revenue100.0 %100.0 %Cost of revenue27.343.1Gross profit72.756.9Operating expensesResearch and development14.227.2Sales, general and administrative4.49.1Acquisition termination cost—5.0Total operating expenses18.641.3Operating income54.115.6Interest income1.41.0Interest expense(0.4)(1.0)0.4(0.1)Other, net1.4(0.1)Other income (expense), netIncome before income tax55.515.5Income tax expense (benefit)6.6(0.7)48.9 %16.2 %Net incomeReportable SegmentsRevenue by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$47,405$15,068$32,337215 %13,51711,9061,611Graphics14 %$60,922$26,974$33,948Total126 %Operating Income by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$32,016$5,083$26,933530 %Graphics5,8464,5521,29428 %(4,890)(5,411)521All Other(10)%$32,972$4,224$28,748Total681 %Compute & Networking revenue – The year-on-year increase was due to higher Data Center revenue.Compute grew 266% due to higher shipments of theNVIDIA Hopper GPU computing platform for the training and inference of LLMs, recommendation engines and generative AI applications.Networking was up133% due to higher shipments of InfiniBand.Graphics revenue – The year-on-year increase was led by growth in Gaming of 15% driven by higher sell-in to partners following the normalization of channelinventory levels.Reportable segment operating income – The year-on-year increase in Compute & Networking and Graphics operating income was driven by higher revenue.39All Other operating loss - The year-on-year decrease was due to the $1.4 billion Arm acquisition termination cost in fiscal year 2023, partially offset by a $839million increase in stock-based compensation expense in fiscal year 2024.Concentration of Revenue", "page_start": 38, "page_end": 40, "section_title": "NVIDIA CORPORATION", "score": 0.7384145}, {"chunk_id": "ddc05d6e-79f6-4ed4-a29b-876d9e76fd48", "text": "We did not experience any significant impact or expense toour business; however, if the conflict is further extended, it could impact future product development, operations, and revenue or create other uncertainty for ourbusiness.35Fiscal Year 2024 SummaryYear EndedJan 28, 2024Jan 29, 2023Change($ in millions, except per share data)Revenue$60,922$26,974Up 126%Gross margin72.7 %56.9 %Up 15.8 ptsOperating expenses$11,329$11,132Up 2%Operating income$32,972$4,224Up 681%Net income$29,760$4,368Up 581%Net income per diluted share$11.93$1.74Up 586%We specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Revenue for fiscal year 2024 was $60.9 billion, up 126% from a year ago.Data Center revenue for fiscal year 2024 was up 217%.Strong demand was driven by enterprise software and consumer internet applications, and multipleindustry verticals including automotive, financial services, and healthcare.Customers across industry verticals access NVIDIA AI infrastructure both through thecloud and on-premises.Data Center compute revenue was up 244% in the fiscal year.Networking revenue was up 133% in the fiscal year.Gaming revenue for fiscal year 2024 was up 15%.The increase reflects higher sell-in to partners following the normalization of channel inventory levels andgrowing demand.Professional Visualization revenue for fiscal year 2024 was up 1%.Automotive revenue for the fiscal year 2024 was up 21%.The increase primarily reflected growth in self-driving platforms.Gross margin increased in fiscal year 2024, primarily driven by Data Center revenue growth and lower net inventory provisions as a percentage of revenue.Operating expenses increased for fiscal year 2024, driven by growth in employees and compensation increases.Fiscal year 2023 also included a $1.4 billionacquisition termination charge related to the proposed Arm transaction.Market Platform HighlightsData Center revenue for fiscal year 2024 was $47.5 billion, up 217% from fiscal year 2023.In Data Center, we launched AI inference platforms that combine ourfull-stack inference software with NVIDIA Ada, NVIDIA Hopper and NVIDIA Grace Hopper processors optimized for generative AI, LLMs and other AI workloads.We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.", "page_start": 35, "page_end": 36, "section_title": "NVIDIA CORPORATION", "score": 0.7154747}, {"chunk_id": "c69190e8-be56-40d7-9e3f-a1b1a8224354", "text": "Estimates for customer program accrualsinclude a combination of historical attainment and claim rates and may be adjusted based on relevant internal and external factors.License and Development ArrangementsRevenue from License and Development Arrangements is recognized over the period in which the development services are performed.Each fiscal reportingperiod, we measure progress to completion based on actual cost incurred to date as a percentage of the estimated total cost required to complete each project.Estimated total cost for each project includes a forecast of internal engineer personnel time expected to be incurred and other third-party costs as applicable.Contracts with Multiple Performance ObligationsOur contracts may contain more than one performance obligation.Judgement is required in determining whether each performance obligation within a customercontract is distinct.Except for License and Development Arrangements, NVIDIA products and services function on a standalone basis and do not require asignificant amount of integration or interdependency.Therefore, multiple performance obligations contained within a customer contract are considered distinctand are not combined for revenue recognition purposes.We allocate the total transaction price to each distinct performance obligation in a multiple performance obligations arrangement on a relative standalone sellingprice basis.In certain cases, we can establish standalone selling price based on directly observable prices of products or services sold separately in comparablecircumstances to similar customers.If standalone selling price is not directly observable, such as when we do not sell a product or service separately, wedetermine standalone selling price based on market data and other observable inputs.Change in Accounting EstimateIn February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of a majority of the server, storage, and network equipment from three years to a range of four to five years, and assembly and testequipment from five years to seven years.The estimated effect of this change for fiscal year 2024 was a benefit of $33 million and $102 million for cost ofrevenue and operating expenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or$0.05 per both basic and diluted share.Results of OperationsA discussion regarding our financial condition and results of operations for fiscal year 2024 compared to fiscal year 2023 is presented below.A discussionregarding our financial condition and results of operations for fiscal year 2023 compared to fiscal year 2022 can be found under Item 7 in our Annual Report onForm 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38", "page_start": 38, "page_end": 38, "section_title": "NVIDIA CORPORATION", "score": 0.6573293}, {"chunk_id": "0dd7fcd2-abcb-4469-bfdc-d6d5c6cb8eae", "text": "*$100 invested on 1/27/19 in stock and in indices, including reinvestment of dividends.Source: FactSet financial data and analytics.1/27/20191/26/20201/31/20211/30/20221/29/20231/28/2024NVIDIA Corporation$100.00$157.02$326.26$574.15$512.40$1,536.28S&P 500$100.00$126.17$144.83$175.25$163.63$199.83Nasdaq 100$100.00$136.15$194.20$218.68$185.67$268.13Item 6.[Reserved]33Item 7.Management's Discussion and Analysis of Financial Condition and Results of OperationsThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with “Item 1A.Risk Factors”, ourConsolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described elsewhere in this Annual Report onForm 10-K, before deciding to purchase, hold or sell shares of our common stock.OverviewOur Company and Our BusinessesNVIDIA pioneered accelerated computing to help solve the most challenging computational problems.Since our original focus on PC graphics, we haveexpanded to several other large and important computationally intensive fields.NVIDIA has leveraged its GPU architecture to create platforms for acceleratedcomputing, AI solutions, scientific computing, data science, AV, robotics, metaverse and 3D internet applications.Our two operating segments are \"Compute & Networking\" and \"Graphics.\" Refer to Note 17 of the Notes to the Consolidated Financial Statements in Part IV,Item 15 of this Annual Report on Form 10-K for additional information.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Recent Developments, Future Objectives and ChallengesDemand and Supply, Product Transitions, and New Products and Business ModelsDemand for our data center systems and products surged in fiscal year 2024.Entering fiscal year 2025, we are gathering customer demand indications acrossseveral product transitions.We have demand visibility for our new data center products ramping later in fiscal year 2025.We have increased our supply andcapacity purchases with existing suppliers, added new vendors and entered into prepaid manufacturing and capacity agreements.These increased purchasevolumes, the number of suppliers, and the integration of new vendors into our supply chain may create more complexity and execution risk.Our purchasecommitments and obligations for inventory and manufacturing capacity at the end of fiscal year 2024 were impacted by shortening lead times for certaincomponents.We may continue to enter into new supplier and capacity arrangements.Supply of Hopper architecture products is improving, and demand remainsvery strong.We expect our next-generation products to be supply-constrained based upon demand indications.", "page_start": 33, "page_end": 34, "section_title": "NVIDIA CORPORATION", "score": 0.65607566}, {"chunk_id": "f2207e14-4c6c-421b-849b-76359737cca1", "text": "Germany, India, Israel, and Taiwan for fiscal years 2005 through 2023.Note 15 - Shareholders’ EquityCapital Return ProgramIn August 2023, our Board of Directors approved an increase to our share repurchase program of an additional $25.0 billion, without expiration.During fiscalyear 2024, we repurchased 21 million shares of our common stock for $9.7 billion.As of January 28, 2024, we were authorized, subject to certain specifications,to repurchase additional shares of our common stock up to $22.5 billion.From January 29, 2024 through February 16, 2024, we repurchased 2.8 million sharesfor $1.9 billion pursuant to a Rule 10b5-1 trading plan.Our share repurchase program aims to offset dilution from shares issued to employees.We may pursueadditional share repurchases as we weigh market factors and other investment opportunities.During fiscal years 2024, 2023, and 2022, we paid $395 million, $398 million, and $399 million in cash dividends to our shareholders, respectively.Our cashdividend program and the payment of future cash dividends under that program are subject to our Board of Directors' continuing determination that the dividendprogram and the declaration of dividends thereunder are in the best interests of our shareholders.In fiscal year 2022, we retired our existing 349 million treasury shares.These shares assumed the status of authorized and unissued shares upon retirement.The excess of repurchase price over par value was allocated between additional paid-in capital and retained earnings, resulting in a reduction in additional paidin capital by $20 million and retained earnings by $12.0 billion.Any future repurchased shares will assume the status of authorized and unissued shares.77NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Note 16 - Employee Retirement PlansWe provide tax-qualified defined contribution plans to eligible employees in the U.S.and certain other countries.Our contribution expense for fiscal years 2024,2023, and 2022 was $255 million, $227 million, and $168 million, respectively.Note 17 - Segment InformationOur Chief Executive Officer, who is considered to be our chief operating decision maker, or CODM, reviews financial information presented on an operatingsegment basis for purposes of making decisions and assessing financial performance.The Compute & Networking segment includes our Data Center accelerated computing platform; networking; automotive artificial intelligence, or AI, Cockpit,autonomous driving development agreements, and autonomous vehicle solutions; electric vehicle computing platforms; Jetson for robotics and other embeddedplatforms; NVIDIA AI Enterprise and other software; and DGX Cloud.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions forgaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across our", "page_start": 77, "page_end": 78, "section_title": "NVIDIA CORPORATION", "score": 0.64287454}, {"chunk_id": "b54b5cad-85c6-477b-a80b-27989017a705", "text": "Fair value is based upon observable inputs in an inactivemarket and the valuation requires our judgment due to the absence of market prices and inherent lack of liquidity.All gains and losses on these investments,realized and unrealized, are recognized in other income (expense), net on our Consolidated Statements of Income.We assess whether an impairment loss has occurred on our investments in non-marketable equity securities, accounted for under the measurement alternativebased on quantitative and qualitative factors.If any impairment is identified for non-marketable equity securities, we write down the investment to its fair valueand record the corresponding charge through other income (expense), net on our Consolidated Statements of Income.59NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Recently Issued Accounting PronouncementsRecent Accounting Pronouncements Not Yet AdoptedIn November 2023, the Financial Accounting Standards Board, or FASB, issued a new accounting standard to provide for additional disclosures about significantexpenses in operating segments.The standard is effective for our annual reporting for fiscal year 2025 and for interim period reporting starting in fiscal year 2026retrospectively.We are currently evaluating the impact of this standard on our Consolidated Financial Statements.In December 2023, the FASB issued a new accounting standard which provides for new and changes to income tax disclosures including disaggregation of therate reconciliation and income taxes paid disclosures.The amendments in the standard are effective for annual periods beginning after December 15, 2024.Early adoption is permitted and should be applied prospectively, with retrospective application permitted.We expect to adopt this standard in our annual periodbeginning fiscal year 2026.We are currently evaluating the impact of this standard on our Consolidated Financial Statements.Note 2 - Business CombinationTermination of the Arm Share Purchase AgreementIn February 2022, NVIDIA and SoftBank Group Corp, or SoftBank, announced the termination of the Share Purchase Agreement whereby NVIDIA would haveacquired Arm from SoftBank.The parties agreed to terminate it due to significant regulatory challenges preventing the completion of the transaction.Werecorded an acquisition termination cost of $1.4 billion in fiscal year 2023 reflecting the write-off of the prepayment provided at signing.Note 3 - LeasesOur lease obligations primarily consist of operating leases for our headquarters complex, domestic and international office facilities, and data center space, withlease periods expiring between fiscal years 2025 and 2035.Futureminimumleasepaymentsunderournon-cancelableoperatingleasesasofJanuary28,2024,areasfollows:Operating Lease Obligations(In millions)Fiscal Year:2025$29020262702027253202823620292022030 and thereafter288Total1,539192Less imputed interestPresent value of net future minimum lease payments1,347228", "page_start": 59, "page_end": 60, "section_title": "NVIDIA CORPORATION", "score": 0.6316987}, {"chunk_id": "16846335-f2af-4650-bf60-d4b5485698d4", "text": "Principles of ConsolidationOur consolidated financial statements include the accounts of NVIDIA Corporation and our wholly-owned subsidiaries.All intercompany balances andtransactions have been eliminated in consolidation.Use of EstimatesThe preparation of financial statements in conformity with U.S.GAAP requires management to make estimates and assumptions that affect the reportedamounts of assets and liabilities and disclosures of contingent assets and liabilities at the date of the financial statements and the reported amounts of revenueand expenses during the reporting period.Actual results could differ materially from our estimates.On an on-going basis, we evaluate our estimates, includingthose related to revenue recognition, cash equivalents and marketable securities, accounts receivable, inventories and product purchase commitments, incometaxes, goodwill, stock-based compensation, litigation, investigation and settlement costs, restructuring and other charges, property, plant, and equipment, andother contingencies.These estimates are based on historical facts and various other assumptions that we believe are reasonable.In February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of most of our server, storage, and network equipment from three to four or five years, and our assembly and test equipment from five toseven years.The effect of this change for the fiscal year ended January 28, 2024 was a benefit of $33 million and $102 million for cost of revenue and operatingexpenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or $0.05 per both basic anddiluted share.Revenue RecognitionWe derive our revenue from product sales, including hardware and systems, license and development arrangements, software licensing, and cloud services.Wedetermine revenue recognition through the following steps: (1) identification of the contract with a customer; (2) identification of the performance obligations inthe contract; (3) determination of the transaction price; (4) allocation of the transaction price to the performance obligations in the contract (where revenue isallocated on a relative standalone selling price basis by maximizing the use of observable inputs to determine the standalone selling price for each performanceobligation); and (5) recognition of revenue when, or as, we satisfy a performance obligation.Product Sales RevenueRevenue from product sales is recognized upon transfer of control of products to customers in an amount that reflects the consideration we expect to receive inexchange for those products.Certain products are sold with support or an extended warranty for the incorporated system, hardware, and/or software.Supportand extended warranty revenue are recognized ratably over the service period, or as services are performed.Revenue is recognized net of allowances forreturns, customer programs and any taxes collected from customers.For products sold with a right of return, we record a reduction to revenue by establishing a sales return allowance for estimated product returns at the timerevenue is recognized, based primarily on historical return rates.", "page_start": 55, "page_end": 55, "section_title": "NVIDIA CORPORATION", "score": 0.6312121}, {"chunk_id": "8d7f8306-8d55-40f5-b383-f4914775e7ee", "text": "(10)Restructuring costs and other—(54)—Acquisition termination cost—(1,353)—10(2)—Other$(4,890)$(5,411)$(3,049)TotalRevenue by geographic areas is designated based upon the billing location of the customer.End customer location may be different than our customer’s billinglocation.Revenue by geographic areas was as follows:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Revenue:(In millions)United States$26,966$8,292$4,349Taiwan13,4056,9868,544China (including Hong Kong)10,3065,7857,11110,2455,9116,910Other countries$60,922$26,974$26,914Total revenueRevenue from sales to customers outside of the United States accounted for 56%, 69%, and 84% of total revenue for fiscal years 2024, 2023, and 2022,respectively.The increase in revenue to the United States for fiscal year 2024 was primarily due to higher U.S.-based Compute & Networking segment demand.Sales to one customer represented 13% of total revenue for fiscal year 2024, which was attributable to the Compute & Networking segment.No customerrepresented 10% or more of total revenue for fiscal years 2023 and 2022.The following table summarizes information pertaining to our revenue by each of the specialized markets we serve:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Revenue:(In millions)Data Center$47,525$15,005$10,613Gaming10,4479,06712,462Professional Visualization1,5531,5442,111Automotive1,0919035663064551,162OEM and Other$60,922$26,974$26,914Total revenue79NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)The following table presents summarized information for long-lived assets by country.Long-lived assets consist of property and equipment and exclude otherassets, operating lease assets, goodwill, and intangible assets.Jan 28, 2024Jan 29, 2023Long-lived assets:(In millions)United States$2,595$2,587Taiwan773702Israel325283Other countries221235$3,914$3,807Total long-lived assets80NVIDIA Corporation and SubsidiariesSchedule II – Valuation and Qualifying AccountsBalance atBeginning ofBalance atDescriptionPeriodAdditionsDeductionsEnd of Period(In millions)Fiscal year 2024Allowance for doubtful accounts$4$— (1)$— (1)$4Sales return allowance$26$213 (2)$", "page_start": 79, "page_end": 81, "section_title": "NVIDIA CORPORATION", "score": 0.6282964}], "comparative_nvidia_03": [{"chunk_id": "823438dd-9ce8-4ae9-924a-70c6bbf5121f", "text": "Form 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38The following table sets forth, for the periods indicated, certain items in our Consolidated Statements of Income expressed as a percentage of revenue.Year EndedJan 28, 2024Jan 29, 2023Revenue100.0 %100.0 %Cost of revenue27.343.1Gross profit72.756.9Operating expensesResearch and development14.227.2Sales, general and administrative4.49.1Acquisition termination cost—5.0Total operating expenses18.641.3Operating income54.115.6Interest income1.41.0Interest expense(0.4)(1.0)0.4(0.1)Other, net1.4(0.1)Other income (expense), netIncome before income tax55.515.5Income tax expense (benefit)6.6(0.7)48.9 %16.2 %Net incomeReportable SegmentsRevenue by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$47,405$15,068$32,337215 %13,51711,9061,611Graphics14 %$60,922$26,974$33,948Total126 %Operating Income by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$32,016$5,083$26,933530 %Graphics5,8464,5521,29428 %(4,890)(5,411)521All Other(10)%$32,972$4,224$28,748Total681 %Compute & Networking revenue – The year-on-year increase was due to higher Data Center revenue.Compute grew 266% due to higher shipments of theNVIDIA Hopper GPU computing platform for the training and inference of LLMs, recommendation engines and generative AI applications.Networking was up133% due to higher shipments of InfiniBand.Graphics revenue – The year-on-year increase was led by growth in Gaming of 15% driven by higher sell-in to partners following the normalization of channelinventory levels.Reportable segment operating income – The year-on-year increase in Compute & Networking and Graphics operating income was driven by higher revenue.39All Other operating loss - The year-on-year decrease was due to the $1.4 billion Arm acquisition termination cost in fiscal year 2023, partially offset by a $839million increase in stock-based compensation expense in fiscal year 2024.Concentration of Revenue", "page_start": 38, "page_end": 40, "section_title": "NVIDIA CORPORATION", "score": 0.70062804}, {"chunk_id": "ddc05d6e-79f6-4ed4-a29b-876d9e76fd48", "text": "We did not experience any significant impact or expense toour business; however, if the conflict is further extended, it could impact future product development, operations, and revenue or create other uncertainty for ourbusiness.35Fiscal Year 2024 SummaryYear EndedJan 28, 2024Jan 29, 2023Change($ in millions, except per share data)Revenue$60,922$26,974Up 126%Gross margin72.7 %56.9 %Up 15.8 ptsOperating expenses$11,329$11,132Up 2%Operating income$32,972$4,224Up 681%Net income$29,760$4,368Up 581%Net income per diluted share$11.93$1.74Up 586%We specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Revenue for fiscal year 2024 was $60.9 billion, up 126% from a year ago.Data Center revenue for fiscal year 2024 was up 217%.Strong demand was driven by enterprise software and consumer internet applications, and multipleindustry verticals including automotive, financial services, and healthcare.Customers across industry verticals access NVIDIA AI infrastructure both through thecloud and on-premises.Data Center compute revenue was up 244% in the fiscal year.Networking revenue was up 133% in the fiscal year.Gaming revenue for fiscal year 2024 was up 15%.The increase reflects higher sell-in to partners following the normalization of channel inventory levels andgrowing demand.Professional Visualization revenue for fiscal year 2024 was up 1%.Automotive revenue for the fiscal year 2024 was up 21%.The increase primarily reflected growth in self-driving platforms.Gross margin increased in fiscal year 2024, primarily driven by Data Center revenue growth and lower net inventory provisions as a percentage of revenue.Operating expenses increased for fiscal year 2024, driven by growth in employees and compensation increases.Fiscal year 2023 also included a $1.4 billionacquisition termination charge related to the proposed Arm transaction.Market Platform HighlightsData Center revenue for fiscal year 2024 was $47.5 billion, up 217% from fiscal year 2023.In Data Center, we launched AI inference platforms that combine ourfull-stack inference software with NVIDIA Ada, NVIDIA Hopper and NVIDIA Grace Hopper processors optimized for generative AI, LLMs and other AI workloads.We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.", "page_start": 35, "page_end": 36, "section_title": "NVIDIA CORPORATION", "score": 0.6581414}, {"chunk_id": "c69190e8-be56-40d7-9e3f-a1b1a8224354", "text": "Estimates for customer program accrualsinclude a combination of historical attainment and claim rates and may be adjusted based on relevant internal and external factors.License and Development ArrangementsRevenue from License and Development Arrangements is recognized over the period in which the development services are performed.Each fiscal reportingperiod, we measure progress to completion based on actual cost incurred to date as a percentage of the estimated total cost required to complete each project.Estimated total cost for each project includes a forecast of internal engineer personnel time expected to be incurred and other third-party costs as applicable.Contracts with Multiple Performance ObligationsOur contracts may contain more than one performance obligation.Judgement is required in determining whether each performance obligation within a customercontract is distinct.Except for License and Development Arrangements, NVIDIA products and services function on a standalone basis and do not require asignificant amount of integration or interdependency.Therefore, multiple performance obligations contained within a customer contract are considered distinctand are not combined for revenue recognition purposes.We allocate the total transaction price to each distinct performance obligation in a multiple performance obligations arrangement on a relative standalone sellingprice basis.In certain cases, we can establish standalone selling price based on directly observable prices of products or services sold separately in comparablecircumstances to similar customers.If standalone selling price is not directly observable, such as when we do not sell a product or service separately, wedetermine standalone selling price based on market data and other observable inputs.Change in Accounting EstimateIn February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of a majority of the server, storage, and network equipment from three years to a range of four to five years, and assembly and testequipment from five years to seven years.The estimated effect of this change for fiscal year 2024 was a benefit of $33 million and $102 million for cost ofrevenue and operating expenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or$0.05 per both basic and diluted share.Results of OperationsA discussion regarding our financial condition and results of operations for fiscal year 2024 compared to fiscal year 2023 is presented below.A discussionregarding our financial condition and results of operations for fiscal year 2023 compared to fiscal year 2022 can be found under Item 7 in our Annual Report onForm 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38", "page_start": 38, "page_end": 38, "section_title": "NVIDIA CORPORATION", "score": 0.65293324}, {"chunk_id": "f2207e14-4c6c-421b-849b-76359737cca1", "text": "Germany, India, Israel, and Taiwan for fiscal years 2005 through 2023.Note 15 - Shareholders’ EquityCapital Return ProgramIn August 2023, our Board of Directors approved an increase to our share repurchase program of an additional $25.0 billion, without expiration.During fiscalyear 2024, we repurchased 21 million shares of our common stock for $9.7 billion.As of January 28, 2024, we were authorized, subject to certain specifications,to repurchase additional shares of our common stock up to $22.5 billion.From January 29, 2024 through February 16, 2024, we repurchased 2.8 million sharesfor $1.9 billion pursuant to a Rule 10b5-1 trading plan.Our share repurchase program aims to offset dilution from shares issued to employees.We may pursueadditional share repurchases as we weigh market factors and other investment opportunities.During fiscal years 2024, 2023, and 2022, we paid $395 million, $398 million, and $399 million in cash dividends to our shareholders, respectively.Our cashdividend program and the payment of future cash dividends under that program are subject to our Board of Directors' continuing determination that the dividendprogram and the declaration of dividends thereunder are in the best interests of our shareholders.In fiscal year 2022, we retired our existing 349 million treasury shares.These shares assumed the status of authorized and unissued shares upon retirement.The excess of repurchase price over par value was allocated between additional paid-in capital and retained earnings, resulting in a reduction in additional paidin capital by $20 million and retained earnings by $12.0 billion.Any future repurchased shares will assume the status of authorized and unissued shares.77NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Note 16 - Employee Retirement PlansWe provide tax-qualified defined contribution plans to eligible employees in the U.S.and certain other countries.Our contribution expense for fiscal years 2024,2023, and 2022 was $255 million, $227 million, and $168 million, respectively.Note 17 - Segment InformationOur Chief Executive Officer, who is considered to be our chief operating decision maker, or CODM, reviews financial information presented on an operatingsegment basis for purposes of making decisions and assessing financial performance.The Compute & Networking segment includes our Data Center accelerated computing platform; networking; automotive artificial intelligence, or AI, Cockpit,autonomous driving development agreements, and autonomous vehicle solutions; electric vehicle computing platforms; Jetson for robotics and other embeddedplatforms; NVIDIA AI Enterprise and other software; and DGX Cloud.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions forgaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across our", "page_start": 77, "page_end": 78, "section_title": "NVIDIA CORPORATION", "score": 0.63286006}, {"chunk_id": "8d7f8306-8d55-40f5-b383-f4914775e7ee", "text": "(10)Restructuring costs and other—(54)—Acquisition termination cost—(1,353)—10(2)—Other$(4,890)$(5,411)$(3,049)TotalRevenue by geographic areas is designated based upon the billing location of the customer.End customer location may be different than our customer’s billinglocation.Revenue by geographic areas was as follows:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Revenue:(In millions)United States$26,966$8,292$4,349Taiwan13,4056,9868,544China (including Hong Kong)10,3065,7857,11110,2455,9116,910Other countries$60,922$26,974$26,914Total revenueRevenue from sales to customers outside of the United States accounted for 56%, 69%, and 84% of total revenue for fiscal years 2024, 2023, and 2022,respectively.The increase in revenue to the United States for fiscal year 2024 was primarily due to higher U.S.-based Compute & Networking segment demand.Sales to one customer represented 13% of total revenue for fiscal year 2024, which was attributable to the Compute & Networking segment.No customerrepresented 10% or more of total revenue for fiscal years 2023 and 2022.The following table summarizes information pertaining to our revenue by each of the specialized markets we serve:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Revenue:(In millions)Data Center$47,525$15,005$10,613Gaming10,4479,06712,462Professional Visualization1,5531,5442,111Automotive1,0919035663064551,162OEM and Other$60,922$26,974$26,914Total revenue79NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)The following table presents summarized information for long-lived assets by country.Long-lived assets consist of property and equipment and exclude otherassets, operating lease assets, goodwill, and intangible assets.Jan 28, 2024Jan 29, 2023Long-lived assets:(In millions)United States$2,595$2,587Taiwan773702Israel325283Other countries221235$3,914$3,807Total long-lived assets80NVIDIA Corporation and SubsidiariesSchedule II – Valuation and Qualifying AccountsBalance atBeginning ofBalance atDescriptionPeriodAdditionsDeductionsEnd of Period(In millions)Fiscal year 2024Allowance for doubtful accounts$4$— (1)$— (1)$4Sales return allowance$26$213 (2)$", "page_start": 79, "page_end": 81, "section_title": "NVIDIA CORPORATION", "score": 0.6172907}, {"chunk_id": "d44f53d4-29ff-496c-a1d8-772d2a857abb", "text": "follows:Operating Lease Obligations(In millions)Fiscal Year:2025$29020262702027253202823620292022030 and thereafter288Total1,539192Less imputed interestPresent value of net future minimum lease payments1,347228Less short-term operating lease liabilities$1,119Long-term operating lease liabilitiesIn addition, we have operating leases, primarily for our data centers, that are expected to commence within fiscal year 2025 with lease terms of 1 to 10 years for$1.1 billion.Operating lease expenses for fiscal years 2024, 2023, and 2022 were $269 million, $193 million, $168 million, respectively.Short-term and variable leaseexpenses for fiscal years 2024, 2023, and 2022 were not significant.60NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Other information related to leases was as follows:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022(In millions)Supplemental cash flows informationOperating cash flows used for operating leases$286$184$154Operating lease assets obtained in exchange for leaseobligations$531$358$266As of January 28, 2024, our operating leases had a weighted average remaining lease term of 6.1 years and a weighted average discount rate of 3.76%.As ofJanuary 29, 2023, our operating leases had a weighted average remaining lease term of 6.8 years and a weighted average discount rate of 3.21%.Note 4 - Stock-Based CompensationOur stock-based compensation expense is associated with RSUs, performance stock units based on our corporate financial performance targets, or PSUs,performance stock units based on market conditions, or market-based PSUs, and our ESPP.Our Consolidated Statements of Income include stock-based compensation expense, net of amounts allocated to inventory, as follows:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022(In millions)Cost of revenue$141$138$141Research and development2,5321,8921,298876680565Sales, general and administrative$3,549$2,710$2,004TotalStock-based compensation capitalized in inventories was not significant during fiscal years 2024, 2023, and 2022.The following is a summary of equity awards granted under our equity incentive plans:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022(In millions, except per share data)RSUs, PSUs and Market-based PSUsAwards granted142518Estimated total grant-date fair value$5,316$4,505$3,492Weighted average grant-date fair value per share$374.08$183.72$190.69ESPPShares purchased335Weighted average price per share$158.07$122.54$56.36Weighted average grant-date fair value per share$69.90$51.87$23.24As of January 28, 2024, there was $8.", "page_start": 60, "page_end": 61, "section_title": "NVIDIA CORPORATION", "score": 0.61151505}, {"chunk_id": "0dd7fcd2-abcb-4469-bfdc-d6d5c6cb8eae", "text": "*$100 invested on 1/27/19 in stock and in indices, including reinvestment of dividends.Source: FactSet financial data and analytics.1/27/20191/26/20201/31/20211/30/20221/29/20231/28/2024NVIDIA Corporation$100.00$157.02$326.26$574.15$512.40$1,536.28S&P 500$100.00$126.17$144.83$175.25$163.63$199.83Nasdaq 100$100.00$136.15$194.20$218.68$185.67$268.13Item 6.[Reserved]33Item 7.Management's Discussion and Analysis of Financial Condition and Results of OperationsThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with “Item 1A.Risk Factors”, ourConsolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described elsewhere in this Annual Report onForm 10-K, before deciding to purchase, hold or sell shares of our common stock.OverviewOur Company and Our BusinessesNVIDIA pioneered accelerated computing to help solve the most challenging computational problems.Since our original focus on PC graphics, we haveexpanded to several other large and important computationally intensive fields.NVIDIA has leveraged its GPU architecture to create platforms for acceleratedcomputing, AI solutions, scientific computing, data science, AV, robotics, metaverse and 3D internet applications.Our two operating segments are \"Compute & Networking\" and \"Graphics.\" Refer to Note 17 of the Notes to the Consolidated Financial Statements in Part IV,Item 15 of this Annual Report on Form 10-K for additional information.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Recent Developments, Future Objectives and ChallengesDemand and Supply, Product Transitions, and New Products and Business ModelsDemand for our data center systems and products surged in fiscal year 2024.Entering fiscal year 2025, we are gathering customer demand indications acrossseveral product transitions.We have demand visibility for our new data center products ramping later in fiscal year 2025.We have increased our supply andcapacity purchases with existing suppliers, added new vendors and entered into prepaid manufacturing and capacity agreements.These increased purchasevolumes, the number of suppliers, and the integration of new vendors into our supply chain may create more complexity and execution risk.Our purchasecommitments and obligations for inventory and manufacturing capacity at the end of fiscal year 2024 were impacted by shortening lead times for certaincomponents.We may continue to enter into new supplier and capacity arrangements.Supply of Hopper architecture products is improving, and demand remainsvery strong.We expect our next-generation products to be supply-constrained based upon demand indications.", "page_start": 33, "page_end": 34, "section_title": "NVIDIA CORPORATION", "score": 0.6059818}, {"chunk_id": "fdb3b279-c598-48ea-9a50-2eb8b1158344", "text": "354Accrued payroll and related expenses675530Product warranty and return provisions415108Taxes payable296467Operating leases228176Unsettled share repurchases187117Licenses and royalties18214919969Other$6,682$4,120Total accrued and other current liabilities(1)In fiscal years 2024 and 2023, we recorded an expense of approximately $1.4 billion and $1.1 billion, respectively, in cost of revenue for inventory purchase obligations in excess of our current demandprojections, supplier charges and for penalties related to cancellations and underutilization.(2)Deferred revenue primarily includes customer advances and deferrals related to support for hardware and software, license and development arrangements, and cloud services.$233 million and$35 million of the balance in fiscal 2024 and 2023 respectively, related to customer advances.Jan 28, 2024Jan 29, 2023(In millions)Other Long-Term Liabilities:Income tax payable (1)$1,361$1,204Deferred income tax462247Deferred revenue (2)573218Licenses payable80181Other6563$2,541$1,913Total other long-term liabilities(1)Income tax payable is comprised of the long-term portion of the one-time transition tax payable, unrecognized tax benefits, and related interest and penalties.(2)Deferred revenue primarily includes deferrals related to support for hardware and software.Deferred RevenueThe following table shows the changes in deferred revenue during fiscal years 2024 and 2023.Jan 28, 2024Jan 29, 2023(In millions)Balance at beginning of period$572$502Deferred revenue additions during the period2,038830(1,273)(760)Revenue recognized during the period$1,337$572Balance at end of periodRevenue recognized during fiscal year 2024 that was included in deferred revenue as of January 29, 2023 was $338 million.Revenue recognized during fiscalyear 2023 that was included in deferred revenue as of January 30, 2022 was $282 million.Revenue related to remaining performance obligations represents the contracted license and development arrangements and support for hardware andsoftware.This includes deferred revenue currently recorded and amounts that will be70NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)invoiced in future periods.Revenue allocated to remaining performance obligations, which includes deferred revenue and amounts that will be invoiced andrecognized as revenue in future periods, was $1.1 billion as of January 28, 2024.We expect to recognize approximately 40% of this revenue over the nexttwelve months and the remainder thereafter.This excludes revenue related to performance obligations for contracts with a length of one year or less.Note 11 - Derivative Financial InstrumentsWe enter into foreign currency forward contracts to mitigate the impact of foreign currency exchange rate movements on our operating expenses.Thesecontracts are designated as cash flow hedges for hedge accounting treatment.Gains or losses on the contracts are recorded in accumulated other", "page_start": 70, "page_end": 71, "section_title": "NVIDIA CORPORATION", "score": 0.6004512}], "comparative_attention_01": [{"chunk_id": "160dd6c9-8965-4dcd-8d74-e0dd157c4fd1", "text": "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step number.We usedwarmup_steps = 4000.5.4RegularizationWe employ three types of regularization during training:7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on theEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.BLEUTraining Cost (FLOPs)ModelEN-DEEN-FREN-DEEN-FRByteNet [18]23.751.0 · 1020Deep-Att + PosUnk [39]39.22.3 · 10191.4 · 1020GNMT + RL [38]24.639.929.6 · 10181.5 · 1020ConvS2S [9]25.1640.462.0 · 10191.2 · 1020MoE [32]26.0340.568.0 · 1020Deep-Att + PosUnk Ensemble [39]40.41.8 · 10201.1 · 1021GNMT + RL Ensemble [38]26.3041.167.7 · 10191.2 · 102141.29ConvS2S Ensemble [9]26.363.3 · 1018Transformer (base model)27.338.12.3 · 101928.441.8Transformer (big)Residual DropoutWe apply dropout [33] to the output of each sub-layer, before it is added to thesub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and thepositional encodings in both the encoder and decoder stacks.For the base model, we use a rate ofPdrop = 0.1.During training, we employed label smoothing of value ϵls = 0.1 [36].ThisLabel Smoothinghurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6Results6.1Machine TranslationOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0BLEU, establishing a new state-of-the-art BLEU score of 28.4.The configuration of this model islisted in the bottom line of Table 3.Training took 3.5 days on 8 P100 GPUs.Even our base modelsurpasses all previously published models and ensembles, at a fraction of the training cost of any ofthe competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,outperforming all of the previously published single models, at less than 1/4 the training cost of theprevious state-of-the-art model.The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We", "page_start": 7, "page_end": 8, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.716509}, {"chunk_id": "914cfb0b-3d4a-4988-8392-851b64c13e57", "text": "The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.Weused beam search with a beam size of 4 and length penalty α = 0.6 [38].These hyperparameterswere chosen after experimentation on the development set.We set the maximum output length duringinference to input length + 50, but terminate early when possible [38].Table 2 summarizes our results and compares our translation quality and training costs to other modelarchitectures from the literature.We estimate the number of floating point operations used to train amodel by multiplying the training time, the number of GPUs used, and an estimate of the sustainedsingle-precision floating-point capacity of each GPU 5.6.2Model VariationsTo evaluate the importance of different components of the Transformer, we varied our base modelin different ways, measuring the change in performance on English-to-German translation on the5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the basemodel.All metrics are on the English-to-German translation development set, newstest2013.Listedperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared toper-word perplexities.trainPPLBLEUparamsNdmodeldffhdkdvPdropϵls×106steps(dev)(dev)base65122048864640.10.1100K4.9225.86515125125.2924.941281285.0025.5(A)1632324.9125.83216165.0125.4165.1625.158(B)325.0125.46026.1123.73645.1925.35084.8825.580(C)25632325.7524.52810241281284.6626.016810245.1225.45340964.7526.2900.05.7724.60.24.9525.5(D)0.04.6725.30.25.4725.7(E)positional embedding instead of sinusoids4.9225.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.", "page_start": 8, "page_end": 9, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.6963464}, {"chunk_id": "8d9a2784-c6bd-4675-a20b-80dd0948329f", "text": "25.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,keeping the amount of computation constant, as described in Section 3.2.2.While single-headattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality.Thissuggests that determining compatibility is not easy and that a more sophisticated compatibilityfunction than dot product may be beneficial.We further observe in rows (C) and (D) that, as expected,bigger models are better, and dropout is very helpful in avoiding over-fitting.In row (E) we replace oursinusoidal positional encoding with learned positional embeddings [9], and observe nearly identicalresults to the base model.6.3English Constituency ParsingTo evaluate if the Transformer can generalize to other tasks we performed experiments on Englishconstituency parsing.This task presents specific challenges: the output is subject to strong structuralconstraints and is significantly longer than the input.Furthermore, RNN sequence-to-sequencemodels have not been able to attain state-of-the-art results in small-data regimes [37].We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of thePenn Treebank [25], about 40K training sentences.We also trained it in a semi-supervised setting,using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences[37].We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokensfor the semi-supervised setting.We performed only a small number of experiments to select the dropout, both attention and residual(section 5.4), learning rates and beam size on the Section 22 development set, all other parametersremained unchanged from the English-to-German base translation model.During inference, we9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23of WSJ)ParserTrainingWSJ 23 F1Vinyals & Kaiser el al.(2014) [37]WSJ only, discriminative88.3Petrov et al.(2006) [29]WSJ only, discriminative90.4Zhu et al.(2013) [40]WSJ only, discriminative90.4Dyer et al.(2016) [8]WSJ only, discriminative91.7Transformer (4 layers)WSJ only, discriminative91.3Zhu et al.(2013) [40]semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised", "page_start": 9, "page_end": 10, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.5667906}, {"chunk_id": "0b373c99-8c0e-44bf-8fc7-f6b6b009590d", "text": "Provided proper attribution is provided, Google hereby grants permission toreproduce the tables and figures in this paper solely for use in journalistic orscholarly works.Ashish Vaswani∗Noam Shazeer∗Niki Parmar∗Jakob Uszkoreit∗Google BrainGoogle BrainGoogle ResearchGoogle Researchavaswani@google.comnoam@google.comnikip@google.comusz@google.comLlion Jones∗Aidan N.Gomez∗†Łukasz Kaiser∗Google ResearchUniversity of TorontoGoogle Brainlukaszkaiser@google.comllion@google.comaidan@cs.toronto.eduIllia Polosukhin∗‡illia.polosukhin@gmail.comAbstractThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder.The bestperforming models also connect the encoder and decoder through an attentionmechanism.We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely.Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring significantlyless time to train.Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU.On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.8 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature.We show that the Transformer generalizes well toother tasks by applying it successfully to English constituency parsing both withlarge and limited training data.∗Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and startedthe effort to evaluate this idea.Ashish, with Illia, designed and implemented the first Transformer models andhas been crucially involved in every aspect of this work.Noam proposed scaled dot-product attention, multi-headattention and the parameter-free position representation and became the other person involved in nearly everydetail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase andtensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, andefficient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of andimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks", "page_start": 1, "page_end": 2, "section_title": "", "score": 0.5228887}, {"chunk_id": "bb77d3cc-59f9-4b97-a21b-132b30215259", "text": "semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised92.1Transformer (4 layers)semi-supervised92.7Luong et al.(2015) [23]multi-task93.0Dyer et al.(2016) [8]generative93.3increased the maximum output length to input length + 300.We used a beam size of 21 and α = 0.3for both WSJ only and the semi-supervised setting.Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of theRecurrent Neural Network Grammar [8].In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.7ConclusionIn this work, we presented the Transformer, the first sequence transduction model based entirely onattention, replacing the recurrent layers most commonly used in encoder-decoder architectures withmulti-headed self-attention.For translation tasks, the Transformer can be trained significantly faster than architectures basedon recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014English-to-French translation tasks, we achieve a new state of the art.In the former task our bestmodel outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.Weplan to extend the Transformer to problems involving input and output modalities other than text andto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputssuch as images, audio and video.Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.AcknowledgementsWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitfulcomments, corrections and inspiration.References[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprintarXiv:1607.06450, 2016.[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointlylearning to align and translate.CoRR, abs/1409.0473, 2014.[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V.Le.Massive exploration of neuralmachine translation architectures.CoRR, abs/1703.03906, 2017.[4] Jianpeng Cheng, Li Dong, and Mirella Lapata.Long short-term memory-networks for machinereading.arXiv preprint arXiv:1601.06733, 2016.10[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,", "page_start": 10, "page_end": 11, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.5225286}, {"chunk_id": "6ecb21e7-c073-4d90-9d8a-41fdfda06afb", "text": "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,textual entailment and learning task-independent sentence representations [4, 27, 28, 22].End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering andlanguage modeling tasks [34].To the best of our knowledge, however, the Transformer is the first transduction model relyingentirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.In the following sections, we will describe the Transformer, motivateself-attention and discuss its advantages over models such as [17, 18] and [9].3Model ArchitectureMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequenceof continuous representations z = (z1, ..., zn).Given z, the decoder then generates an outputsequence (y1, ..., ym) of symbols one element at a time.At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next.2Figure 1: The Transformer - model architecture.The Transformer follows this overall architecture using stacked self-attention and point-wise, fullyconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,respectively.3.1Encoder and Decoder StacksThe encoder is composed of a stack of N = 6 identical layers.Each layer has twoEncoder:sub-layers.The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network.We employ a residual connection [11] around each ofthe two sub-layers, followed by layer normalization [1].That is, the output of each sub-layer isLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layeritself.To facilitate these residual connections, all sub-layers in the model, as well as the embeddinglayers, produce outputs of dimension dmodel = 512.The decoder is also composed of a stack of N = 6 identical layers.In addition to the twoDecoder:sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-headattention over the output of the encoder stack.Similar to the encoder, we employ residual connectionsaround each of the sub-layers, followed by layer normalization.We also modify the self-attentionsub-layer in the decoder stack to prevent positions from attending to subsequent positions.Thismasking, combined with fact that the output embeddings are offset by one position, ensures that the", "page_start": 2, "page_end": 3, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4137563}, {"chunk_id": "92be8a85-80ac-4327-a0b3-f2fff684bda4", "text": "the input sequence centered around the respective output position.This would increase the maximumpath length to O(n/r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k < n does not connect all pairs of input and outputpositions.Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest pathsbetween any two positions in the network.Convolutional layers are generally more expensive thanrecurrent layers, by a factor of k.Separable convolutions [6], however, decrease the complexityconsiderably, to O(k · n · d + n · d2).Even with k = n, however, the complexity of a separableconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,the approach we take in our model.As side benefit, self-attention could yield more interpretable models.We inspect attention distributionsfrom our models and present and discuss examples in the appendix.Not only do individual attentionheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntacticand semantic structure of the sentences.5TrainingThis section describes the training regime for our models.5.1Training Data and BatchingWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 millionsentence pairs.Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens.For English-French, we used the significantly larger WMT2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piecevocabulary [38].Sentence pairs were batched together by approximate sequence length.Each trainingbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000target tokens.5.2Hardware and ScheduleWe trained our models on one machine with 8 NVIDIA P100 GPUs.For our base models usingthe hyperparameters described throughout the paper, each training step took about 0.4 seconds.Wetrained the base models for a total of 100,000 steps or 12 hours.For our big models,(described on thebottom line of table 3), step time was 1.0 seconds.The big models were trained for 300,000 steps(3.5 days).5.3OptimizerWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9.We varied the learningrate over the course of training, according to the formula:lrate = d−0.5model · min(step_num−0.5, step_num · warmup_steps−1.5)(3)This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step number.We usedwarmup_steps = 4000.5.4RegularizationWe employ three types of regularization during training:7", "page_start": 7, "page_end": 7, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.41204855}, {"chunk_id": "5151352b-ae46-44f3-83b0-8b8748547941", "text": "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networksin particular, have been firmly established as state of the art approaches in sequence modeling andtransduction problems such as language modeling and machine translation [35, 2, 5].Numerousefforts have since continued to push the boundaries of recurrent language models and encoder-decoderarchitectures [38, 24, 15].Recurrent models typically factor computation along the symbol positions of the input and outputsequences.Aligning the positions to steps in computation time, they generate a sequence of hiddenstates ht, as a function of the previous hidden state ht−1 and the input for position t.This inherentlysequential nature precludes parallelization within training examples, which becomes critical at longersequence lengths, as memory constraints limit batching across examples.Recent work has achievedsignificant improvements in computational efficiency through factorization tricks [21] and conditionalcomputation [32], while also improving model performance in case of the latter.The fundamentalconstraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance inthe input or output sequences [2, 19].In all but a few cases [27], however, such attention mechanismsare used in conjunction with a recurrent network.In this work we propose the Transformer, a model architecture eschewing recurrence and insteadrelying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for significantly more parallelization and can reach a new state of the art intranslation quality after being trained for as little as twelve hours on eight P100 GPUs.2BackgroundThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic buildingblock, computing hidden representations in parallel for all input and output positions.In these models,the number of operations required to relate signals from two arbitrary input or output positions growsin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.This makesit more difficult to learn dependencies between distant positions [12].In the Transformer this isreduced to a constant number of operations, albeit at the cost of reduced effective resolution dueto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,", "page_start": 1, "page_end": 2, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4116003}], "comparative_attention_02": [{"chunk_id": "160dd6c9-8965-4dcd-8d74-e0dd157c4fd1", "text": "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step number.We usedwarmup_steps = 4000.5.4RegularizationWe employ three types of regularization during training:7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on theEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.BLEUTraining Cost (FLOPs)ModelEN-DEEN-FREN-DEEN-FRByteNet [18]23.751.0 · 1020Deep-Att + PosUnk [39]39.22.3 · 10191.4 · 1020GNMT + RL [38]24.639.929.6 · 10181.5 · 1020ConvS2S [9]25.1640.462.0 · 10191.2 · 1020MoE [32]26.0340.568.0 · 1020Deep-Att + PosUnk Ensemble [39]40.41.8 · 10201.1 · 1021GNMT + RL Ensemble [38]26.3041.167.7 · 10191.2 · 102141.29ConvS2S Ensemble [9]26.363.3 · 1018Transformer (base model)27.338.12.3 · 101928.441.8Transformer (big)Residual DropoutWe apply dropout [33] to the output of each sub-layer, before it is added to thesub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and thepositional encodings in both the encoder and decoder stacks.For the base model, we use a rate ofPdrop = 0.1.During training, we employed label smoothing of value ϵls = 0.1 [36].ThisLabel Smoothinghurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6Results6.1Machine TranslationOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0BLEU, establishing a new state-of-the-art BLEU score of 28.4.The configuration of this model islisted in the bottom line of Table 3.Training took 3.5 days on 8 P100 GPUs.Even our base modelsurpasses all previously published models and ensembles, at a fraction of the training cost of any ofthe competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,outperforming all of the previously published single models, at less than 1/4 the training cost of theprevious state-of-the-art model.The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We", "page_start": 7, "page_end": 8, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.6247547}, {"chunk_id": "914cfb0b-3d4a-4988-8392-851b64c13e57", "text": "The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.Weused beam search with a beam size of 4 and length penalty α = 0.6 [38].These hyperparameterswere chosen after experimentation on the development set.We set the maximum output length duringinference to input length + 50, but terminate early when possible [38].Table 2 summarizes our results and compares our translation quality and training costs to other modelarchitectures from the literature.We estimate the number of floating point operations used to train amodel by multiplying the training time, the number of GPUs used, and an estimate of the sustainedsingle-precision floating-point capacity of each GPU 5.6.2Model VariationsTo evaluate the importance of different components of the Transformer, we varied our base modelin different ways, measuring the change in performance on English-to-German translation on the5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the basemodel.All metrics are on the English-to-German translation development set, newstest2013.Listedperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared toper-word perplexities.trainPPLBLEUparamsNdmodeldffhdkdvPdropϵls×106steps(dev)(dev)base65122048864640.10.1100K4.9225.86515125125.2924.941281285.0025.5(A)1632324.9125.83216165.0125.4165.1625.158(B)325.0125.46026.1123.73645.1925.35084.8825.580(C)25632325.7524.52810241281284.6626.016810245.1225.45340964.7526.2900.05.7724.60.24.9525.5(D)0.04.6725.30.25.4725.7(E)positional embedding instead of sinusoids4.9225.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.", "page_start": 8, "page_end": 9, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.61875325}, {"chunk_id": "0b373c99-8c0e-44bf-8fc7-f6b6b009590d", "text": "Provided proper attribution is provided, Google hereby grants permission toreproduce the tables and figures in this paper solely for use in journalistic orscholarly works.Ashish Vaswani∗Noam Shazeer∗Niki Parmar∗Jakob Uszkoreit∗Google BrainGoogle BrainGoogle ResearchGoogle Researchavaswani@google.comnoam@google.comnikip@google.comusz@google.comLlion Jones∗Aidan N.Gomez∗†Łukasz Kaiser∗Google ResearchUniversity of TorontoGoogle Brainlukaszkaiser@google.comllion@google.comaidan@cs.toronto.eduIllia Polosukhin∗‡illia.polosukhin@gmail.comAbstractThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder.The bestperforming models also connect the encoder and decoder through an attentionmechanism.We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely.Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring significantlyless time to train.Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU.On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.8 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature.We show that the Transformer generalizes well toother tasks by applying it successfully to English constituency parsing both withlarge and limited training data.∗Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and startedthe effort to evaluate this idea.Ashish, with Illia, designed and implemented the first Transformer models andhas been crucially involved in every aspect of this work.Noam proposed scaled dot-product attention, multi-headattention and the parameter-free position representation and became the other person involved in nearly everydetail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase andtensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, andefficient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of andimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks", "page_start": 1, "page_end": 2, "section_title": "", "score": 0.53371}, {"chunk_id": "5151352b-ae46-44f3-83b0-8b8748547941", "text": "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networksin particular, have been firmly established as state of the art approaches in sequence modeling andtransduction problems such as language modeling and machine translation [35, 2, 5].Numerousefforts have since continued to push the boundaries of recurrent language models and encoder-decoderarchitectures [38, 24, 15].Recurrent models typically factor computation along the symbol positions of the input and outputsequences.Aligning the positions to steps in computation time, they generate a sequence of hiddenstates ht, as a function of the previous hidden state ht−1 and the input for position t.This inherentlysequential nature precludes parallelization within training examples, which becomes critical at longersequence lengths, as memory constraints limit batching across examples.Recent work has achievedsignificant improvements in computational efficiency through factorization tricks [21] and conditionalcomputation [32], while also improving model performance in case of the latter.The fundamentalconstraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance inthe input or output sequences [2, 19].In all but a few cases [27], however, such attention mechanismsare used in conjunction with a recurrent network.In this work we propose the Transformer, a model architecture eschewing recurrence and insteadrelying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for significantly more parallelization and can reach a new state of the art intranslation quality after being trained for as little as twelve hours on eight P100 GPUs.2BackgroundThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic buildingblock, computing hidden representations in parallel for all input and output positions.In these models,the number of operations required to relate signals from two arbitrary input or output positions growsin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.This makesit more difficult to learn dependencies between distant positions [12].In the Transformer this isreduced to a constant number of operations, albeit at the cost of reduced effective resolution dueto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,", "page_start": 1, "page_end": 2, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.50228816}, {"chunk_id": "8d9a2784-c6bd-4675-a20b-80dd0948329f", "text": "25.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,keeping the amount of computation constant, as described in Section 3.2.2.While single-headattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality.Thissuggests that determining compatibility is not easy and that a more sophisticated compatibilityfunction than dot product may be beneficial.We further observe in rows (C) and (D) that, as expected,bigger models are better, and dropout is very helpful in avoiding over-fitting.In row (E) we replace oursinusoidal positional encoding with learned positional embeddings [9], and observe nearly identicalresults to the base model.6.3English Constituency ParsingTo evaluate if the Transformer can generalize to other tasks we performed experiments on Englishconstituency parsing.This task presents specific challenges: the output is subject to strong structuralconstraints and is significantly longer than the input.Furthermore, RNN sequence-to-sequencemodels have not been able to attain state-of-the-art results in small-data regimes [37].We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of thePenn Treebank [25], about 40K training sentences.We also trained it in a semi-supervised setting,using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences[37].We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokensfor the semi-supervised setting.We performed only a small number of experiments to select the dropout, both attention and residual(section 5.4), learning rates and beam size on the Section 22 development set, all other parametersremained unchanged from the English-to-German base translation model.During inference, we9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23of WSJ)ParserTrainingWSJ 23 F1Vinyals & Kaiser el al.(2014) [37]WSJ only, discriminative88.3Petrov et al.(2006) [29]WSJ only, discriminative90.4Zhu et al.(2013) [40]WSJ only, discriminative90.4Dyer et al.(2016) [8]WSJ only, discriminative91.7Transformer (4 layers)WSJ only, discriminative91.3Zhu et al.(2013) [40]semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised", "page_start": 9, "page_end": 10, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.50064903}, {"chunk_id": "bb77d3cc-59f9-4b97-a21b-132b30215259", "text": "semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised92.1Transformer (4 layers)semi-supervised92.7Luong et al.(2015) [23]multi-task93.0Dyer et al.(2016) [8]generative93.3increased the maximum output length to input length + 300.We used a beam size of 21 and α = 0.3for both WSJ only and the semi-supervised setting.Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of theRecurrent Neural Network Grammar [8].In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.7ConclusionIn this work, we presented the Transformer, the first sequence transduction model based entirely onattention, replacing the recurrent layers most commonly used in encoder-decoder architectures withmulti-headed self-attention.For translation tasks, the Transformer can be trained significantly faster than architectures basedon recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014English-to-French translation tasks, we achieve a new state of the art.In the former task our bestmodel outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.Weplan to extend the Transformer to problems involving input and output modalities other than text andto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputssuch as images, audio and video.Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.AcknowledgementsWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitfulcomments, corrections and inspiration.References[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprintarXiv:1607.06450, 2016.[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointlylearning to align and translate.CoRR, abs/1409.0473, 2014.[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V.Le.Massive exploration of neuralmachine translation architectures.CoRR, abs/1703.03906, 2017.[4] Jianpeng Cheng, Li Dong, and Mirella Lapata.Long short-term memory-networks for machinereading.arXiv preprint arXiv:1601.06733, 2016.10[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,", "page_start": 10, "page_end": 11, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4892326}, {"chunk_id": "6ecb21e7-c073-4d90-9d8a-41fdfda06afb", "text": "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,textual entailment and learning task-independent sentence representations [4, 27, 28, 22].End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering andlanguage modeling tasks [34].To the best of our knowledge, however, the Transformer is the first transduction model relyingentirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.In the following sections, we will describe the Transformer, motivateself-attention and discuss its advantages over models such as [17, 18] and [9].3Model ArchitectureMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequenceof continuous representations z = (z1, ..., zn).Given z, the decoder then generates an outputsequence (y1, ..., ym) of symbols one element at a time.At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next.2Figure 1: The Transformer - model architecture.The Transformer follows this overall architecture using stacked self-attention and point-wise, fullyconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,respectively.3.1Encoder and Decoder StacksThe encoder is composed of a stack of N = 6 identical layers.Each layer has twoEncoder:sub-layers.The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network.We employ a residual connection [11] around each ofthe two sub-layers, followed by layer normalization [1].That is, the output of each sub-layer isLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layeritself.To facilitate these residual connections, all sub-layers in the model, as well as the embeddinglayers, produce outputs of dimension dmodel = 512.The decoder is also composed of a stack of N = 6 identical layers.In addition to the twoDecoder:sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-headattention over the output of the encoder stack.Similar to the encoder, we employ residual connectionsaround each of the sub-layers, followed by layer normalization.We also modify the self-attentionsub-layer in the decoder stack to prevent positions from attending to subsequent positions.Thismasking, combined with fact that the output embeddings are offset by one position, ensures that the", "page_start": 2, "page_end": 3, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4753728}, {"chunk_id": "92be8a85-80ac-4327-a0b3-f2fff684bda4", "text": "the input sequence centered around the respective output position.This would increase the maximumpath length to O(n/r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k < n does not connect all pairs of input and outputpositions.Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest pathsbetween any two positions in the network.Convolutional layers are generally more expensive thanrecurrent layers, by a factor of k.Separable convolutions [6], however, decrease the complexityconsiderably, to O(k · n · d + n · d2).Even with k = n, however, the complexity of a separableconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,the approach we take in our model.As side benefit, self-attention could yield more interpretable models.We inspect attention distributionsfrom our models and present and discuss examples in the appendix.Not only do individual attentionheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntacticand semantic structure of the sentences.5TrainingThis section describes the training regime for our models.5.1Training Data and BatchingWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 millionsentence pairs.Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens.For English-French, we used the significantly larger WMT2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piecevocabulary [38].Sentence pairs were batched together by approximate sequence length.Each trainingbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000target tokens.5.2Hardware and ScheduleWe trained our models on one machine with 8 NVIDIA P100 GPUs.For our base models usingthe hyperparameters described throughout the paper, each training step took about 0.4 seconds.Wetrained the base models for a total of 100,000 steps or 12 hours.For our big models,(described on thebottom line of table 3), step time was 1.0 seconds.The big models were trained for 300,000 steps(3.5 days).5.3OptimizerWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9.We varied the learningrate over the course of training, according to the formula:lrate = d−0.5model · min(step_num−0.5, step_num · warmup_steps−1.5)(3)This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step number.We usedwarmup_steps = 4000.5.4RegularizationWe employ three types of regularization during training:7", "page_start": 7, "page_end": 7, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4247507}], "comparative_nda_01": [{"chunk_id": "94d954fb-950b-4dcf-8049-c72deb8243c9", "text": "Date: 201[ ]Parties:[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]and[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]1.Each of the parties to this Agreement intends to disclose information (the ConfidentialInformation) to the other party for the purpose of [insert details e.g.discussing thepossibility of the parties entering into a joint venture] (the Purpose).2.Each party to this Agreement is referred to as ‘the Recipient’ when it receives or usesthe Confidential Information disclosed by the other party.3.The Recipient undertakes not to use the Confidential Information disclosed by the otherparty for any purpose except the Purpose, without first obtaining the written agreementof the other party.4.The Recipient undertakes to keep the Confidential Information disclosed by the otherparty secure and not to disclose it to any third party [except to its employees [andprofessional advisers] who need to know the same for the Purpose, who know theyowe a duty of confidence to the other party and who are bound by obligationsequivalent to those in clause 3 above and this clause 4.5.The undertakings in clauses 3 and 4 above apply to all of the information disclosed byeach of the parties to the other, regardless of the way or form in which it is disclosed orrecorded but they do not apply to:a) any information which is or in future comes into the public domain (unless as a result ofthe breach of this Agreement); orb) any information which is already known to the Recipient and which was not subject toany obligation of confidence before it was disclosed to the Recipient by the other party.6.Nothing in this Agreement will prevent the Recipient from making any disclosure of theConfidential Information required by law or by any competent authority.7.The Recipient will, on request from the other party, return all copies and records of theConfidential Information disclosed by the other party to the Recipient and will not retainany copies or records of the Confidential Information disclosed by the other party.8.Neither this Agreement nor the supply of any information grants the Recipient anylicence, interest or right in respect of any intellectual property rights of the other partyexcept the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.", "page_start": 1, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.49730864}, {"chunk_id": "6c64d139-fd3d-400d-b64b-3ddf0dea3473", "text": "except the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.The English Courts will have non-exclusive jurisdiction to deal with any dispute whichhas arisen or may arise out of, or in connection with, this Agreement.Signed [by [insert name]] OR [on behalf of][insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________PositionSigned [by [insert name]] OR [on behalf of] [insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________Position", "page_start": 2, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.4107579}], "summary_nvidia_01": [{"chunk_id": "34353893-a857-4785-8970-188532137074", "text": "Prior to NVIDIA, Mr.Teter spent more than two decades at the law firm of Cooley LLP, where he focused on litigating patent andtechnology related matters.Prior to attending law school, he worked as an engineer at Lockheed Missiles and Space Company, an aerospace company.Mr.Teter holds a B.S.degree in Mechanical Engineering from the University of California at Davis and a J.D.degree from Stanford Law School.Available InformationOur annual reports on Form 10-K, quarterly reports on Form 10-Q, current reports on Form 8-K and, if applicable, amendments to those reports filed or furnishedpursuant to Section 13(a) or 15(d) of the Securities Exchange Act of 1934, as amended, or the Exchange Act, are available free of charge on or through ourwebsite, http://www.nvidia.com, as soon as reasonably practicable after we electronically file such material with, or furnish it to, the Securities and ExchangeCommission, or the SEC.The SEC’s website, http://www.sec.gov, contains reports, proxy and information statements, and other information regarding issuersthat file electronically with the SEC.Our web site and the information on it or connected to it are not a part of this Annual Report on Form 10-K.Item 1A.Risk FactorsThe following risk factors should be considered in addition to the other information in this Annual Report on Form 10-K.The following risks could harm ourbusiness, financial condition, results of operations or reputation, which could cause our stock price to decline.Additional risks, trends and uncertainties notpresently known to us or that we currently believe are immaterial may also harm our business, financial condition, results of operations or reputation.Risk Factors SummaryRisks Related to Our Industry and Markets•Failure to meet the evolving needs of our industry may adversely impact our financial results.•Competition could adversely impact our market share and financial results.Risks Related to Demand, Supply and Manufacturing•Failure to estimate customer demand accurately has led and could lead to mismatches between supply and demand.•Dependency on third-party suppliers and their technology to manufacture, assemble, test, or package our products reduces our control over productquantity and quality, manufacturing yields, and product delivery schedules and could harm our business.•Defects in our products have caused and could cause us to incur significant expenses to remediate and could damage our business.Risks Related to Our Global Operating Business•Adverse economic conditions may harm our business.13•International sales and operations are a significant part of our business, which exposes us to risks that could harm our business.•Product, system security and data breaches and cyber-attacks could disrupt our operations and adversely affect our financial condition, stock price andreputation.•Business disruptions could harm our operations and financial results.•Climate change may have a long-term impact on our business.•", "page_start": 13, "page_end": 14, "section_title": "NVIDIA CORPORATION", "score": 0.68720317}, {"chunk_id": "0dd7fcd2-abcb-4469-bfdc-d6d5c6cb8eae", "text": "*$100 invested on 1/27/19 in stock and in indices, including reinvestment of dividends.Source: FactSet financial data and analytics.1/27/20191/26/20201/31/20211/30/20221/29/20231/28/2024NVIDIA Corporation$100.00$157.02$326.26$574.15$512.40$1,536.28S&P 500$100.00$126.17$144.83$175.25$163.63$199.83Nasdaq 100$100.00$136.15$194.20$218.68$185.67$268.13Item 6.[Reserved]33Item 7.Management's Discussion and Analysis of Financial Condition and Results of OperationsThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with “Item 1A.Risk Factors”, ourConsolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described elsewhere in this Annual Report onForm 10-K, before deciding to purchase, hold or sell shares of our common stock.OverviewOur Company and Our BusinessesNVIDIA pioneered accelerated computing to help solve the most challenging computational problems.Since our original focus on PC graphics, we haveexpanded to several other large and important computationally intensive fields.NVIDIA has leveraged its GPU architecture to create platforms for acceleratedcomputing, AI solutions, scientific computing, data science, AV, robotics, metaverse and 3D internet applications.Our two operating segments are \"Compute & Networking\" and \"Graphics.\" Refer to Note 17 of the Notes to the Consolidated Financial Statements in Part IV,Item 15 of this Annual Report on Form 10-K for additional information.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Recent Developments, Future Objectives and ChallengesDemand and Supply, Product Transitions, and New Products and Business ModelsDemand for our data center systems and products surged in fiscal year 2024.Entering fiscal year 2025, we are gathering customer demand indications acrossseveral product transitions.We have demand visibility for our new data center products ramping later in fiscal year 2025.We have increased our supply andcapacity purchases with existing suppliers, added new vendors and entered into prepaid manufacturing and capacity agreements.These increased purchasevolumes, the number of suppliers, and the integration of new vendors into our supply chain may create more complexity and execution risk.Our purchasecommitments and obligations for inventory and manufacturing capacity at the end of fiscal year 2024 were impacted by shortening lead times for certaincomponents.We may continue to enter into new supplier and capacity arrangements.Supply of Hopper architecture products is improving, and demand remainsvery strong.We expect our next-generation products to be supply-constrained based upon demand indications.", "page_start": 33, "page_end": 34, "section_title": "NVIDIA CORPORATION", "score": 0.67969817}, {"chunk_id": "096b49f9-ef3d-4e1e-a07d-95303cb2ec48", "text": "not be able to reduce our inventory or other contractual purchase commitments.In the past, we have experienced a reduction in average selling prices, includingdue to channel pricing programs that we have implemented and may continue to implement, as a result of our overestimation of future demand, and we mayneed to continue these reductions.We have had to increase prices for certain of our products as a result of our suppliers’ increase in prices, and we may need tocontinue to do so for other products in the future.We have also written down our inventory, incurred cancellation penalties, and recorded impairments and mayhave to do so in the future.These impacts were amplified by our placement of non-cancellable and non-returnable purchasing terms well in advance of ourhistorical lead times and could be exacerbated if we need to make changes to the design of future products.The risk of these impacts has increased and maycontinue to increase as our purchase obligations and prepaids have grown and are expected to continue to grow and become a greater portion of our totalsupply.All of these factors may negatively impact our gross margins and financial results.We build technology and introduce products for new and innovative use cases and applications, such as NVIDIA DGX Cloud services, NVIDIA AI Foundations,Omniverse platform, LLMs, and generative AI models.Our demand estimates for new use cases, applications, and services can be incorrect and create volatilityin our revenue or supply levels, and we may not be able to generate significant revenue from these use cases, applications, and services.Recent technologies,such as generative AI models, have emerged, and while they have driven increased demand for Data Center, the long-term trajectory is unknown.Because ourproducts may be used in multiple use cases and applications, it is difficult for us to estimate with any reasonable degree of precision the impact of generative AImodels on our reported revenue or forecasted demand.Additionally, we started shipping our CPU product offerings, the Grace CPU and Grace HopperSuperchips, in the third quarter of fiscal year 2024.Our ability to adequately predict our CPU demand may create volatility in our revenue or supply levels.Challenges in estimating demand could become more pronounced or volatile in the future on both a global and regional basis.Extended lead times may occur ifwe experience other supply constraints caused by natural disasters, pandemics or other events.In addition, geopolitical tensions, such as those involving Taiwanand China, which comprise a significant portion of our revenue and where we have suppliers, contract manufacturers, and assembly partners who are critical toour supply continuity, could have a material adverse impact on us.The use of our GPUs other than that for which they were designed and marketed, including new and unexpected use cases, has impacted and can in the futureimpact demand for our products, including by leading to inconsistent spikes and drops in demand.", "page_start": 17, "page_end": 17, "section_title": "NVIDIA CORPORATION", "score": 0.6460118}, {"chunk_id": "23afd62a-0f84-4746-bbbb-fb6b9ff1b3ce", "text": "and capacity supply, and our supply deliveries and production may be non-linear within a quarter or year.If our estimates of customer demand are inaccurate, aswe have experienced in the past, there could be a significant mismatch between supply and demand.This mismatch has resulted in both product shortages andexcess inventory, has varied across our market platforms, and has significantly harmed our financial results.We build finished products and maintain inventory in advance of anticipated demand.While we have in the past entered and may in the future enter into longterm supply and capacity commitments, we may not be able to secure sufficient15commitments for capacity to address our business needs, or our long-term demand expectations may change.These risks may increase as we shorten ourproduct development cycles, enter new lines of business, or integrate new suppliers or components into our supply chain, creating additional supply chaincomplexity.Additionally, our ability to sell certain products has been and could be impeded if components necessary for the finished products are not availablefrom third parties.This risk may increase as a result of our platform strategy.In periods of shortages impacting the semiconductor industry and/or limited supplyor capacity in our supply chain, the lead times on our orders may be extended.We have previously experienced and may continue to experience extended leadtimes of more than 12 months.We have paid premiums and provided deposits to secure future supply and capacity, which have increased our product costs andmay continue to do so.If our existing suppliers are unable to scale their capabilities to meet our supply needs, we may require additional sources of capacity,which may require additional deposits.We may not have the ability to reduce our supply commitments at the same rate or at all if our revenue declines.Many additional factors have caused and/or could in the future cause us to either underestimate or overestimate our customers’ future demand for our products,or otherwise cause a mismatch between supply and demand for our products and impact the timing and volume of our revenue, including:•changes in product development cycles and time to market;•competing technologies and competitor product releases and announcements;•changes in business and economic conditions resulting in decreased end demand;•sudden or sustained government lockdowns or actions to control case spread of global or local health issues;•rapidly changing technology or customer requirements;•the availability of sufficient data center capacity and energy for customers to procure;•new product introductions and transitions resulting in less demand for existing products;•new or unexpected end-use cases;•increase in demand for competitive products, including competitive actions;•business decisions made by third parties;•the demand for accelerated or AI-related cloud services, including our own software and NVIDIA DGX Cloud services;•changes that impact the ecosystem for the architectures underlying our products and technologies;•", "page_start": 15, "page_end": 16, "section_title": "NVIDIA CORPORATION", "score": 0.642989}, {"chunk_id": "d749f15f-9510-4d07-8ec8-b5bec12a263e", "text": "support these offerings and our research and development activities.The timing and availability of these cloud services has changed and may continue tochange, impacting our revenue, expenses, and development timelines.NVIDIA DGX Cloud services may not be successful and will take time, resources, andinvestment.We also offer or plan to offer standalone software solutions, including NVIDIA AI Enterprise, NVIDIA Omniverse, NVIDIA DRIVE, and several othersoftware solutions.These new business models or strategies may not be successful, and we may fail to sell any meaningful standalone software or services.Wemay incur significant costs and may not achieve any significant revenue from these offerings.Competition could adversely impact our market share and financial results.Our target markets remain competitive, and competition may intensify with expanding and changing product and service offerings, industry standards, customerneeds, new entrants and consolidations.Our competitors’ products, services and technologies, including those mentioned above in this Annual Report on Form10-K, may be cheaper or provide better functionality or features than ours, which has resulted and may in the future result in lower-than-expected selling pricesfor our products.Some of our competitors operate their own fabrication facilities, and have longer operating histories, larger customer bases, morecomprehensive IP portfolios and patent protections, more design wins, and greater financial, sales, marketing and distribution resources than we do.Thesecompetitors may be able to acquire market share and/or prevent us from doing so, more effectively identify and capitalize upon opportunities in new markets andend-user trends, more quickly transition their products, and impinge on our ability to procure sufficient foundry capacity and scarce input materials during asupply-constrained environment, which could harm our business.Some of our customers have in-house expertise and internal development capabilities similarto some of ours and can use or develop their own solutions to replace those we are providing.For example, others may offer cloud-based services that competewith our AI cloud service offerings, and we may not be able to establish market share sufficient to achieve the scale necessary to meet our business objectives.Ifwe are unable to successfully compete in this environment, demand for our products, services and technologies could decrease and we may not establishmeaningful revenue.Risks Related to Demand, Supply and ManufacturingFailure to estimate customer demand accurately has led and could lead to mismatches between supply and demand.We use third parties to manufacture and assemble our products, and we have long manufacturing lead times.We are not provided guaranteed wafer, componentand capacity supply, and our supply deliveries and production may be non-linear within a quarter or year.If our estimates of customer demand are inaccurate, aswe have experienced in the past, there could be a significant mismatch between supply and demand.This mismatch has resulted in both product shortages andexcess inventory, has varied across our market platforms, and has significantly harmed our financial results.", "page_start": 15, "page_end": 15, "section_title": "NVIDIA CORPORATION", "score": 0.6237427}, {"chunk_id": "d8e7c688-4425-408c-9955-d540c5db28f6", "text": "– Risks Related to Regulatory, Legal, Our Stock and Other Matters” for a discussion of this potential impact.Compliance with laws, rules, and regulations has not otherwise had a material effect upon our capital expenditures, results of operations, or competitive positionand we do not currently anticipate material capital expenditures for environmental control facilities.Compliance with existing or future governmental regulations,including, but not limited to, those pertaining to IP ownership and infringement, taxes, import and export requirements and tariffs, anti-corruption, businessacquisitions, foreign exchange controls and cash repatriation restrictions, data privacy requirements, competition and antitrust, advertising, employment, productregulations, cybersecurity, environmental, health and safety requirements, the responsible use of AI, climate change, cryptocurrency, and consumer laws, couldincrease our costs, impact our competitive position, and otherwise may have a material adverse impact on our business, financial condition and results ofoperations in subsequent periods.Refer to “Item 1A.Risk Factors” for a discussion of these potential impacts.Sustainability and GovernanceNVIDIA invents computing technologies that improve lives and address global challenges.Our goal is to integrate sound environmental, social, and corporategovernance principles and practices into every aspect of the Company.The Nominating and Corporate Governance Committee of our Board of Directors isresponsible for reviewing and discussing with management our practices related to sustainability and corporate governance.We assess our programs annuallyin consideration of stakeholder expectations, market trends, and business risks and opportunities.These issues are important for our continued businesssuccess and reflect the topics of highest concern to NVIDIA and our stakeholders.The following section and the Human Capital Management Section below provide an overview of our principles and practices.More information can be found onour website and in our annual Sustainability Report.Information contained on our website or in our annual Sustainability Report is not incorporated by referenceinto this or any other report we file with the Securities and Exchange Commission, or the SEC.Refer to “Item 1A.Risk Factors” for a discussion of risks anduncertainties we face related to sustainability.Climate ChangeIn the area of environmental sustainability, we address our climate impacts across our product lifecycle and assess risks, including current and emergingregulations and market impacts.In May 2023, we published metrics related to our environmental impact for fiscal year 2023.Fiscal year 2024 metrics are expected to be published in the firsthalf of fiscal year 2025.There has been no material impact to our capital expenditures, results of operations or competitive position associated with globalenvironmental sustainability regulations, compliance, or costs from sourcing renewable energy.By the end of fiscal year 2025, our goal is to purchase10or generate enough renewable energy to match 100% of our global electricity usage for our offices and data centers.In fiscal year 2023, we increased thepercentage of our total electricity use matched by renewable energy purchases to 44%.By fiscal year 2026, we aim to engage manufacturing suppliers", "page_start": 10, "page_end": 11, "section_title": "NVIDIA CORPORATION", "score": 0.6220932}, {"chunk_id": "bd3ca6de-70c3-422f-81b7-d31f90e1da1c", "text": "our supply continuity, could have a material adverse impact on us.The use of our GPUs other than that for which they were designed and marketed, including new and unexpected use cases, has impacted and can in the futureimpact demand for our products, including by leading to inconsistent spikes and drops in demand.For example, several years ago, our Gaming GPUs began tobe used for mining digital currencies, such as Ethereum.It is difficult for us to estimate with any reasonable degree of precision the past or current impact ofcryptocurrency mining, or forecast the future impact of cryptocurrency mining, on demand for our products.Volatility in the cryptocurrency market, including newcompute technologies, price changes in cryptocurrencies, government cryptocurrency policies and regulations, new cryptocurrency standards and changes inthe method of verifying blockchain transactions, has impacted and can in the future impact cryptocurrency mining and demand for our products and can furtherimpact our ability to estimate demand for our products.Changes to cryptocurrency standards and processes including, but not limited to, the Ethereum 2.0merge in 2022, have reduced and may in the future decrease the usage of GPUs for Ethereum mining.This has created and may in the future create increasedaftermarket sales of our17GPUs, which could negatively impact retail prices for our GPUs and reduce demand for our new GPUs.In general, our new products or previously sold productsmay be resold online or on the unauthorized “gray market,” which also makes demand forecasting difficult.Gray market products and reseller marketplacescompete with our new products and distribution channels.Additionally, we depend on developers, customers and other third parties to build, enhance, and maintain accelerated computing applications that leverage ourplatforms.We also rely on third-party content providers and publishers to make their content available on our platforms, such as GeForce NOW.Failure bydevelopers, customers, and other third parties to build, enhance, and maintain applications that leverage our platforms, or failure by third-party content providersor publishers to make their content available on reasonable terms or at all for use by our customers or end users on our platforms, could adversely affectcustomer demand.Dependency on third-party suppliers and their technology to manufacture, assemble, test, or package our products reduces our control over productquantity and quality, manufacturing yields, and product delivery schedules and could harm our business.We depend on foundries to manufacture our semiconductor wafers using their fabrication equipment and techniques.We do not assemble, test, or package ourproducts, but instead contract with independent subcontractors.These subcontractors assist with procuring components used in our systems, boards, andproducts.We face several risks which have adversely affected or could adversely affect our ability to meet customer demand and scale our supply chain,negatively impact longer-term demand for our products and services, and adversely affect our business operations, gross margin, revenue and/or financialresults, including:•", "page_start": 17, "page_end": 18, "section_title": "NVIDIA CORPORATION", "score": 0.6175236}, {"chunk_id": "4aa3d666-113b-4b25-9f5b-59bad8ce72a9", "text": "demand for our networking products used in servers containing our GPUs.The USG may also impose export controls on our networking products, such as highspeed network interconnects, to limit the ability of downstream parties to create large clusters for frontier model training.Any new control that impacts a widerrange of our products would likely have a disproportionate impact on NVIDIA and may disadvantage us against certain of our competitors that sell chips that areoutside the scope of such control.Excessive or shifting export controls have already and may in the future encourage customers outside China and otherimpacted regions to “design-out” certain U.S.semiconductors from their products to reduce the compliance burden and risk, and to ensure that they are able toserve markets worldwide.Excessive or shifting export controls have already encouraged and may in the future encourage overseas governments to request thatour customers purchase from our competitors rather than NVIDIA or other U.S.firms, harming our business, market position, and financial results.As a result,excessive or shifting export controls may negatively impact demand for our products and services not only in China, but also in other markets, such as Europe,Latin America, and Southeast Asia.Excessive or shifting export controls increase the risk of investing in U.S.advanced semiconductor products, because by thetime a new product is ready for market, it may be subject to new unilateral export controls restricting its sale.At the same time, such controls may increaseinvestment in foreign competitors, which would be less likely to be restricted by U.S.controls.Additionally, restrictions imposed by the Chinese government on the duration of gaming activities and access to games may adversely affect our Gamingrevenue, and increased oversight of digital platform companies may adversely affect our Data Center revenue.The Chinese government may impose restrictionson the sale to certain customers of our products, or any products containing components made by our partners and suppliers.For example, the Chinesegovernment announced restrictions relating to certain sales of products containing certain products made by Micron, a supplier of ours.Further restrictions onour products or the products of our suppliers could negatively impact our business and financial results.Finally, our business depends on our ability to receive consistent and reliable supply from our overseas partners, especially in Taiwan.Any new restrictions thatnegatively impact our ability to receive supply of components, parts, or services from Taiwan, would negatively impact our business and financial results.Increased scrutiny from shareholders, regulators and others regarding our corporate sustainability practices could result in additional costs or risksand adversely impact our reputation and willingness of customers and suppliers to do business with us.Shareholder advocacy groups, certain investment funds, other market participants, shareholders, customers and government regulators have focusedincreasingly on corporate sustainability practices and disclosures, including those associated with climate change and human rights.Stakeholders may not be", "page_start": 27, "page_end": 27, "section_title": "NVIDIA CORPORATION", "score": 0.60808194}], "summary_attention_01": [{"chunk_id": "0b373c99-8c0e-44bf-8fc7-f6b6b009590d", "text": "Provided proper attribution is provided, Google hereby grants permission toreproduce the tables and figures in this paper solely for use in journalistic orscholarly works.Ashish Vaswani∗Noam Shazeer∗Niki Parmar∗Jakob Uszkoreit∗Google BrainGoogle BrainGoogle ResearchGoogle Researchavaswani@google.comnoam@google.comnikip@google.comusz@google.comLlion Jones∗Aidan N.Gomez∗†Łukasz Kaiser∗Google ResearchUniversity of TorontoGoogle Brainlukaszkaiser@google.comllion@google.comaidan@cs.toronto.eduIllia Polosukhin∗‡illia.polosukhin@gmail.comAbstractThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder.The bestperforming models also connect the encoder and decoder through an attentionmechanism.We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely.Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring significantlyless time to train.Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU.On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.8 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature.We show that the Transformer generalizes well toother tasks by applying it successfully to English constituency parsing both withlarge and limited training data.∗Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and startedthe effort to evaluate this idea.Ashish, with Illia, designed and implemented the first Transformer models andhas been crucially involved in every aspect of this work.Noam proposed scaled dot-product attention, multi-headattention and the parameter-free position representation and became the other person involved in nearly everydetail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase andtensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, andefficient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of andimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks", "page_start": 1, "page_end": 2, "section_title": "", "score": 0.44818777}, {"chunk_id": "d487540d-470c-4230-b88f-4a3e27e977d2", "text": "Similar to the encoder, we employ residual connectionsaround each of the sub-layers, followed by layer normalization.We also modify the self-attentionsub-layer in the decoder stack to prevent positions from attending to subsequent positions.Thismasking, combined with fact that the output embeddings are offset by one position, ensures that thepredictions for position i can depend only on the known outputs at positions less than i.3.2AttentionAn attention function can be described as mapping a query and a set of key-value pairs to an output,where the query, keys, values, and output are all vectors.The output is computed as a weighted sum3Scaled Dot-Product AttentionMulti-Head AttentionFigure 2: (left) Scaled Dot-Product Attention.(right) Multi-Head Attention consists of severalattention layers running in parallel.of the values, where the weight assigned to each value is computed by a compatibility function of thequery with the corresponding key.3.2.1Scaled Dot-Product AttentionWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2).The input consists ofqueries and keys of dimension dk, and values of dimension dv.We compute the dot products of thequery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on thevalues.In practice, we compute the attention function on a set of queries simultaneously, packed togetherinto a matrix Q.The keys and values are also packed together into matrices K and V .We computethe matrix of outputs as:Attention(Q, K, V ) = softmax(QKT√dk)V(1)The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention.Dot-product attention is identical to our algorithm, except for the scaling factor1of√dk .Additive attention computes the compatibility function using a feed-forward network witha single hidden layer.While the two are similar in theoretical complexity, dot-product attention ismuch faster and more space-efficient in practice, since it can be implemented using highly optimizedmatrix multiplication code.While for small values of dk the two mechanisms perform similarly, additive attention outperformsdot product attention without scaling for larger values of dk [3].We suspect that for large values ofdk, the dot products grow large in magnitude, pushing the softmax function into regions where it has1extremely small gradients 4.To counteract this effect, we scale the dot products by√dk .3.2.2Multi-Head AttentionInstead of performing a single attention function with dmodel-dimensional keys, values and queries,we found it beneficial to linearly project the queries, keys and values h times with different, learnedlinear projections to dk, dk and dv dimensions, respectively.On each of these projected versions ofqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional", "page_start": 3, "page_end": 4, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.44564113}, {"chunk_id": "58f1c685-b5d0-4ba4-8711-0ba5768f729c", "text": "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,we found it beneficial to linearly project the queries, keys and values h times with different, learnedlinear projections to dk, dk and dv dimensions, respectively.On each of these projected versions ofqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional4To illustrate why the dot products get large, assume that the components of q and k are independent randomvariables with mean 0 and variance 1.Then their dot product, q · k = Pdki=1 qiki, has mean 0 and variance dk.4output values.These are concatenated and once again projected, resulting in the final values, asdepicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representationsubspaces at different positions.With a single attention head, averaging inhibits this.MultiHead(Q, K, V ) = Concat(head1, ..., headh)W Owhere headi = Attention(QW Qi , KW Ki , V W Vi )Where the projections are parameter matrices W Q∈Rdmodel×dk, W V∈Rdmodel×dv∈Rdmodel×dk, W Kiiiand W O ∈Rhdv×dmodel.In this work we employ h = 8 parallel attention layers, or heads.For each of these we usedk = dv = dmodel/h = 64.Due to the reduced dimension of each head, the total computational costis similar to that of single-head attention with full dimensionality.3.2.3Applications of Attention in our ModelThe Transformer uses multi-head attention in three different ways:• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,and the memory keys and values come from the output of the encoder.This allows everyposition in the decoder to attend over all positions in the input sequence.This mimics thetypical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38, 2, 9].• The encoder contains self-attention layers.In a self-attention layer all of the keys, valuesand queries come from the same place, in this case, the output of the previous layer in theencoder.Each position in the encoder can attend to all positions in the previous layer of theencoder.• Similarly, self-attention layers in the decoder allow each position in the decoder to attend toall positions in the decoder up to and including that position.We need to prevent leftwardinformation flow in the decoder to preserve the auto-regressive property.We implement thisinside of scaled dot-product attention by masking out (setting to −∞) all values in the inputof the softmax which correspond to illegal connections.See Figure 2.3.3Position-wise Feed-Forward NetworksIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fullyconnected feed-forward network, which is applied to each position separately and identically.This", "page_start": 4, "page_end": 5, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.439194}, {"chunk_id": "26550dd8-8e7b-4b95-a9e9-2035c8c211aa", "text": "Advances in Neural Information Processing Systems, 2015.[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, WolfgangMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.Google’s neural machinetranslation system: Bridging the gap between human and machine translation.arXiv preprintarXiv:1609.08144, 2016.[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.Deep recurrent models withfast-forward connections for neural machine translation.CoRR, abs/1606.04199, 2016.[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu.Fast and accurateshift-reduce constituent parsing.In Proceedings of the 51st Annual Meeting of the ACL (Volume1: Long Papers), pages 434–443.ACL, August 2013.12Attention VisualizationsgovernmentsregistrationAmericanprocessmajority<EOS>passedmakingdifficult<pad><pad><pad><pad><pad><pad>votingsincemore2009havespiritlawsnewthatthistheorofinisaIt.It.<EOS>thatoforthistheisadifficultmorespirit<pad>inprocesssince<pad><pad><pad><pad>lawsnew<pad>votinghaveAmericanmajority2009passedmakingregistrationgovernmentsFigure 3: An example of the attention mechanism following long-distance dependencies in theencoder self-attention in layer 5 of 6.Many of the attention heads attend to a distant dependency ofthe verb ‘making’, completing the phrase ‘making...more difficult’.Attentions here shown only forthe word ‘making’.Different colors represent different heads.Best viewed in color.13applicationmissing<EOS>opinionperfectshould<pad>neverwhatLawThejustthisarebutwillmywebebeitsinis-,,..<EOS>,,its-thisTheperfectmyisbutjustbebeinwhatareweLawnever<pad>missingshouldwillopinionapplicationapplicationmissing<EOS>opinionperfectshould<pad>neverwhatLawThejustthisarebutwillmywebebeitsinis-,,..<EOS>,,its-thisTheperfectmyisbutjustbebeinare", "page_start": 12, "page_end": 14, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.42917585}, {"chunk_id": "5151352b-ae46-44f3-83b0-8b8748547941", "text": "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networksin particular, have been firmly established as state of the art approaches in sequence modeling andtransduction problems such as language modeling and machine translation [35, 2, 5].Numerousefforts have since continued to push the boundaries of recurrent language models and encoder-decoderarchitectures [38, 24, 15].Recurrent models typically factor computation along the symbol positions of the input and outputsequences.Aligning the positions to steps in computation time, they generate a sequence of hiddenstates ht, as a function of the previous hidden state ht−1 and the input for position t.This inherentlysequential nature precludes parallelization within training examples, which becomes critical at longersequence lengths, as memory constraints limit batching across examples.Recent work has achievedsignificant improvements in computational efficiency through factorization tricks [21] and conditionalcomputation [32], while also improving model performance in case of the latter.The fundamentalconstraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance inthe input or output sequences [2, 19].In all but a few cases [27], however, such attention mechanismsare used in conjunction with a recurrent network.In this work we propose the Transformer, a model architecture eschewing recurrence and insteadrelying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for significantly more parallelization and can reach a new state of the art intranslation quality after being trained for as little as twelve hours on eight P100 GPUs.2BackgroundThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic buildingblock, computing hidden representations in parallel for all input and output positions.In these models,the number of operations required to relate signals from two arbitrary input or output positions growsin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.This makesit more difficult to learn dependencies between distant positions [12].In the Transformer this isreduced to a constant number of operations, albeit at the cost of reduced effective resolution dueto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,", "page_start": 1, "page_end": 2, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.42808086}, {"chunk_id": "1abdec62-7bd0-4192-b611-f148cee3a9b6", "text": "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, BowenZhou, and Yoshua Bengio.A structured self-attentive sentence embedding.arXiv preprintarXiv:1703.03130, 2017.[23] Minh-Thang Luong, Quoc V.Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.Multi-tasksequence to sequence learning.arXiv preprint arXiv:1511.06114, 2015.[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.Effective approaches to attentionbased neural machine translation.arXiv preprint arXiv:1508.04025, 2015.11[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.Building a large annotatedcorpus of english: The penn treebank.Computational linguistics, 19(2):313–330, 1993.[26] David McClosky, Eugene Charniak, and Mark Johnson.Effective self-training for parsing.InProceedings of the Human Language Technology Conference of the NAACL, Main Conference,pages 152–159.ACL, June 2006.[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit.A decomposable attentionmodel.In Empirical Methods in Natural Language Processing, 2016.[28] Romain Paulus, Caiming Xiong, and Richard Socher.A deep reinforced model for abstractivesummarization.arXiv preprint arXiv:1705.04304, 2017.[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.Learning accurate, compact,and interpretable tree annotation.In Proceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440.ACL, July2006.[30] Ofir Press and Lior Wolf.Using the output embedding to improve language models.arXivpreprint arXiv:1608.05859, 2016.[31] Rico Sennrich, Barry Haddow, and Alexandra Birch.Neural machine translation of rare wordswith subword units.arXiv preprint arXiv:1508.07909, 2015.[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,and Jeff Dean.Outrageously large neural networks: The sparsely-gated mixture-of-expertslayer.arXiv preprint arXiv:1701.06538, 2017.[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overfitting.Journal of MachineLearning Research, 15(1):1929–1958, 2014.[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.End-to-end memorynetworks.In C.Cortes, N.D.Lawrence, D.D.Lee, M.Sugiyama, and R.Garnett, editors,Advances in Neural Information Processing Systems 28, pages 2440–2448.Curran Associates,Inc., 2015.[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.Sequence to sequence learning with neuralnetworks.In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.Rethinking the inception architecture for computer vision.CoRR, abs/1512.00567, 2015.[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton.Grammar as a foreign language.InAdvances in Neural Information Processing Systems, 2015.[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, WolfgangMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.Google’s neural machinetranslation system: Bridging the gap between human and machine translation.arXiv preprintarXiv:1609.08144, 2016.", "page_start": 11, "page_end": 12, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4231569}, {"chunk_id": "d3264ea3-3920-4ed4-bbb0-d3dcc4fc4658", "text": "should<pad>neverwhatLawThejustthisarebutwillmywebebeitsinis-,,..<EOS>,,its-thisTheperfectmyisbutjustbebeinarewhatweLawnever<pad>missingshouldwillopinionapplicationFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution.Top:Full attentions for head 5.Bottom: Isolated attentions from just the word ‘its’ for attention heads 5and 6.Note that the attentions are very sharp for this word.14applicationmissing<EOS>opinionperfectshould<pad>neverwhatLawThejustthisarebutwillmywebebeitsinis-,,..<EOS>,,its-thisTheperfectmyisbutjustbebeinarewhatweneverLaw<pad>missingshouldwillopinionapplicationapplicationmissing<EOS>opinionperfectshould<pad>neverwhatLawThejustthisarebutwillmywebebeitsinis-,,..<EOS>,,its-thisTheperfectmyisbutjustbebeinarewhatweLawnever<pad>missingshouldwillopinionapplicationFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of thesentence.We give two such examples above, from two different heads from the encoder self-attentionat layer 5 of 6.The heads clearly learned to perform different tasks.15", "page_start": 14, "page_end": 15, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.42089748}, {"chunk_id": "df8dae53-302d-45cc-8864-b1ae084274b0", "text": "applicationFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of thesentence.We give two such examples above, from two different heads from the encoder self-attentionat layer 5 of 6.The heads clearly learned to perform different tasks.15", "page_start": 15, "page_end": 15, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.41446716}], "summary_nda_01": [{"chunk_id": "94d954fb-950b-4dcf-8049-c72deb8243c9", "text": "Date: 201[ ]Parties:[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]and[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]1.Each of the parties to this Agreement intends to disclose information (the ConfidentialInformation) to the other party for the purpose of [insert details e.g.discussing thepossibility of the parties entering into a joint venture] (the Purpose).2.Each party to this Agreement is referred to as ‘the Recipient’ when it receives or usesthe Confidential Information disclosed by the other party.3.The Recipient undertakes not to use the Confidential Information disclosed by the otherparty for any purpose except the Purpose, without first obtaining the written agreementof the other party.4.The Recipient undertakes to keep the Confidential Information disclosed by the otherparty secure and not to disclose it to any third party [except to its employees [andprofessional advisers] who need to know the same for the Purpose, who know theyowe a duty of confidence to the other party and who are bound by obligationsequivalent to those in clause 3 above and this clause 4.5.The undertakings in clauses 3 and 4 above apply to all of the information disclosed byeach of the parties to the other, regardless of the way or form in which it is disclosed orrecorded but they do not apply to:a) any information which is or in future comes into the public domain (unless as a result ofthe breach of this Agreement); orb) any information which is already known to the Recipient and which was not subject toany obligation of confidence before it was disclosed to the Recipient by the other party.6.Nothing in this Agreement will prevent the Recipient from making any disclosure of theConfidential Information required by law or by any competent authority.7.The Recipient will, on request from the other party, return all copies and records of theConfidential Information disclosed by the other party to the Recipient and will not retainany copies or records of the Confidential Information disclosed by the other party.8.Neither this Agreement nor the supply of any information grants the Recipient anylicence, interest or right in respect of any intellectual property rights of the other partyexcept the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.", "page_start": 1, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.4970886}, {"chunk_id": "6c64d139-fd3d-400d-b64b-3ddf0dea3473", "text": "except the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.The English Courts will have non-exclusive jurisdiction to deal with any dispute whichhas arisen or may arise out of, or in connection with, this Agreement.Signed [by [insert name]] OR [on behalf of][insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________PositionSigned [by [insert name]] OR [on behalf of] [insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________Position", "page_start": 2, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.4786514}], "inference_nvidia_01": [{"chunk_id": "ddc05d6e-79f6-4ed4-a29b-876d9e76fd48", "text": "We did not experience any significant impact or expense toour business; however, if the conflict is further extended, it could impact future product development, operations, and revenue or create other uncertainty for ourbusiness.35Fiscal Year 2024 SummaryYear EndedJan 28, 2024Jan 29, 2023Change($ in millions, except per share data)Revenue$60,922$26,974Up 126%Gross margin72.7 %56.9 %Up 15.8 ptsOperating expenses$11,329$11,132Up 2%Operating income$32,972$4,224Up 681%Net income$29,760$4,368Up 581%Net income per diluted share$11.93$1.74Up 586%We specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Revenue for fiscal year 2024 was $60.9 billion, up 126% from a year ago.Data Center revenue for fiscal year 2024 was up 217%.Strong demand was driven by enterprise software and consumer internet applications, and multipleindustry verticals including automotive, financial services, and healthcare.Customers across industry verticals access NVIDIA AI infrastructure both through thecloud and on-premises.Data Center compute revenue was up 244% in the fiscal year.Networking revenue was up 133% in the fiscal year.Gaming revenue for fiscal year 2024 was up 15%.The increase reflects higher sell-in to partners following the normalization of channel inventory levels andgrowing demand.Professional Visualization revenue for fiscal year 2024 was up 1%.Automotive revenue for the fiscal year 2024 was up 21%.The increase primarily reflected growth in self-driving platforms.Gross margin increased in fiscal year 2024, primarily driven by Data Center revenue growth and lower net inventory provisions as a percentage of revenue.Operating expenses increased for fiscal year 2024, driven by growth in employees and compensation increases.Fiscal year 2023 also included a $1.4 billionacquisition termination charge related to the proposed Arm transaction.Market Platform HighlightsData Center revenue for fiscal year 2024 was $47.5 billion, up 217% from fiscal year 2023.In Data Center, we launched AI inference platforms that combine ourfull-stack inference software with NVIDIA Ada, NVIDIA Hopper and NVIDIA Grace Hopper processors optimized for generative AI, LLMs and other AI workloads.We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.", "page_start": 35, "page_end": 36, "section_title": "NVIDIA CORPORATION", "score": 0.7092403}, {"chunk_id": "823438dd-9ce8-4ae9-924a-70c6bbf5121f", "text": "Form 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38The following table sets forth, for the periods indicated, certain items in our Consolidated Statements of Income expressed as a percentage of revenue.Year EndedJan 28, 2024Jan 29, 2023Revenue100.0 %100.0 %Cost of revenue27.343.1Gross profit72.756.9Operating expensesResearch and development14.227.2Sales, general and administrative4.49.1Acquisition termination cost—5.0Total operating expenses18.641.3Operating income54.115.6Interest income1.41.0Interest expense(0.4)(1.0)0.4(0.1)Other, net1.4(0.1)Other income (expense), netIncome before income tax55.515.5Income tax expense (benefit)6.6(0.7)48.9 %16.2 %Net incomeReportable SegmentsRevenue by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$47,405$15,068$32,337215 %13,51711,9061,611Graphics14 %$60,922$26,974$33,948Total126 %Operating Income by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$32,016$5,083$26,933530 %Graphics5,8464,5521,29428 %(4,890)(5,411)521All Other(10)%$32,972$4,224$28,748Total681 %Compute & Networking revenue – The year-on-year increase was due to higher Data Center revenue.Compute grew 266% due to higher shipments of theNVIDIA Hopper GPU computing platform for the training and inference of LLMs, recommendation engines and generative AI applications.Networking was up133% due to higher shipments of InfiniBand.Graphics revenue – The year-on-year increase was led by growth in Gaming of 15% driven by higher sell-in to partners following the normalization of channelinventory levels.Reportable segment operating income – The year-on-year increase in Compute & Networking and Graphics operating income was driven by higher revenue.39All Other operating loss - The year-on-year decrease was due to the $1.4 billion Arm acquisition termination cost in fiscal year 2023, partially offset by a $839million increase in stock-based compensation expense in fiscal year 2024.Concentration of Revenue", "page_start": 38, "page_end": 40, "section_title": "NVIDIA CORPORATION", "score": 0.68495965}, {"chunk_id": "0dd7fcd2-abcb-4469-bfdc-d6d5c6cb8eae", "text": "*$100 invested on 1/27/19 in stock and in indices, including reinvestment of dividends.Source: FactSet financial data and analytics.1/27/20191/26/20201/31/20211/30/20221/29/20231/28/2024NVIDIA Corporation$100.00$157.02$326.26$574.15$512.40$1,536.28S&P 500$100.00$126.17$144.83$175.25$163.63$199.83Nasdaq 100$100.00$136.15$194.20$218.68$185.67$268.13Item 6.[Reserved]33Item 7.Management's Discussion and Analysis of Financial Condition and Results of OperationsThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with “Item 1A.Risk Factors”, ourConsolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described elsewhere in this Annual Report onForm 10-K, before deciding to purchase, hold or sell shares of our common stock.OverviewOur Company and Our BusinessesNVIDIA pioneered accelerated computing to help solve the most challenging computational problems.Since our original focus on PC graphics, we haveexpanded to several other large and important computationally intensive fields.NVIDIA has leveraged its GPU architecture to create platforms for acceleratedcomputing, AI solutions, scientific computing, data science, AV, robotics, metaverse and 3D internet applications.Our two operating segments are \"Compute & Networking\" and \"Graphics.\" Refer to Note 17 of the Notes to the Consolidated Financial Statements in Part IV,Item 15 of this Annual Report on Form 10-K for additional information.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Recent Developments, Future Objectives and ChallengesDemand and Supply, Product Transitions, and New Products and Business ModelsDemand for our data center systems and products surged in fiscal year 2024.Entering fiscal year 2025, we are gathering customer demand indications acrossseveral product transitions.We have demand visibility for our new data center products ramping later in fiscal year 2025.We have increased our supply andcapacity purchases with existing suppliers, added new vendors and entered into prepaid manufacturing and capacity agreements.These increased purchasevolumes, the number of suppliers, and the integration of new vendors into our supply chain may create more complexity and execution risk.Our purchasecommitments and obligations for inventory and manufacturing capacity at the end of fiscal year 2024 were impacted by shortening lead times for certaincomponents.We may continue to enter into new supplier and capacity arrangements.Supply of Hopper architecture products is improving, and demand remainsvery strong.We expect our next-generation products to be supply-constrained based upon demand indications.", "page_start": 33, "page_end": 34, "section_title": "NVIDIA CORPORATION", "score": 0.65545857}, {"chunk_id": "af665b9d-5bc9-4f58-8c86-b860d78f48e8", "text": "TOP500 list, including 24 of the top 30 systems on the Green500 list.Gamers choose NVIDIA GPUs to enjoy immersive, increasingly cinematic virtual worlds.In addition to serving the growing number of gamers, the market for PCGPUs is expanding because of the burgeoning population of live streamers, broadcasters, artists, and creators.With the advent of generative AI, we expect abroader set of PC users to choose NVIDIA GPUs for running generative AI applications locally on their PC, which is critical for privacy, latency, and cost-sensitiveAI applications.Professional artists, architects and designers use NVIDIA partner products accelerated with our GPUs and software platform for a range of creative and designuse cases, such as creating visual effects in movies or designing buildings and products.In addition, generative AI is expanding the market for our workstationclass GPUs, as more enterprise customers develop and deploy AI applications with their data on-premises.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Our BusinessesWe report our business results in two segments.The Compute & Networking segment is comprised of our Data Center accelerated computing platforms and end-to-end networking platforms including Quantumfor InfiniBand and Spectrum for Ethernet; our NVIDIA DRIVE automated-driving platform and automotive development agreements; Jetson robotics and otherembedded platforms; NVIDIA AI Enterprise and other software; and DGX Cloud software and services.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure; Quadro/NVIDIARTX GPUs for enterprise workstation graphics; virtual GPU, or vGPU, software for cloud-based visual and virtual computing; automotive platforms forinfotainment systems; and Omniverse Enterprise software for building and operating metaverse and 3D internet applications.Our MarketsWe specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Data CenterThe NVIDIA Data Center platform is focused on accelerating the most compute-intensive workloads, such as AI, data analytics, graphics and scientificcomputing, delivering significantly better performance and power efficiency relative to conventional CPU-only approaches.It is deployed in cloud, hyperscale,on-premises and edge data centers.The platform consists of compute and networking offerings typically delivered to customers as systems, subsystems, ormodules, along with software and services.Our compute offerings include supercomputing platforms and servers, bringing together our energy efficient GPUs, DPUs, interconnects, and fully optimized AIand high-performance computing, or HPC, software stacks.In addition, they include NVIDIA AI Enterprise software; our DGX Cloud service; and a growing bodyof acceleration libraries, APIs, SDKs, and domain-specific application frameworks.Our networking offerings include end-to-end platforms for InfiniBand and Ethernet, consisting of network adapters, cables, DPUs, and switch systems, as well asa full software stack.", "page_start": 5, "page_end": 5, "section_title": "NVIDIA CORPORATION", "score": 0.65383595}, {"chunk_id": "f9f46931-b66e-4235-8451-f372d6442a23", "text": "In addition, they include NVIDIA AI Enterprise software; our DGX Cloud service; and a growing bodyof acceleration libraries, APIs, SDKs, and domain-specific application frameworks.Our networking offerings include end-to-end platforms for InfiniBand and Ethernet, consisting of network adapters, cables, DPUs, and switch systems, as well asa full software stack.This has enabled us to architect data center-scale computing platforms that can interconnect thousands of compute nodes with highperformance networking.While historically the server was the unit of computing, as AI and HPC workloads have become extremely large spanning thousands ofcompute nodes, the data center has become the new unit of computing, with networking as an integral part.Our end customers include the world’s leading public cloud and consumer internet companies, thousands of enterprises and startups, and public sector entities.We work with industry leaders to help build or transform their applications and data center infrastructure.Our direct customers include original equipmentmanufacturers, or OEMs, original device manufacturers, or ODMs, system integrators and distributors which we partner with to help bring our products tomarket.We also have partnerships in automotive, healthcare, financial services, manufacturing, and retail among others, to accelerate the adoption of AI.5At the foundation of the NVIDIA accelerated computing platform are our GPUs, which excel at parallel workloads such as the training and inferencing of neuralnetworks.They are available in the NVIDIA accelerated computing platform and in industry standard servers from every major cloud provider and server maker.Beyond GPUs, our data center platform expanded to include DPUs in fiscal year 2022 and CPUs in fiscal year 2024.We can optimize across the entirecomputing, networking and storage stack to deliver data center-scale computing solutions.While our approach starts with powerful chips, what makes it a full-stack computing platform is our large body of software, including the CUDA parallelprogramming model, the CUDA-X collection of acceleration libraries, APIs, SDKs, and domain-specific application frameworks.In addition to software delivered to customers as an integral part of our data center computing platform, we offer paid licenses to NVIDIA AI Enterprise, acomprehensive suite of enterprise-grade AI software and NVIDIA vGPU software for graphics-rich virtual desktops and workstations.In fiscal year 2024, we launched the NVIDIA DGX Cloud, an AI-training-as-a-service platform which includes cloud-based infrastructure and software for AI,customizable pretrained AI models, and access to NVIDIA experts.We have partnered with leading cloud service providers to host this service in their datacenters.GamingGaming is the largest entertainment industry, with PC gaming as the predominant platform.Many factors propel its growth, including new high production valuegames and franchises, the continued rise of competitive gaming, or eSports, social connectivity and the increasing popularity of game streamers, modders, orgamers who remaster games, and creators.Our gaming platforms leverage our GPUs and sophisticated software to enhance the gaming experience with smoother, higher quality graphics.", "page_start": 5, "page_end": 6, "section_title": "NVIDIA CORPORATION", "score": 0.6517037}, {"chunk_id": "f2207e14-4c6c-421b-849b-76359737cca1", "text": "Germany, India, Israel, and Taiwan for fiscal years 2005 through 2023.Note 15 - Shareholders’ EquityCapital Return ProgramIn August 2023, our Board of Directors approved an increase to our share repurchase program of an additional $25.0 billion, without expiration.During fiscalyear 2024, we repurchased 21 million shares of our common stock for $9.7 billion.As of January 28, 2024, we were authorized, subject to certain specifications,to repurchase additional shares of our common stock up to $22.5 billion.From January 29, 2024 through February 16, 2024, we repurchased 2.8 million sharesfor $1.9 billion pursuant to a Rule 10b5-1 trading plan.Our share repurchase program aims to offset dilution from shares issued to employees.We may pursueadditional share repurchases as we weigh market factors and other investment opportunities.During fiscal years 2024, 2023, and 2022, we paid $395 million, $398 million, and $399 million in cash dividends to our shareholders, respectively.Our cashdividend program and the payment of future cash dividends under that program are subject to our Board of Directors' continuing determination that the dividendprogram and the declaration of dividends thereunder are in the best interests of our shareholders.In fiscal year 2022, we retired our existing 349 million treasury shares.These shares assumed the status of authorized and unissued shares upon retirement.The excess of repurchase price over par value was allocated between additional paid-in capital and retained earnings, resulting in a reduction in additional paidin capital by $20 million and retained earnings by $12.0 billion.Any future repurchased shares will assume the status of authorized and unissued shares.77NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Note 16 - Employee Retirement PlansWe provide tax-qualified defined contribution plans to eligible employees in the U.S.and certain other countries.Our contribution expense for fiscal years 2024,2023, and 2022 was $255 million, $227 million, and $168 million, respectively.Note 17 - Segment InformationOur Chief Executive Officer, who is considered to be our chief operating decision maker, or CODM, reviews financial information presented on an operatingsegment basis for purposes of making decisions and assessing financial performance.The Compute & Networking segment includes our Data Center accelerated computing platform; networking; automotive artificial intelligence, or AI, Cockpit,autonomous driving development agreements, and autonomous vehicle solutions; electric vehicle computing platforms; Jetson for robotics and other embeddedplatforms; NVIDIA AI Enterprise and other software; and DGX Cloud.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions forgaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across our", "page_start": 77, "page_end": 78, "section_title": "NVIDIA CORPORATION", "score": 0.64830834}, {"chunk_id": "66e77043-a370-4255-b50b-931d8a75e255", "text": "We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.In the fourth quarter of fiscal year 2024, largecloud providers represented more than half of our Data Center revenue, supporting both internal workloads and external customers.We announced NVIDIASpectrum-X, an accelerated networking platform for AI.Gaming revenue for fiscal year 2024 was $10.4 billion, up 15% from fiscal year 2023.In Gaming, we launched the GeForce RTX 4060 and 4070 GPUs based onthe NVIDIA Ada Lovelace architecture.We announced NVIDIA Avatar Cloud Engine for Games, a custom AI model foundry service using AI-powered naturallanguage interactions to transform games and launched DLSS 3.5 Ray Reconstruction.Additionally, we released TensorRT-LLM for Windows and launchedGeForce RTX 40-Series SUPER GPUs.Gaming reached a milestone of 500 AI-powered RTX games and applications utilizing NVIDIA DLSS, ray tracing andother NVIDIA RTX technologies.Professional Visualization revenue for fiscal year 2024 was $1.6 billion, up 1% from fiscal year 2023.In Professional Visualization, we announced new GPUsbased on the NVIDIA RTX Ada Lovelace architecture, and announced NVIDIA Omniverse Cloud, a fully managed service running in Microsoft Azure, for thedevelopment and deployment of industrial metaverse applications.Automotive revenue for fiscal year 2024 was $1.1 billion, up 21% from fiscal year 2023.In Automotive, we announced a partnership with MediaTek, which willdevelop mainstream automotive systems on chips for global OEMs integrating a new NVIDIA GPU chiplet IP for AI and graphics.We furthered our collaborationwith Foxconn to develop next-generation36electric vehicles, and announced further adoption of NVIDIA DRIVE platform with BYD, XPENG, GWM, Li Auto, ZEEKR and Xiaomi.Critical Accounting EstimatesOur consolidated financial statements are prepared in accordance with accounting principles generally accepted in the United States, or U.S.GAAP.Thepreparation of these financial statements requires us to make estimates and judgments that affect the reported amounts of assets, liabilities, revenue, cost ofrevenue, expenses and related disclosure of contingencies.Critical accounting estimates are those estimates that involve a significant level of estimationuncertainty and could have a material impact on our financial condition or results of operations.We have critical accounting estimates in the areas of inventories,revenue recognition, and income taxes.Refer to Note 1 of the Notes to the Consolidated Financial Statements in Part IV, Item 15 of this Annual Report on Form10-K for a summary of significant accounting policies.InventoriesWe charge cost of sales for inventory provisions to write-down our inventory to the lower of cost or net realizable value or for obsolete or excess inventory, and", "page_start": 36, "page_end": 37, "section_title": "NVIDIA CORPORATION", "score": 0.6091483}, {"chunk_id": "a39adb5c-8c62-4daa-b471-d64ac0bed3a6", "text": "gaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across ourunified architecture and therefore allocated between our two segments.The “All Other” category includes the expenses that our CODM does not assign to either Compute & Networking or Graphics for purposes of making operatingdecisions or assessing financial performance.The expenses include stock-based compensation expense, corporate infrastructure and support costs, acquisitionrelated and other costs, intellectual property related, or IP-related costs, acquisition termination cost, and other non-recurring charges and benefits that ourCODM deems to be enterprise in nature.Our CODM does not review any information regarding total assets on a reportable segment basis.Depreciation and amortization expense directly attributable toeach reportable segment is included in operating results for each segment.However, our CODM does not evaluate depreciation and amortization expense byoperating segment and, therefore, it is not separately presented.There is no intersegment revenue.The accounting policies for segment reporting are the sameas for our consolidated financial statements.The table below presents details of our reportable segments and the “All Other” category.Compute &NetworkingGraphicsAll OtherConsolidated(In millions)Year Ended Jan 28, 2024:Revenue$47,405$13,517$—$60,922Operating income (loss)$32,016$5,846$(4,890)$32,972Year Ended Jan 29, 2023:Revenue$15,068$11,906$—$26,974Operating income (loss)$5,083$4,552$(5,411)$4,224Year Ended Jan 30, 2022:Revenue$11,046$15,868$—$26,914Operating income (loss)$4,598$8,492$(3,049)$10,04178NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022(In millions)Reconciling items included in \"All Other\" category:Stock-based compensation expense$(3,549)$(2,710)$(2,004)Unallocated cost of revenue and operating expenses(728)(595)(399)Acquisition-related and other costs(583)(674)(636)IP-related and legal settlement costs(40)(23)(10)Restructuring costs and other—(54)—Acquisition termination cost—(1,353)—10(2)—Other$(4,890)$(5,411)$(3,049)TotalRevenue by geographic areas is designated based upon the billing location of the customer.", "page_start": 78, "page_end": 79, "section_title": "NVIDIA CORPORATION", "score": 0.60789484}], "inference_nvidia_02": [{"chunk_id": "0dd7fcd2-abcb-4469-bfdc-d6d5c6cb8eae", "text": "*$100 invested on 1/27/19 in stock and in indices, including reinvestment of dividends.Source: FactSet financial data and analytics.1/27/20191/26/20201/31/20211/30/20221/29/20231/28/2024NVIDIA Corporation$100.00$157.02$326.26$574.15$512.40$1,536.28S&P 500$100.00$126.17$144.83$175.25$163.63$199.83Nasdaq 100$100.00$136.15$194.20$218.68$185.67$268.13Item 6.[Reserved]33Item 7.Management's Discussion and Analysis of Financial Condition and Results of OperationsThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with “Item 1A.Risk Factors”, ourConsolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described elsewhere in this Annual Report onForm 10-K, before deciding to purchase, hold or sell shares of our common stock.OverviewOur Company and Our BusinessesNVIDIA pioneered accelerated computing to help solve the most challenging computational problems.Since our original focus on PC graphics, we haveexpanded to several other large and important computationally intensive fields.NVIDIA has leveraged its GPU architecture to create platforms for acceleratedcomputing, AI solutions, scientific computing, data science, AV, robotics, metaverse and 3D internet applications.Our two operating segments are \"Compute & Networking\" and \"Graphics.\" Refer to Note 17 of the Notes to the Consolidated Financial Statements in Part IV,Item 15 of this Annual Report on Form 10-K for additional information.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Recent Developments, Future Objectives and ChallengesDemand and Supply, Product Transitions, and New Products and Business ModelsDemand for our data center systems and products surged in fiscal year 2024.Entering fiscal year 2025, we are gathering customer demand indications acrossseveral product transitions.We have demand visibility for our new data center products ramping later in fiscal year 2025.We have increased our supply andcapacity purchases with existing suppliers, added new vendors and entered into prepaid manufacturing and capacity agreements.These increased purchasevolumes, the number of suppliers, and the integration of new vendors into our supply chain may create more complexity and execution risk.Our purchasecommitments and obligations for inventory and manufacturing capacity at the end of fiscal year 2024 were impacted by shortening lead times for certaincomponents.We may continue to enter into new supplier and capacity arrangements.Supply of Hopper architecture products is improving, and demand remainsvery strong.We expect our next-generation products to be supply-constrained based upon demand indications.", "page_start": 33, "page_end": 34, "section_title": "NVIDIA CORPORATION", "score": 0.6384929}, {"chunk_id": "823438dd-9ce8-4ae9-924a-70c6bbf5121f", "text": "Form 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38The following table sets forth, for the periods indicated, certain items in our Consolidated Statements of Income expressed as a percentage of revenue.Year EndedJan 28, 2024Jan 29, 2023Revenue100.0 %100.0 %Cost of revenue27.343.1Gross profit72.756.9Operating expensesResearch and development14.227.2Sales, general and administrative4.49.1Acquisition termination cost—5.0Total operating expenses18.641.3Operating income54.115.6Interest income1.41.0Interest expense(0.4)(1.0)0.4(0.1)Other, net1.4(0.1)Other income (expense), netIncome before income tax55.515.5Income tax expense (benefit)6.6(0.7)48.9 %16.2 %Net incomeReportable SegmentsRevenue by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$47,405$15,068$32,337215 %13,51711,9061,611Graphics14 %$60,922$26,974$33,948Total126 %Operating Income by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$32,016$5,083$26,933530 %Graphics5,8464,5521,29428 %(4,890)(5,411)521All Other(10)%$32,972$4,224$28,748Total681 %Compute & Networking revenue – The year-on-year increase was due to higher Data Center revenue.Compute grew 266% due to higher shipments of theNVIDIA Hopper GPU computing platform for the training and inference of LLMs, recommendation engines and generative AI applications.Networking was up133% due to higher shipments of InfiniBand.Graphics revenue – The year-on-year increase was led by growth in Gaming of 15% driven by higher sell-in to partners following the normalization of channelinventory levels.Reportable segment operating income – The year-on-year increase in Compute & Networking and Graphics operating income was driven by higher revenue.39All Other operating loss - The year-on-year decrease was due to the $1.4 billion Arm acquisition termination cost in fiscal year 2023, partially offset by a $839million increase in stock-based compensation expense in fiscal year 2024.Concentration of Revenue", "page_start": 38, "page_end": 40, "section_title": "NVIDIA CORPORATION", "score": 0.5927236}, {"chunk_id": "ddc05d6e-79f6-4ed4-a29b-876d9e76fd48", "text": "We did not experience any significant impact or expense toour business; however, if the conflict is further extended, it could impact future product development, operations, and revenue or create other uncertainty for ourbusiness.35Fiscal Year 2024 SummaryYear EndedJan 28, 2024Jan 29, 2023Change($ in millions, except per share data)Revenue$60,922$26,974Up 126%Gross margin72.7 %56.9 %Up 15.8 ptsOperating expenses$11,329$11,132Up 2%Operating income$32,972$4,224Up 681%Net income$29,760$4,368Up 581%Net income per diluted share$11.93$1.74Up 586%We specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Revenue for fiscal year 2024 was $60.9 billion, up 126% from a year ago.Data Center revenue for fiscal year 2024 was up 217%.Strong demand was driven by enterprise software and consumer internet applications, and multipleindustry verticals including automotive, financial services, and healthcare.Customers across industry verticals access NVIDIA AI infrastructure both through thecloud and on-premises.Data Center compute revenue was up 244% in the fiscal year.Networking revenue was up 133% in the fiscal year.Gaming revenue for fiscal year 2024 was up 15%.The increase reflects higher sell-in to partners following the normalization of channel inventory levels andgrowing demand.Professional Visualization revenue for fiscal year 2024 was up 1%.Automotive revenue for the fiscal year 2024 was up 21%.The increase primarily reflected growth in self-driving platforms.Gross margin increased in fiscal year 2024, primarily driven by Data Center revenue growth and lower net inventory provisions as a percentage of revenue.Operating expenses increased for fiscal year 2024, driven by growth in employees and compensation increases.Fiscal year 2023 also included a $1.4 billionacquisition termination charge related to the proposed Arm transaction.Market Platform HighlightsData Center revenue for fiscal year 2024 was $47.5 billion, up 217% from fiscal year 2023.In Data Center, we launched AI inference platforms that combine ourfull-stack inference software with NVIDIA Ada, NVIDIA Hopper and NVIDIA Grace Hopper processors optimized for generative AI, LLMs and other AI workloads.We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.", "page_start": 35, "page_end": 36, "section_title": "NVIDIA CORPORATION", "score": 0.591389}, {"chunk_id": "4d665fa7-dd83-464c-908e-2634e7e09406", "text": "10or generate enough renewable energy to match 100% of our global electricity usage for our offices and data centers.In fiscal year 2023, we increased thepercentage of our total electricity use matched by renewable energy purchases to 44%.By fiscal year 2026, we aim to engage manufacturing supplierscomprising at least 67% of NVIDIA’s scope 3 category 1 GHG emissions with goal of effecting supplier adoption of science-based targets.Whether it is creation of technology to power next-generation laptops or designs to support high-performance supercomputers, improving energy efficiency isimportant in our research, development, and design processes.GPU-accelerated computing is inherently more energy efficient than traditional computing formany workloads because it is optimized for throughput, performance per watt, and certain AI workloads.The energy efficiency of our products is evidenced byour continued strong presence on the Green500 list of the most energy-efficient systems.We powered 24 of the top 30 most energy efficient systems, includingthe top supercomputer, on the Green500 list.We plan to build Earth-2, a digital twin of the Earth on NVIDIA AI and NVIDIA Omniverse platforms.Earth-2 will enable scientists, companies, and policy makersto do ultra-high-resolution predictions of the impact of climate change and explore mitigation and adaptation strategies.Human Capital ManagementWe believe that our employees are our greatest assets, and they play a key role in creating long-term value for our stakeholders.As of the end of fiscal year2024, we had approximately 29,600 employees in 36 countries, 22,200 were engaged in research and development and 7,400 were engaged in sales,marketing, operations, and administrative positions.The Compensation Committee of our Board of Directors assists in the oversight of policies and strategiesrelating to human capital management.To be competitive and execute our business strategy successfully, we must recruit, develop, and retain talented employees, including qualified executives,scientists, engineers, and technical and non-technical staff.RecruitmentAs the demand for global technical talent continues to be competitive, we have grown our technical workforce and have been successful in attracting top talentto NVIDIA.We have attracted talent globally through our strong employer brand and differentiated hiring strategies for college, professional, and leadershiptalent.Our workforce is 83% technical and 49% hold advanced degrees.Additionally, we have increased focus on diversity recruiting, resulting in an increase inglobal female hiring in each channel.Our own employees help to surface top talent, with over 40% of our new hires in fiscal year 2024 coming from employeereferrals.Development and RetentionTo support employee development, we provide opportunities to learn on-the-job through training courses, targeted development programs, mentoring and peercoaching and ongoing feedback.We have a library of live and on-demand learning experiences that include workshops, panel discussions, and speaker forums.We create learning paths focused on our most common development needs and constantly upgrade our offerings to ensure that our employees are exposed tothe most current content and technologies available.", "page_start": 10, "page_end": 11, "section_title": "NVIDIA CORPORATION", "score": 0.5862731}, {"chunk_id": "f2207e14-4c6c-421b-849b-76359737cca1", "text": "Germany, India, Israel, and Taiwan for fiscal years 2005 through 2023.Note 15 - Shareholders’ EquityCapital Return ProgramIn August 2023, our Board of Directors approved an increase to our share repurchase program of an additional $25.0 billion, without expiration.During fiscalyear 2024, we repurchased 21 million shares of our common stock for $9.7 billion.As of January 28, 2024, we were authorized, subject to certain specifications,to repurchase additional shares of our common stock up to $22.5 billion.From January 29, 2024 through February 16, 2024, we repurchased 2.8 million sharesfor $1.9 billion pursuant to a Rule 10b5-1 trading plan.Our share repurchase program aims to offset dilution from shares issued to employees.We may pursueadditional share repurchases as we weigh market factors and other investment opportunities.During fiscal years 2024, 2023, and 2022, we paid $395 million, $398 million, and $399 million in cash dividends to our shareholders, respectively.Our cashdividend program and the payment of future cash dividends under that program are subject to our Board of Directors' continuing determination that the dividendprogram and the declaration of dividends thereunder are in the best interests of our shareholders.In fiscal year 2022, we retired our existing 349 million treasury shares.These shares assumed the status of authorized and unissued shares upon retirement.The excess of repurchase price over par value was allocated between additional paid-in capital and retained earnings, resulting in a reduction in additional paidin capital by $20 million and retained earnings by $12.0 billion.Any future repurchased shares will assume the status of authorized and unissued shares.77NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Note 16 - Employee Retirement PlansWe provide tax-qualified defined contribution plans to eligible employees in the U.S.and certain other countries.Our contribution expense for fiscal years 2024,2023, and 2022 was $255 million, $227 million, and $168 million, respectively.Note 17 - Segment InformationOur Chief Executive Officer, who is considered to be our chief operating decision maker, or CODM, reviews financial information presented on an operatingsegment basis for purposes of making decisions and assessing financial performance.The Compute & Networking segment includes our Data Center accelerated computing platform; networking; automotive artificial intelligence, or AI, Cockpit,autonomous driving development agreements, and autonomous vehicle solutions; electric vehicle computing platforms; Jetson for robotics and other embeddedplatforms; NVIDIA AI Enterprise and other software; and DGX Cloud.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions forgaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across our", "page_start": 77, "page_end": 78, "section_title": "NVIDIA CORPORATION", "score": 0.5855056}, {"chunk_id": "ea4279b8-2d86-4d6d-8b49-3cae1b849f61", "text": "company with data-center-scale offerings that are reshaping industry.Our full-stack includes the foundational CUDA programming model that runs on all NVIDIA GPUs, as well as hundreds of domain-specific software libraries,software development kits, or SDKs, and Application Programming Interfaces, or APIs.This deep and broad software stack accelerates the performance andeases the deployment of NVIDIA accelerated computing for computationally intensive workloads such as artificial intelligence, or AI, model training andinference, data analytics, scientific computing, and 3D graphics, with vertical-specific optimizations to address industries ranging from healthcare and telecom toautomotive and manufacturing.Our data-center-scale offerings are comprised of compute and networking solutions that can scale to tens of thousands of GPU-accelerated serversinterconnected to function as a single giant computer; this type of data center architecture and scale is needed for the development and deployment of modernAI applications.The GPU was initially used to simulate human imagination, enabling the virtual worlds of video games and films.Today, it also simulates human intelligence,enabling a deeper understanding of the physical world.Its parallel processing capabilities, supported by thousands of computing cores, are essential for deeplearning algorithms.This form of AI, in which software writes itself by learning from large amounts of data, can serve as the brain of computers, robots and selfdriving cars that can perceive and understand the world.GPU-powered AI solutions are being developed by thousands of enterprises to deliver services andproducts that would have been immensely difficult or even impossible with traditional coding.Examples include generative AI, which can create new contentsuch as text, code, images, audio, video, and molecule structures, and recommendation systems, which can recommend highly relevant content such asproducts, services, media or ads using deep neural networks trained on vast datasets that capture the user preferences.NVIDIA has a platform strategy, bringing together hardware, systems, software, algorithms, libraries, and services to create unique value for the markets weserve.While the computing requirements of these end markets are diverse, we address them with a unified underlying architecture leveraging our GPUs andnetworking and software stacks.The programmable nature of our architecture allows us to support several multi-billion-dollar end markets with the sameunderlying technology by using a variety of software stacks developed either internally or by third-party developers and partners.The large and growing numberof developers and installed base across our platforms strengthens our ecosystem and increases the value of our platform to our customers.Innovation is at our core.We have invested over $45.3 billion in research and development since our inception, yielding inventions that are essential to moderncomputing.Our invention of the GPU in 1999 sparked the growth of the PC gaming market and redefined computer graphics.With our introduction of the CUDAprogramming model in 2006, we opened the parallel processing capabilities of our GPU to a broad range of compute-intensive applications, paving the way forthe emergence of modern AI.", "page_start": 4, "page_end": 4, "section_title": "NVIDIA CORPORATION", "score": 0.5683161}, {"chunk_id": "66e77043-a370-4255-b50b-931d8a75e255", "text": "We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.In the fourth quarter of fiscal year 2024, largecloud providers represented more than half of our Data Center revenue, supporting both internal workloads and external customers.We announced NVIDIASpectrum-X, an accelerated networking platform for AI.Gaming revenue for fiscal year 2024 was $10.4 billion, up 15% from fiscal year 2023.In Gaming, we launched the GeForce RTX 4060 and 4070 GPUs based onthe NVIDIA Ada Lovelace architecture.We announced NVIDIA Avatar Cloud Engine for Games, a custom AI model foundry service using AI-powered naturallanguage interactions to transform games and launched DLSS 3.5 Ray Reconstruction.Additionally, we released TensorRT-LLM for Windows and launchedGeForce RTX 40-Series SUPER GPUs.Gaming reached a milestone of 500 AI-powered RTX games and applications utilizing NVIDIA DLSS, ray tracing andother NVIDIA RTX technologies.Professional Visualization revenue for fiscal year 2024 was $1.6 billion, up 1% from fiscal year 2023.In Professional Visualization, we announced new GPUsbased on the NVIDIA RTX Ada Lovelace architecture, and announced NVIDIA Omniverse Cloud, a fully managed service running in Microsoft Azure, for thedevelopment and deployment of industrial metaverse applications.Automotive revenue for fiscal year 2024 was $1.1 billion, up 21% from fiscal year 2023.In Automotive, we announced a partnership with MediaTek, which willdevelop mainstream automotive systems on chips for global OEMs integrating a new NVIDIA GPU chiplet IP for AI and graphics.We furthered our collaborationwith Foxconn to develop next-generation36electric vehicles, and announced further adoption of NVIDIA DRIVE platform with BYD, XPENG, GWM, Li Auto, ZEEKR and Xiaomi.Critical Accounting EstimatesOur consolidated financial statements are prepared in accordance with accounting principles generally accepted in the United States, or U.S.GAAP.Thepreparation of these financial statements requires us to make estimates and judgments that affect the reported amounts of assets, liabilities, revenue, cost ofrevenue, expenses and related disclosure of contingencies.Critical accounting estimates are those estimates that involve a significant level of estimationuncertainty and could have a material impact on our financial condition or results of operations.We have critical accounting estimates in the areas of inventories,revenue recognition, and income taxes.Refer to Note 1 of the Notes to the Consolidated Financial Statements in Part IV, Item 15 of this Annual Report on Form10-K for a summary of significant accounting policies.InventoriesWe charge cost of sales for inventory provisions to write-down our inventory to the lower of cost or net realizable value or for obsolete or excess inventory, and", "page_start": 36, "page_end": 37, "section_title": "NVIDIA CORPORATION", "score": 0.55498326}, {"chunk_id": "c58f6814-3792-439d-ae9a-52a1a68b16ef", "text": "Risk FactorsRisks Related to Our Industry and MarketsFailure to meet the evolving needs of our industry and markets may adversely impact our financial results.Our accelerated computing platforms experience rapid changes in technology, customer requirements, competitive products, and industry standards.Our success depends on our ability to:•timely identify industry changes, adapt our strategies, and develop new or enhance and maintain existing products and technologies that meet theevolving needs of these markets, including due to unexpected changes in industry standards or disruptive technological innovation that could renderour products incompatible with products developed by other companies;14•develop or acquire new products and technologies through investments in research and development;•launch new offerings with new business models including software, services, and cloud solutions, as well as software-, infrastructure-, or platform-as-aservice solutions;•expand the ecosystem for our products and technologies;•meet evolving and prevailing customer and industry safety, security, reliability expectations, and compliance standards;•manage product and software lifecycles to maintain customer and end-user satisfaction;•develop, acquire, maintain, and secure access to the internal and external infrastructure needed to scale our business, including sufficient energy forpowering data centers using our products, acquisition integrations, customer support, e-commerce, IP licensing capabilities and cloud service capacity;and•complete technical, financial, operational, compliance, sales and marketing investments for the above activities.We have invested in research and development in markets where we have a limited operating history, which may not produce meaningful revenue for severalyears, if at all.If we fail to develop or monetize new products and technologies, or if they do not become widely adopted, our financial results could be adverselyaffected.Obtaining design wins may involve a lengthy process and depends on our ability to anticipate and provide features and functionality that customers willdemand.They also do not guarantee revenue.Failure to obtain a design win may prevent us from obtaining future design wins in subsequent generations.Wecannot ensure that the products and technologies we bring to market will provide value to our customers and partners.If we fail any of these key successcriteria, our financial results may be harmed.We have begun offering enterprise customers NVIDIA DGX Cloud services directly and through our network of partners, which include cloud-basedinfrastructure, software and services for training and deploying AI models, and NVIDIA AI Foundations for customizable pretrained AI models.We have partneredwith CSPs to host such software and services in their data centers, and we entered and may continue to enter into multi-year cloud service agreements tosupport these offerings and our research and development activities.The timing and availability of these cloud services has changed and may continue tochange, impacting our revenue, expenses, and development timelines.NVIDIA DGX Cloud services may not be successful and will take time, resources, andinvestment.", "page_start": 14, "page_end": 15, "section_title": "NVIDIA CORPORATION", "score": 0.55480075}], "inference_attention_01": [{"chunk_id": "72538026-a082-4975-9165-c903508ae7da", "text": "In this work, we use sine and cosine functions of different frequencies:PE(pos,2i) = sin(pos/100002i/dmodel)PE(pos,2i+1) = cos(pos/100002i/dmodel)where pos is the position and i is the dimension.That is, each dimension of the positional encodingcorresponds to a sinusoid.The wavelengths form a geometric progression from 2π to 10000 · 2π.Wechose this function because we hypothesized it would allow the model to easily learn to attend byrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function ofPEpos.We also experimented with using learned positional embeddings [9] instead, and found that the twoversions produced nearly identical results (see Table 3 row (E)).We chose the sinusoidal versionbecause it may allow the model to extrapolate to sequence lengths longer than the ones encounteredduring training.4Why Self-AttentionIn this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hiddenlayer in a typical sequence transduction encoder or decoder.Motivating our use of self-attention weconsider three desiderata.One is the total computational complexity per layer.Another is the amount of computation that canbe parallelized, as measured by the minimum number of sequential operations required.The third is the path length between long-range dependencies in the network.Learning long-rangedependencies is a key challenge in many sequence transduction tasks.One key factor affecting theability to learn such dependencies is the length of the paths forward and backward signals have totraverse in the network.The shorter these paths between any combination of positions in the inputand output sequences, the easier it is to learn long-range dependencies [12].Hence we also comparethe maximum path length between any two input and output positions in networks composed of thedifferent layer types.As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentiallyexecuted operations, whereas a recurrent layer requires O(n) sequential operations.In terms ofcomputational complexity, self-attention layers are faster than recurrent layers when the sequence6length n is smaller than the representation dimensionality d, which is most often the case withsentence representations used by state-of-the-art models in machine translations, such as word-piece[38] and byte-pair [31] representations.To improve computational performance for tasks involvingvery long sequences, self-attention could be restricted to considering only a neighborhood of size r inthe input sequence centered around the respective output position.This would increase the maximumpath length to O(n/r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k < n does not connect all pairs of input and outputpositions.", "page_start": 6, "page_end": 7, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.5215101}, {"chunk_id": "5151352b-ae46-44f3-83b0-8b8748547941", "text": "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networksin particular, have been firmly established as state of the art approaches in sequence modeling andtransduction problems such as language modeling and machine translation [35, 2, 5].Numerousefforts have since continued to push the boundaries of recurrent language models and encoder-decoderarchitectures [38, 24, 15].Recurrent models typically factor computation along the symbol positions of the input and outputsequences.Aligning the positions to steps in computation time, they generate a sequence of hiddenstates ht, as a function of the previous hidden state ht−1 and the input for position t.This inherentlysequential nature precludes parallelization within training examples, which becomes critical at longersequence lengths, as memory constraints limit batching across examples.Recent work has achievedsignificant improvements in computational efficiency through factorization tricks [21] and conditionalcomputation [32], while also improving model performance in case of the latter.The fundamentalconstraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance inthe input or output sequences [2, 19].In all but a few cases [27], however, such attention mechanismsare used in conjunction with a recurrent network.In this work we propose the Transformer, a model architecture eschewing recurrence and insteadrelying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for significantly more parallelization and can reach a new state of the art intranslation quality after being trained for as little as twelve hours on eight P100 GPUs.2BackgroundThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic buildingblock, computing hidden representations in parallel for all input and output positions.In these models,the number of operations required to relate signals from two arbitrary input or output positions growsin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.This makesit more difficult to learn dependencies between distant positions [12].In the Transformer this isreduced to a constant number of operations, albeit at the cost of reduced effective resolution dueto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,", "page_start": 1, "page_end": 2, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4452647}, {"chunk_id": "df8dae53-302d-45cc-8864-b1ae084274b0", "text": "applicationFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of thesentence.We give two such examples above, from two different heads from the encoder self-attentionat layer 5 of 6.The heads clearly learned to perform different tasks.15", "page_start": 15, "page_end": 15, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.41562888}, {"chunk_id": "222d0339-104c-4f96-bec5-f86a9511af81", "text": "[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V.Le.Massive exploration of neuralmachine translation architectures.CoRR, abs/1703.03906, 2017.[4] Jianpeng Cheng, Li Dong, and Mirella Lapata.Long short-term memory-networks for machinereading.arXiv preprint arXiv:1601.06733, 2016.10[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,and Yoshua Bengio.Learning phrase representations using rnn encoder-decoder for statisticalmachine translation.CoRR, abs/1406.1078, 2014.[6] Francois Chollet.Xception: Deep learning with depthwise separable convolutions.arXivpreprint arXiv:1610.02357, 2016.[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio.Empirical evaluationof gated recurrent neural networks on sequence modeling.CoRR, abs/1412.3555, 2014.[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A.Smith.Recurrent neuralnetwork grammars.In Proc.of NAACL, 2016.[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N.Dauphin.Convolutional sequence to sequence learning.arXiv preprint arXiv:1705.03122v2, 2017.arXiv preprint[10] Alex Graves.Generating sequences with recurrent neural networks.arXiv:1308.0850, 2013.[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition.In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pages 770–778, 2016.[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber.Gradient flow inrecurrent nets: the difficulty of learning long-term dependencies, 2001.[13] Sepp Hochreiter and Jürgen Schmidhuber.Long short-term memory.Neural computation,9(8):1735–1780, 1997.[14] Zhongqiang Huang and Mary Harper.Self-training PCFG grammars with latent annotationsacross languages.In Proceedings of the 2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 832–841.ACL, August 2009.[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.Exploringthe limits of language modeling.arXiv preprint arXiv:1602.02410, 2016.[16] Łukasz Kaiser and Samy Bengio.Can active memory replace attention?In Advances in NeuralInformation Processing Systems, (NIPS), 2016.[17] Łukasz Kaiser and Ilya Sutskever.Neural GPUs learn algorithms.In International Conferenceon Learning Representations (ICLR), 2016.[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu.Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,2017.[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M.Rush.Structured attention networks.In International Conference on Learning Representations, 2017.[20] Diederik Kingma and Jimmy Ba.Adam: A method for stochastic optimization.In ICLR, 2015.[21] Oleksii Kuchaiev and Boris Ginsburg.Factorization tricks for LSTM networks.arXiv preprintarXiv:1703.10722, 2017.[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, BowenZhou, and Yoshua Bengio.A structured self-attentive sentence embedding.arXiv preprintarXiv:1703.03130, 2017.[23] Minh-Thang Luong, Quoc V.Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.Multi-tasksequence to sequence learning.arXiv preprint arXiv:1511.", "page_start": 10, "page_end": 11, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4133314}, {"chunk_id": "d3264ea3-3920-4ed4-bbb0-d3dcc4fc4658", "text": "should<pad>neverwhatLawThejustthisarebutwillmywebebeitsinis-,,..<EOS>,,its-thisTheperfectmyisbutjustbebeinarewhatweLawnever<pad>missingshouldwillopinionapplicationFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution.Top:Full attentions for head 5.Bottom: Isolated attentions from just the word ‘its’ for attention heads 5and 6.Note that the attentions are very sharp for this word.14applicationmissing<EOS>opinionperfectshould<pad>neverwhatLawThejustthisarebutwillmywebebeitsinis-,,..<EOS>,,its-thisTheperfectmyisbutjustbebeinarewhatweneverLaw<pad>missingshouldwillopinionapplicationapplicationmissing<EOS>opinionperfectshould<pad>neverwhatLawThejustthisarebutwillmywebebeitsinis-,,..<EOS>,,its-thisTheperfectmyisbutjustbebeinarewhatweLawnever<pad>missingshouldwillopinionapplicationFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of thesentence.We give two such examples above, from two different heads from the encoder self-attentionat layer 5 of 6.The heads clearly learned to perform different tasks.15", "page_start": 14, "page_end": 15, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.40356144}, {"chunk_id": "ccb42afb-c457-4988-937a-0e3d197a0da9", "text": "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of thesentence.We give two such examples above, from two different heads from the encoder self-attentionat layer 5 of 6.The heads clearly learned to perform different tasks.15", "page_start": 15, "page_end": 15, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.39669785}, {"chunk_id": "6ecb21e7-c073-4d90-9d8a-41fdfda06afb", "text": "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,textual entailment and learning task-independent sentence representations [4, 27, 28, 22].End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering andlanguage modeling tasks [34].To the best of our knowledge, however, the Transformer is the first transduction model relyingentirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.In the following sections, we will describe the Transformer, motivateself-attention and discuss its advantages over models such as [17, 18] and [9].3Model ArchitectureMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequenceof continuous representations z = (z1, ..., zn).Given z, the decoder then generates an outputsequence (y1, ..., ym) of symbols one element at a time.At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next.2Figure 1: The Transformer - model architecture.The Transformer follows this overall architecture using stacked self-attention and point-wise, fullyconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,respectively.3.1Encoder and Decoder StacksThe encoder is composed of a stack of N = 6 identical layers.Each layer has twoEncoder:sub-layers.The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network.We employ a residual connection [11] around each ofthe two sub-layers, followed by layer normalization [1].That is, the output of each sub-layer isLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layeritself.To facilitate these residual connections, all sub-layers in the model, as well as the embeddinglayers, produce outputs of dimension dmodel = 512.The decoder is also composed of a stack of N = 6 identical layers.In addition to the twoDecoder:sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-headattention over the output of the encoder stack.Similar to the encoder, we employ residual connectionsaround each of the sub-layers, followed by layer normalization.We also modify the self-attentionsub-layer in the decoder stack to prevent positions from attending to subsequent positions.Thismasking, combined with fact that the output embeddings are offset by one position, ensures that the", "page_start": 2, "page_end": 3, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.39492673}, {"chunk_id": "0b373c99-8c0e-44bf-8fc7-f6b6b009590d", "text": "Provided proper attribution is provided, Google hereby grants permission toreproduce the tables and figures in this paper solely for use in journalistic orscholarly works.Ashish Vaswani∗Noam Shazeer∗Niki Parmar∗Jakob Uszkoreit∗Google BrainGoogle BrainGoogle ResearchGoogle Researchavaswani@google.comnoam@google.comnikip@google.comusz@google.comLlion Jones∗Aidan N.Gomez∗†Łukasz Kaiser∗Google ResearchUniversity of TorontoGoogle Brainlukaszkaiser@google.comllion@google.comaidan@cs.toronto.eduIllia Polosukhin∗‡illia.polosukhin@gmail.comAbstractThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder.The bestperforming models also connect the encoder and decoder through an attentionmechanism.We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely.Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring significantlyless time to train.Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU.On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.8 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature.We show that the Transformer generalizes well toother tasks by applying it successfully to English constituency parsing both withlarge and limited training data.∗Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and startedthe effort to evaluate this idea.Ashish, with Illia, designed and implemented the first Transformer models andhas been crucially involved in every aspect of this work.Noam proposed scaled dot-product attention, multi-headattention and the parameter-free position representation and became the other person involved in nearly everydetail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase andtensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, andefficient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of andimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks", "page_start": 1, "page_end": 2, "section_title": "", "score": 0.36885613}], "inference_attention_02": [{"chunk_id": "0b373c99-8c0e-44bf-8fc7-f6b6b009590d", "text": "Provided proper attribution is provided, Google hereby grants permission toreproduce the tables and figures in this paper solely for use in journalistic orscholarly works.Ashish Vaswani∗Noam Shazeer∗Niki Parmar∗Jakob Uszkoreit∗Google BrainGoogle BrainGoogle ResearchGoogle Researchavaswani@google.comnoam@google.comnikip@google.comusz@google.comLlion Jones∗Aidan N.Gomez∗†Łukasz Kaiser∗Google ResearchUniversity of TorontoGoogle Brainlukaszkaiser@google.comllion@google.comaidan@cs.toronto.eduIllia Polosukhin∗‡illia.polosukhin@gmail.comAbstractThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder.The bestperforming models also connect the encoder and decoder through an attentionmechanism.We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely.Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring significantlyless time to train.Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU.On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.8 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature.We show that the Transformer generalizes well toother tasks by applying it successfully to English constituency parsing both withlarge and limited training data.∗Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and startedthe effort to evaluate this idea.Ashish, with Illia, designed and implemented the first Transformer models andhas been crucially involved in every aspect of this work.Noam proposed scaled dot-product attention, multi-headattention and the parameter-free position representation and became the other person involved in nearly everydetail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase andtensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, andefficient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of andimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks", "page_start": 1, "page_end": 2, "section_title": "", "score": 0.5548388}, {"chunk_id": "5151352b-ae46-44f3-83b0-8b8748547941", "text": "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networksin particular, have been firmly established as state of the art approaches in sequence modeling andtransduction problems such as language modeling and machine translation [35, 2, 5].Numerousefforts have since continued to push the boundaries of recurrent language models and encoder-decoderarchitectures [38, 24, 15].Recurrent models typically factor computation along the symbol positions of the input and outputsequences.Aligning the positions to steps in computation time, they generate a sequence of hiddenstates ht, as a function of the previous hidden state ht−1 and the input for position t.This inherentlysequential nature precludes parallelization within training examples, which becomes critical at longersequence lengths, as memory constraints limit batching across examples.Recent work has achievedsignificant improvements in computational efficiency through factorization tricks [21] and conditionalcomputation [32], while also improving model performance in case of the latter.The fundamentalconstraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance inthe input or output sequences [2, 19].In all but a few cases [27], however, such attention mechanismsare used in conjunction with a recurrent network.In this work we propose the Transformer, a model architecture eschewing recurrence and insteadrelying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for significantly more parallelization and can reach a new state of the art intranslation quality after being trained for as little as twelve hours on eight P100 GPUs.2BackgroundThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic buildingblock, computing hidden representations in parallel for all input and output positions.In these models,the number of operations required to relate signals from two arbitrary input or output positions growsin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.This makesit more difficult to learn dependencies between distant positions [12].In the Transformer this isreduced to a constant number of operations, albeit at the cost of reduced effective resolution dueto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,", "page_start": 1, "page_end": 2, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.5400853}, {"chunk_id": "6ecb21e7-c073-4d90-9d8a-41fdfda06afb", "text": "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,textual entailment and learning task-independent sentence representations [4, 27, 28, 22].End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering andlanguage modeling tasks [34].To the best of our knowledge, however, the Transformer is the first transduction model relyingentirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.In the following sections, we will describe the Transformer, motivateself-attention and discuss its advantages over models such as [17, 18] and [9].3Model ArchitectureMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequenceof continuous representations z = (z1, ..., zn).Given z, the decoder then generates an outputsequence (y1, ..., ym) of symbols one element at a time.At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next.2Figure 1: The Transformer - model architecture.The Transformer follows this overall architecture using stacked self-attention and point-wise, fullyconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,respectively.3.1Encoder and Decoder StacksThe encoder is composed of a stack of N = 6 identical layers.Each layer has twoEncoder:sub-layers.The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network.We employ a residual connection [11] around each ofthe two sub-layers, followed by layer normalization [1].That is, the output of each sub-layer isLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layeritself.To facilitate these residual connections, all sub-layers in the model, as well as the embeddinglayers, produce outputs of dimension dmodel = 512.The decoder is also composed of a stack of N = 6 identical layers.In addition to the twoDecoder:sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-headattention over the output of the encoder stack.Similar to the encoder, we employ residual connectionsaround each of the sub-layers, followed by layer normalization.We also modify the self-attentionsub-layer in the decoder stack to prevent positions from attending to subsequent positions.Thismasking, combined with fact that the output embeddings are offset by one position, ensures that the", "page_start": 2, "page_end": 3, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.53018665}, {"chunk_id": "bb77d3cc-59f9-4b97-a21b-132b30215259", "text": "semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised92.1Transformer (4 layers)semi-supervised92.7Luong et al.(2015) [23]multi-task93.0Dyer et al.(2016) [8]generative93.3increased the maximum output length to input length + 300.We used a beam size of 21 and α = 0.3for both WSJ only and the semi-supervised setting.Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of theRecurrent Neural Network Grammar [8].In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.7ConclusionIn this work, we presented the Transformer, the first sequence transduction model based entirely onattention, replacing the recurrent layers most commonly used in encoder-decoder architectures withmulti-headed self-attention.For translation tasks, the Transformer can be trained significantly faster than architectures basedon recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014English-to-French translation tasks, we achieve a new state of the art.In the former task our bestmodel outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.Weplan to extend the Transformer to problems involving input and output modalities other than text andto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputssuch as images, audio and video.Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.AcknowledgementsWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitfulcomments, corrections and inspiration.References[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprintarXiv:1607.06450, 2016.[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointlylearning to align and translate.CoRR, abs/1409.0473, 2014.[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V.Le.Massive exploration of neuralmachine translation architectures.CoRR, abs/1703.03906, 2017.[4] Jianpeng Cheng, Li Dong, and Mirella Lapata.Long short-term memory-networks for machinereading.arXiv preprint arXiv:1601.06733, 2016.10[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,", "page_start": 10, "page_end": 11, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.495687}, {"chunk_id": "914cfb0b-3d4a-4988-8392-851b64c13e57", "text": "The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.Weused beam search with a beam size of 4 and length penalty α = 0.6 [38].These hyperparameterswere chosen after experimentation on the development set.We set the maximum output length duringinference to input length + 50, but terminate early when possible [38].Table 2 summarizes our results and compares our translation quality and training costs to other modelarchitectures from the literature.We estimate the number of floating point operations used to train amodel by multiplying the training time, the number of GPUs used, and an estimate of the sustainedsingle-precision floating-point capacity of each GPU 5.6.2Model VariationsTo evaluate the importance of different components of the Transformer, we varied our base modelin different ways, measuring the change in performance on English-to-German translation on the5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the basemodel.All metrics are on the English-to-German translation development set, newstest2013.Listedperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared toper-word perplexities.trainPPLBLEUparamsNdmodeldffhdkdvPdropϵls×106steps(dev)(dev)base65122048864640.10.1100K4.9225.86515125125.2924.941281285.0025.5(A)1632324.9125.83216165.0125.4165.1625.158(B)325.0125.46026.1123.73645.1925.35084.8825.580(C)25632325.7524.52810241281284.6626.016810245.1225.45340964.7526.2900.05.7724.60.24.9525.5(D)0.04.6725.30.25.4725.7(E)positional embedding instead of sinusoids4.9225.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.", "page_start": 8, "page_end": 9, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.48334253}, {"chunk_id": "8d9a2784-c6bd-4675-a20b-80dd0948329f", "text": "25.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,keeping the amount of computation constant, as described in Section 3.2.2.While single-headattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality.Thissuggests that determining compatibility is not easy and that a more sophisticated compatibilityfunction than dot product may be beneficial.We further observe in rows (C) and (D) that, as expected,bigger models are better, and dropout is very helpful in avoiding over-fitting.In row (E) we replace oursinusoidal positional encoding with learned positional embeddings [9], and observe nearly identicalresults to the base model.6.3English Constituency ParsingTo evaluate if the Transformer can generalize to other tasks we performed experiments on Englishconstituency parsing.This task presents specific challenges: the output is subject to strong structuralconstraints and is significantly longer than the input.Furthermore, RNN sequence-to-sequencemodels have not been able to attain state-of-the-art results in small-data regimes [37].We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of thePenn Treebank [25], about 40K training sentences.We also trained it in a semi-supervised setting,using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences[37].We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokensfor the semi-supervised setting.We performed only a small number of experiments to select the dropout, both attention and residual(section 5.4), learning rates and beam size on the Section 22 development set, all other parametersremained unchanged from the English-to-German base translation model.During inference, we9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23of WSJ)ParserTrainingWSJ 23 F1Vinyals & Kaiser el al.(2014) [37]WSJ only, discriminative88.3Petrov et al.(2006) [29]WSJ only, discriminative90.4Zhu et al.(2013) [40]WSJ only, discriminative90.4Dyer et al.(2016) [8]WSJ only, discriminative91.7Transformer (4 layers)WSJ only, discriminative91.3Zhu et al.(2013) [40]semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised", "page_start": 9, "page_end": 10, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.47977155}, {"chunk_id": "160dd6c9-8965-4dcd-8d74-e0dd157c4fd1", "text": "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step number.We usedwarmup_steps = 4000.5.4RegularizationWe employ three types of regularization during training:7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on theEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.BLEUTraining Cost (FLOPs)ModelEN-DEEN-FREN-DEEN-FRByteNet [18]23.751.0 · 1020Deep-Att + PosUnk [39]39.22.3 · 10191.4 · 1020GNMT + RL [38]24.639.929.6 · 10181.5 · 1020ConvS2S [9]25.1640.462.0 · 10191.2 · 1020MoE [32]26.0340.568.0 · 1020Deep-Att + PosUnk Ensemble [39]40.41.8 · 10201.1 · 1021GNMT + RL Ensemble [38]26.3041.167.7 · 10191.2 · 102141.29ConvS2S Ensemble [9]26.363.3 · 1018Transformer (base model)27.338.12.3 · 101928.441.8Transformer (big)Residual DropoutWe apply dropout [33] to the output of each sub-layer, before it is added to thesub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and thepositional encodings in both the encoder and decoder stacks.For the base model, we use a rate ofPdrop = 0.1.During training, we employed label smoothing of value ϵls = 0.1 [36].ThisLabel Smoothinghurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6Results6.1Machine TranslationOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0BLEU, establishing a new state-of-the-art BLEU score of 28.4.The configuration of this model islisted in the bottom line of Table 3.Training took 3.5 days on 8 P100 GPUs.Even our base modelsurpasses all previously published models and ensembles, at a fraction of the training cost of any ofthe competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,outperforming all of the previously published single models, at less than 1/4 the training cost of theprevious state-of-the-art model.The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We", "page_start": 7, "page_end": 8, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4642138}, {"chunk_id": "58f1c685-b5d0-4ba4-8711-0ba5768f729c", "text": "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,we found it beneficial to linearly project the queries, keys and values h times with different, learnedlinear projections to dk, dk and dv dimensions, respectively.On each of these projected versions ofqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional4To illustrate why the dot products get large, assume that the components of q and k are independent randomvariables with mean 0 and variance 1.Then their dot product, q · k = Pdki=1 qiki, has mean 0 and variance dk.4output values.These are concatenated and once again projected, resulting in the final values, asdepicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representationsubspaces at different positions.With a single attention head, averaging inhibits this.MultiHead(Q, K, V ) = Concat(head1, ..., headh)W Owhere headi = Attention(QW Qi , KW Ki , V W Vi )Where the projections are parameter matrices W Q∈Rdmodel×dk, W V∈Rdmodel×dv∈Rdmodel×dk, W Kiiiand W O ∈Rhdv×dmodel.In this work we employ h = 8 parallel attention layers, or heads.For each of these we usedk = dv = dmodel/h = 64.Due to the reduced dimension of each head, the total computational costis similar to that of single-head attention with full dimensionality.3.2.3Applications of Attention in our ModelThe Transformer uses multi-head attention in three different ways:• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,and the memory keys and values come from the output of the encoder.This allows everyposition in the decoder to attend over all positions in the input sequence.This mimics thetypical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38, 2, 9].• The encoder contains self-attention layers.In a self-attention layer all of the keys, valuesand queries come from the same place, in this case, the output of the previous layer in theencoder.Each position in the encoder can attend to all positions in the previous layer of theencoder.• Similarly, self-attention layers in the decoder allow each position in the decoder to attend toall positions in the decoder up to and including that position.We need to prevent leftwardinformation flow in the decoder to preserve the auto-regressive property.We implement thisinside of scaled dot-product attention by masking out (setting to −∞) all values in the inputof the softmax which correspond to illegal connections.See Figure 2.3.3Position-wise Feed-Forward NetworksIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fullyconnected feed-forward network, which is applied to each position separately and identically.This", "page_start": 4, "page_end": 5, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.40673545}], "inference_nda_01": [{"chunk_id": "94d954fb-950b-4dcf-8049-c72deb8243c9", "text": "Date: 201[ ]Parties:[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]and[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]1.Each of the parties to this Agreement intends to disclose information (the ConfidentialInformation) to the other party for the purpose of [insert details e.g.discussing thepossibility of the parties entering into a joint venture] (the Purpose).2.Each party to this Agreement is referred to as ‘the Recipient’ when it receives or usesthe Confidential Information disclosed by the other party.3.The Recipient undertakes not to use the Confidential Information disclosed by the otherparty for any purpose except the Purpose, without first obtaining the written agreementof the other party.4.The Recipient undertakes to keep the Confidential Information disclosed by the otherparty secure and not to disclose it to any third party [except to its employees [andprofessional advisers] who need to know the same for the Purpose, who know theyowe a duty of confidence to the other party and who are bound by obligationsequivalent to those in clause 3 above and this clause 4.5.The undertakings in clauses 3 and 4 above apply to all of the information disclosed byeach of the parties to the other, regardless of the way or form in which it is disclosed orrecorded but they do not apply to:a) any information which is or in future comes into the public domain (unless as a result ofthe breach of this Agreement); orb) any information which is already known to the Recipient and which was not subject toany obligation of confidence before it was disclosed to the Recipient by the other party.6.Nothing in this Agreement will prevent the Recipient from making any disclosure of theConfidential Information required by law or by any competent authority.7.The Recipient will, on request from the other party, return all copies and records of theConfidential Information disclosed by the other party to the Recipient and will not retainany copies or records of the Confidential Information disclosed by the other party.8.Neither this Agreement nor the supply of any information grants the Recipient anylicence, interest or right in respect of any intellectual property rights of the other partyexcept the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.", "page_start": 1, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.50819325}, {"chunk_id": "6c64d139-fd3d-400d-b64b-3ddf0dea3473", "text": "except the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.The English Courts will have non-exclusive jurisdiction to deal with any dispute whichhas arisen or may arise out of, or in connection with, this Agreement.Signed [by [insert name]] OR [on behalf of][insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________PositionSigned [by [insert name]] OR [on behalf of] [insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________Position", "page_start": 2, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.44444478}], "inference_nda_02": [{"chunk_id": "94d954fb-950b-4dcf-8049-c72deb8243c9", "text": "Date: 201[ ]Parties:[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]and[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]1.Each of the parties to this Agreement intends to disclose information (the ConfidentialInformation) to the other party for the purpose of [insert details e.g.discussing thepossibility of the parties entering into a joint venture] (the Purpose).2.Each party to this Agreement is referred to as ‘the Recipient’ when it receives or usesthe Confidential Information disclosed by the other party.3.The Recipient undertakes not to use the Confidential Information disclosed by the otherparty for any purpose except the Purpose, without first obtaining the written agreementof the other party.4.The Recipient undertakes to keep the Confidential Information disclosed by the otherparty secure and not to disclose it to any third party [except to its employees [andprofessional advisers] who need to know the same for the Purpose, who know theyowe a duty of confidence to the other party and who are bound by obligationsequivalent to those in clause 3 above and this clause 4.5.The undertakings in clauses 3 and 4 above apply to all of the information disclosed byeach of the parties to the other, regardless of the way or form in which it is disclosed orrecorded but they do not apply to:a) any information which is or in future comes into the public domain (unless as a result ofthe breach of this Agreement); orb) any information which is already known to the Recipient and which was not subject toany obligation of confidence before it was disclosed to the Recipient by the other party.6.Nothing in this Agreement will prevent the Recipient from making any disclosure of theConfidential Information required by law or by any competent authority.7.The Recipient will, on request from the other party, return all copies and records of theConfidential Information disclosed by the other party to the Recipient and will not retainany copies or records of the Confidential Information disclosed by the other party.8.Neither this Agreement nor the supply of any information grants the Recipient anylicence, interest or right in respect of any intellectual property rights of the other partyexcept the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.", "page_start": 1, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.46590126}, {"chunk_id": "6c64d139-fd3d-400d-b64b-3ddf0dea3473", "text": "except the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.The English Courts will have non-exclusive jurisdiction to deal with any dispute whichhas arisen or may arise out of, or in connection with, this Agreement.Signed [by [insert name]] OR [on behalf of][insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________PositionSigned [by [insert name]] OR [on behalf of] [insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________Position", "page_start": 2, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.4112761}], "multihop_nvidia_01": [{"chunk_id": "ddc05d6e-79f6-4ed4-a29b-876d9e76fd48", "text": "We did not experience any significant impact or expense toour business; however, if the conflict is further extended, it could impact future product development, operations, and revenue or create other uncertainty for ourbusiness.35Fiscal Year 2024 SummaryYear EndedJan 28, 2024Jan 29, 2023Change($ in millions, except per share data)Revenue$60,922$26,974Up 126%Gross margin72.7 %56.9 %Up 15.8 ptsOperating expenses$11,329$11,132Up 2%Operating income$32,972$4,224Up 681%Net income$29,760$4,368Up 581%Net income per diluted share$11.93$1.74Up 586%We specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Revenue for fiscal year 2024 was $60.9 billion, up 126% from a year ago.Data Center revenue for fiscal year 2024 was up 217%.Strong demand was driven by enterprise software and consumer internet applications, and multipleindustry verticals including automotive, financial services, and healthcare.Customers across industry verticals access NVIDIA AI infrastructure both through thecloud and on-premises.Data Center compute revenue was up 244% in the fiscal year.Networking revenue was up 133% in the fiscal year.Gaming revenue for fiscal year 2024 was up 15%.The increase reflects higher sell-in to partners following the normalization of channel inventory levels andgrowing demand.Professional Visualization revenue for fiscal year 2024 was up 1%.Automotive revenue for the fiscal year 2024 was up 21%.The increase primarily reflected growth in self-driving platforms.Gross margin increased in fiscal year 2024, primarily driven by Data Center revenue growth and lower net inventory provisions as a percentage of revenue.Operating expenses increased for fiscal year 2024, driven by growth in employees and compensation increases.Fiscal year 2023 also included a $1.4 billionacquisition termination charge related to the proposed Arm transaction.Market Platform HighlightsData Center revenue for fiscal year 2024 was $47.5 billion, up 217% from fiscal year 2023.In Data Center, we launched AI inference platforms that combine ourfull-stack inference software with NVIDIA Ada, NVIDIA Hopper and NVIDIA Grace Hopper processors optimized for generative AI, LLMs and other AI workloads.We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.", "page_start": 35, "page_end": 36, "section_title": "NVIDIA CORPORATION", "score": 0.73882943}, {"chunk_id": "823438dd-9ce8-4ae9-924a-70c6bbf5121f", "text": "Form 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38The following table sets forth, for the periods indicated, certain items in our Consolidated Statements of Income expressed as a percentage of revenue.Year EndedJan 28, 2024Jan 29, 2023Revenue100.0 %100.0 %Cost of revenue27.343.1Gross profit72.756.9Operating expensesResearch and development14.227.2Sales, general and administrative4.49.1Acquisition termination cost—5.0Total operating expenses18.641.3Operating income54.115.6Interest income1.41.0Interest expense(0.4)(1.0)0.4(0.1)Other, net1.4(0.1)Other income (expense), netIncome before income tax55.515.5Income tax expense (benefit)6.6(0.7)48.9 %16.2 %Net incomeReportable SegmentsRevenue by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$47,405$15,068$32,337215 %13,51711,9061,611Graphics14 %$60,922$26,974$33,948Total126 %Operating Income by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$32,016$5,083$26,933530 %Graphics5,8464,5521,29428 %(4,890)(5,411)521All Other(10)%$32,972$4,224$28,748Total681 %Compute & Networking revenue – The year-on-year increase was due to higher Data Center revenue.Compute grew 266% due to higher shipments of theNVIDIA Hopper GPU computing platform for the training and inference of LLMs, recommendation engines and generative AI applications.Networking was up133% due to higher shipments of InfiniBand.Graphics revenue – The year-on-year increase was led by growth in Gaming of 15% driven by higher sell-in to partners following the normalization of channelinventory levels.Reportable segment operating income – The year-on-year increase in Compute & Networking and Graphics operating income was driven by higher revenue.39All Other operating loss - The year-on-year decrease was due to the $1.4 billion Arm acquisition termination cost in fiscal year 2023, partially offset by a $839million increase in stock-based compensation expense in fiscal year 2024.Concentration of Revenue", "page_start": 38, "page_end": 40, "section_title": "NVIDIA CORPORATION", "score": 0.6919429}, {"chunk_id": "8d7f8306-8d55-40f5-b383-f4914775e7ee", "text": "(10)Restructuring costs and other—(54)—Acquisition termination cost—(1,353)—10(2)—Other$(4,890)$(5,411)$(3,049)TotalRevenue by geographic areas is designated based upon the billing location of the customer.End customer location may be different than our customer’s billinglocation.Revenue by geographic areas was as follows:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Revenue:(In millions)United States$26,966$8,292$4,349Taiwan13,4056,9868,544China (including Hong Kong)10,3065,7857,11110,2455,9116,910Other countries$60,922$26,974$26,914Total revenueRevenue from sales to customers outside of the United States accounted for 56%, 69%, and 84% of total revenue for fiscal years 2024, 2023, and 2022,respectively.The increase in revenue to the United States for fiscal year 2024 was primarily due to higher U.S.-based Compute & Networking segment demand.Sales to one customer represented 13% of total revenue for fiscal year 2024, which was attributable to the Compute & Networking segment.No customerrepresented 10% or more of total revenue for fiscal years 2023 and 2022.The following table summarizes information pertaining to our revenue by each of the specialized markets we serve:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Revenue:(In millions)Data Center$47,525$15,005$10,613Gaming10,4479,06712,462Professional Visualization1,5531,5442,111Automotive1,0919035663064551,162OEM and Other$60,922$26,974$26,914Total revenue79NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)The following table presents summarized information for long-lived assets by country.Long-lived assets consist of property and equipment and exclude otherassets, operating lease assets, goodwill, and intangible assets.Jan 28, 2024Jan 29, 2023Long-lived assets:(In millions)United States$2,595$2,587Taiwan773702Israel325283Other countries221235$3,914$3,807Total long-lived assets80NVIDIA Corporation and SubsidiariesSchedule II – Valuation and Qualifying AccountsBalance atBeginning ofBalance atDescriptionPeriodAdditionsDeductionsEnd of Period(In millions)Fiscal year 2024Allowance for doubtful accounts$4$— (1)$— (1)$4Sales return allowance$26$213 (2)$", "page_start": 79, "page_end": 81, "section_title": "NVIDIA CORPORATION", "score": 0.6485245}, {"chunk_id": "f2207e14-4c6c-421b-849b-76359737cca1", "text": "Germany, India, Israel, and Taiwan for fiscal years 2005 through 2023.Note 15 - Shareholders’ EquityCapital Return ProgramIn August 2023, our Board of Directors approved an increase to our share repurchase program of an additional $25.0 billion, without expiration.During fiscalyear 2024, we repurchased 21 million shares of our common stock for $9.7 billion.As of January 28, 2024, we were authorized, subject to certain specifications,to repurchase additional shares of our common stock up to $22.5 billion.From January 29, 2024 through February 16, 2024, we repurchased 2.8 million sharesfor $1.9 billion pursuant to a Rule 10b5-1 trading plan.Our share repurchase program aims to offset dilution from shares issued to employees.We may pursueadditional share repurchases as we weigh market factors and other investment opportunities.During fiscal years 2024, 2023, and 2022, we paid $395 million, $398 million, and $399 million in cash dividends to our shareholders, respectively.Our cashdividend program and the payment of future cash dividends under that program are subject to our Board of Directors' continuing determination that the dividendprogram and the declaration of dividends thereunder are in the best interests of our shareholders.In fiscal year 2022, we retired our existing 349 million treasury shares.These shares assumed the status of authorized and unissued shares upon retirement.The excess of repurchase price over par value was allocated between additional paid-in capital and retained earnings, resulting in a reduction in additional paidin capital by $20 million and retained earnings by $12.0 billion.Any future repurchased shares will assume the status of authorized and unissued shares.77NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Note 16 - Employee Retirement PlansWe provide tax-qualified defined contribution plans to eligible employees in the U.S.and certain other countries.Our contribution expense for fiscal years 2024,2023, and 2022 was $255 million, $227 million, and $168 million, respectively.Note 17 - Segment InformationOur Chief Executive Officer, who is considered to be our chief operating decision maker, or CODM, reviews financial information presented on an operatingsegment basis for purposes of making decisions and assessing financial performance.The Compute & Networking segment includes our Data Center accelerated computing platform; networking; automotive artificial intelligence, or AI, Cockpit,autonomous driving development agreements, and autonomous vehicle solutions; electric vehicle computing platforms; Jetson for robotics and other embeddedplatforms; NVIDIA AI Enterprise and other software; and DGX Cloud.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions forgaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across our", "page_start": 77, "page_end": 78, "section_title": "NVIDIA CORPORATION", "score": 0.6325048}, {"chunk_id": "0dd7fcd2-abcb-4469-bfdc-d6d5c6cb8eae", "text": "*$100 invested on 1/27/19 in stock and in indices, including reinvestment of dividends.Source: FactSet financial data and analytics.1/27/20191/26/20201/31/20211/30/20221/29/20231/28/2024NVIDIA Corporation$100.00$157.02$326.26$574.15$512.40$1,536.28S&P 500$100.00$126.17$144.83$175.25$163.63$199.83Nasdaq 100$100.00$136.15$194.20$218.68$185.67$268.13Item 6.[Reserved]33Item 7.Management's Discussion and Analysis of Financial Condition and Results of OperationsThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with “Item 1A.Risk Factors”, ourConsolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described elsewhere in this Annual Report onForm 10-K, before deciding to purchase, hold or sell shares of our common stock.OverviewOur Company and Our BusinessesNVIDIA pioneered accelerated computing to help solve the most challenging computational problems.Since our original focus on PC graphics, we haveexpanded to several other large and important computationally intensive fields.NVIDIA has leveraged its GPU architecture to create platforms for acceleratedcomputing, AI solutions, scientific computing, data science, AV, robotics, metaverse and 3D internet applications.Our two operating segments are \"Compute & Networking\" and \"Graphics.\" Refer to Note 17 of the Notes to the Consolidated Financial Statements in Part IV,Item 15 of this Annual Report on Form 10-K for additional information.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Recent Developments, Future Objectives and ChallengesDemand and Supply, Product Transitions, and New Products and Business ModelsDemand for our data center systems and products surged in fiscal year 2024.Entering fiscal year 2025, we are gathering customer demand indications acrossseveral product transitions.We have demand visibility for our new data center products ramping later in fiscal year 2025.We have increased our supply andcapacity purchases with existing suppliers, added new vendors and entered into prepaid manufacturing and capacity agreements.These increased purchasevolumes, the number of suppliers, and the integration of new vendors into our supply chain may create more complexity and execution risk.Our purchasecommitments and obligations for inventory and manufacturing capacity at the end of fiscal year 2024 were impacted by shortening lead times for certaincomponents.We may continue to enter into new supplier and capacity arrangements.Supply of Hopper architecture products is improving, and demand remainsvery strong.We expect our next-generation products to be supply-constrained based upon demand indications.", "page_start": 33, "page_end": 34, "section_title": "NVIDIA CORPORATION", "score": 0.62207556}, {"chunk_id": "c69190e8-be56-40d7-9e3f-a1b1a8224354", "text": "Estimates for customer program accrualsinclude a combination of historical attainment and claim rates and may be adjusted based on relevant internal and external factors.License and Development ArrangementsRevenue from License and Development Arrangements is recognized over the period in which the development services are performed.Each fiscal reportingperiod, we measure progress to completion based on actual cost incurred to date as a percentage of the estimated total cost required to complete each project.Estimated total cost for each project includes a forecast of internal engineer personnel time expected to be incurred and other third-party costs as applicable.Contracts with Multiple Performance ObligationsOur contracts may contain more than one performance obligation.Judgement is required in determining whether each performance obligation within a customercontract is distinct.Except for License and Development Arrangements, NVIDIA products and services function on a standalone basis and do not require asignificant amount of integration or interdependency.Therefore, multiple performance obligations contained within a customer contract are considered distinctand are not combined for revenue recognition purposes.We allocate the total transaction price to each distinct performance obligation in a multiple performance obligations arrangement on a relative standalone sellingprice basis.In certain cases, we can establish standalone selling price based on directly observable prices of products or services sold separately in comparablecircumstances to similar customers.If standalone selling price is not directly observable, such as when we do not sell a product or service separately, wedetermine standalone selling price based on market data and other observable inputs.Change in Accounting EstimateIn February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of a majority of the server, storage, and network equipment from three years to a range of four to five years, and assembly and testequipment from five years to seven years.The estimated effect of this change for fiscal year 2024 was a benefit of $33 million and $102 million for cost ofrevenue and operating expenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or$0.05 per both basic and diluted share.Results of OperationsA discussion regarding our financial condition and results of operations for fiscal year 2024 compared to fiscal year 2023 is presented below.A discussionregarding our financial condition and results of operations for fiscal year 2023 compared to fiscal year 2022 can be found under Item 7 in our Annual Report onForm 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38", "page_start": 38, "page_end": 38, "section_title": "NVIDIA CORPORATION", "score": 0.6188996}, {"chunk_id": "16846335-f2af-4650-bf60-d4b5485698d4", "text": "Principles of ConsolidationOur consolidated financial statements include the accounts of NVIDIA Corporation and our wholly-owned subsidiaries.All intercompany balances andtransactions have been eliminated in consolidation.Use of EstimatesThe preparation of financial statements in conformity with U.S.GAAP requires management to make estimates and assumptions that affect the reportedamounts of assets and liabilities and disclosures of contingent assets and liabilities at the date of the financial statements and the reported amounts of revenueand expenses during the reporting period.Actual results could differ materially from our estimates.On an on-going basis, we evaluate our estimates, includingthose related to revenue recognition, cash equivalents and marketable securities, accounts receivable, inventories and product purchase commitments, incometaxes, goodwill, stock-based compensation, litigation, investigation and settlement costs, restructuring and other charges, property, plant, and equipment, andother contingencies.These estimates are based on historical facts and various other assumptions that we believe are reasonable.In February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of most of our server, storage, and network equipment from three to four or five years, and our assembly and test equipment from five toseven years.The effect of this change for the fiscal year ended January 28, 2024 was a benefit of $33 million and $102 million for cost of revenue and operatingexpenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or $0.05 per both basic anddiluted share.Revenue RecognitionWe derive our revenue from product sales, including hardware and systems, license and development arrangements, software licensing, and cloud services.Wedetermine revenue recognition through the following steps: (1) identification of the contract with a customer; (2) identification of the performance obligations inthe contract; (3) determination of the transaction price; (4) allocation of the transaction price to the performance obligations in the contract (where revenue isallocated on a relative standalone selling price basis by maximizing the use of observable inputs to determine the standalone selling price for each performanceobligation); and (5) recognition of revenue when, or as, we satisfy a performance obligation.Product Sales RevenueRevenue from product sales is recognized upon transfer of control of products to customers in an amount that reflects the consideration we expect to receive inexchange for those products.Certain products are sold with support or an extended warranty for the incorporated system, hardware, and/or software.Supportand extended warranty revenue are recognized ratably over the service period, or as services are performed.Revenue is recognized net of allowances forreturns, customer programs and any taxes collected from customers.For products sold with a right of return, we record a reduction to revenue by establishing a sales return allowance for estimated product returns at the timerevenue is recognized, based primarily on historical return rates.", "page_start": 55, "page_end": 55, "section_title": "NVIDIA CORPORATION", "score": 0.61103016}, {"chunk_id": "66e77043-a370-4255-b50b-931d8a75e255", "text": "We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.In the fourth quarter of fiscal year 2024, largecloud providers represented more than half of our Data Center revenue, supporting both internal workloads and external customers.We announced NVIDIASpectrum-X, an accelerated networking platform for AI.Gaming revenue for fiscal year 2024 was $10.4 billion, up 15% from fiscal year 2023.In Gaming, we launched the GeForce RTX 4060 and 4070 GPUs based onthe NVIDIA Ada Lovelace architecture.We announced NVIDIA Avatar Cloud Engine for Games, a custom AI model foundry service using AI-powered naturallanguage interactions to transform games and launched DLSS 3.5 Ray Reconstruction.Additionally, we released TensorRT-LLM for Windows and launchedGeForce RTX 40-Series SUPER GPUs.Gaming reached a milestone of 500 AI-powered RTX games and applications utilizing NVIDIA DLSS, ray tracing andother NVIDIA RTX technologies.Professional Visualization revenue for fiscal year 2024 was $1.6 billion, up 1% from fiscal year 2023.In Professional Visualization, we announced new GPUsbased on the NVIDIA RTX Ada Lovelace architecture, and announced NVIDIA Omniverse Cloud, a fully managed service running in Microsoft Azure, for thedevelopment and deployment of industrial metaverse applications.Automotive revenue for fiscal year 2024 was $1.1 billion, up 21% from fiscal year 2023.In Automotive, we announced a partnership with MediaTek, which willdevelop mainstream automotive systems on chips for global OEMs integrating a new NVIDIA GPU chiplet IP for AI and graphics.We furthered our collaborationwith Foxconn to develop next-generation36electric vehicles, and announced further adoption of NVIDIA DRIVE platform with BYD, XPENG, GWM, Li Auto, ZEEKR and Xiaomi.Critical Accounting EstimatesOur consolidated financial statements are prepared in accordance with accounting principles generally accepted in the United States, or U.S.GAAP.Thepreparation of these financial statements requires us to make estimates and judgments that affect the reported amounts of assets, liabilities, revenue, cost ofrevenue, expenses and related disclosure of contingencies.Critical accounting estimates are those estimates that involve a significant level of estimationuncertainty and could have a material impact on our financial condition or results of operations.We have critical accounting estimates in the areas of inventories,revenue recognition, and income taxes.Refer to Note 1 of the Notes to the Consolidated Financial Statements in Part IV, Item 15 of this Annual Report on Form10-K for a summary of significant accounting policies.InventoriesWe charge cost of sales for inventory provisions to write-down our inventory to the lower of cost or net realizable value or for obsolete or excess inventory, and", "page_start": 36, "page_end": 37, "section_title": "NVIDIA CORPORATION", "score": 0.6046383}], "multihop_attention_01": [{"chunk_id": "72538026-a082-4975-9165-c903508ae7da", "text": "In this work, we use sine and cosine functions of different frequencies:PE(pos,2i) = sin(pos/100002i/dmodel)PE(pos,2i+1) = cos(pos/100002i/dmodel)where pos is the position and i is the dimension.That is, each dimension of the positional encodingcorresponds to a sinusoid.The wavelengths form a geometric progression from 2π to 10000 · 2π.Wechose this function because we hypothesized it would allow the model to easily learn to attend byrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function ofPEpos.We also experimented with using learned positional embeddings [9] instead, and found that the twoversions produced nearly identical results (see Table 3 row (E)).We chose the sinusoidal versionbecause it may allow the model to extrapolate to sequence lengths longer than the ones encounteredduring training.4Why Self-AttentionIn this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hiddenlayer in a typical sequence transduction encoder or decoder.Motivating our use of self-attention weconsider three desiderata.One is the total computational complexity per layer.Another is the amount of computation that canbe parallelized, as measured by the minimum number of sequential operations required.The third is the path length between long-range dependencies in the network.Learning long-rangedependencies is a key challenge in many sequence transduction tasks.One key factor affecting theability to learn such dependencies is the length of the paths forward and backward signals have totraverse in the network.The shorter these paths between any combination of positions in the inputand output sequences, the easier it is to learn long-range dependencies [12].Hence we also comparethe maximum path length between any two input and output positions in networks composed of thedifferent layer types.As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentiallyexecuted operations, whereas a recurrent layer requires O(n) sequential operations.In terms ofcomputational complexity, self-attention layers are faster than recurrent layers when the sequence6length n is smaller than the representation dimensionality d, which is most often the case withsentence representations used by state-of-the-art models in machine translations, such as word-piece[38] and byte-pair [31] representations.To improve computational performance for tasks involvingvery long sequences, self-attention could be restricted to considering only a neighborhood of size r inthe input sequence centered around the respective output position.This would increase the maximumpath length to O(n/r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k < n does not connect all pairs of input and outputpositions.", "page_start": 6, "page_end": 7, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.71034396}, {"chunk_id": "d18429f8-020c-433c-b74a-eb6e9b76b836", "text": "inside of scaled dot-product attention by masking out (setting to −∞) all values in the inputof the softmax which correspond to illegal connections.See Figure 2.3.3Position-wise Feed-Forward NetworksIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fullyconnected feed-forward network, which is applied to each position separately and identically.Thisconsists of two linear transformations with a ReLU activation in between.FFN(x) = max(0, xW1 + b1)W2 + b2(2)While the linear transformations are the same across different positions, they use different parametersfrom layer to layer.Another way of describing this is as two convolutions with kernel size 1.The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionalitydff = 2048.3.4Embeddings and SoftmaxSimilarly to other sequence transduction models, we use learned embeddings to convert the inputtokens and output tokens to vectors of dimension dmodel.We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.Inour model, we share the same weight matrix between the two embedding layers and the pre-softmaxlinear transformation, similar to [30].In the embedding layers, we multiply those weights by √dmodel.5Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operationsfor different layer types.n is the sequence length, d is the representation dimension, k is the kernelsize of convolutions and r the size of the neighborhood in restricted self-attention.Layer TypeComplexity per LayerSequentialMaximum Path LengthOperationsO(n2 · d)O(1)O(1)Self-AttentionO(n · d2)O(n)O(n)RecurrentO(k · n · d2)O(1)O(logk(n))ConvolutionalO(r · n · d)O(1)O(n/r)Self-Attention (restricted)3.5Positional EncodingSince our model contains no recurrence and no convolution, in order for the model to make use of theorder of the sequence, we must inject some information about the relative or absolute position of thetokens in the sequence.To this end, we add \"positional encodings\" to the input embeddings at thebottoms of the encoder and decoder stacks.The positional encodings have the same dimension dmodelas the embeddings, so that the two can be summed.There are many choices of positional encodings,learned and fixed [9].In this work, we use sine and cosine functions of different frequencies:PE(pos,2i) = sin(pos/100002i/dmodel)PE(pos,2i+1) = cos(pos/100002i/dmodel)where pos is the position and i is the dimension.That is, each dimension of the positional encodingcorresponds to a sinusoid.The wavelengths form a geometric progression from 2π to 10000 · 2π.We", "page_start": 5, "page_end": 6, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.5851355}, {"chunk_id": "5151352b-ae46-44f3-83b0-8b8748547941", "text": "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networksin particular, have been firmly established as state of the art approaches in sequence modeling andtransduction problems such as language modeling and machine translation [35, 2, 5].Numerousefforts have since continued to push the boundaries of recurrent language models and encoder-decoderarchitectures [38, 24, 15].Recurrent models typically factor computation along the symbol positions of the input and outputsequences.Aligning the positions to steps in computation time, they generate a sequence of hiddenstates ht, as a function of the previous hidden state ht−1 and the input for position t.This inherentlysequential nature precludes parallelization within training examples, which becomes critical at longersequence lengths, as memory constraints limit batching across examples.Recent work has achievedsignificant improvements in computational efficiency through factorization tricks [21] and conditionalcomputation [32], while also improving model performance in case of the latter.The fundamentalconstraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance inthe input or output sequences [2, 19].In all but a few cases [27], however, such attention mechanismsare used in conjunction with a recurrent network.In this work we propose the Transformer, a model architecture eschewing recurrence and insteadrelying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for significantly more parallelization and can reach a new state of the art intranslation quality after being trained for as little as twelve hours on eight P100 GPUs.2BackgroundThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic buildingblock, computing hidden representations in parallel for all input and output positions.In these models,the number of operations required to relate signals from two arbitrary input or output positions growsin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.This makesit more difficult to learn dependencies between distant positions [12].In the Transformer this isreduced to a constant number of operations, albeit at the cost of reduced effective resolution dueto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,", "page_start": 1, "page_end": 2, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.5641417}, {"chunk_id": "6ecb21e7-c073-4d90-9d8a-41fdfda06afb", "text": "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,textual entailment and learning task-independent sentence representations [4, 27, 28, 22].End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering andlanguage modeling tasks [34].To the best of our knowledge, however, the Transformer is the first transduction model relyingentirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.In the following sections, we will describe the Transformer, motivateself-attention and discuss its advantages over models such as [17, 18] and [9].3Model ArchitectureMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequenceof continuous representations z = (z1, ..., zn).Given z, the decoder then generates an outputsequence (y1, ..., ym) of symbols one element at a time.At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next.2Figure 1: The Transformer - model architecture.The Transformer follows this overall architecture using stacked self-attention and point-wise, fullyconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,respectively.3.1Encoder and Decoder StacksThe encoder is composed of a stack of N = 6 identical layers.Each layer has twoEncoder:sub-layers.The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network.We employ a residual connection [11] around each ofthe two sub-layers, followed by layer normalization [1].That is, the output of each sub-layer isLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layeritself.To facilitate these residual connections, all sub-layers in the model, as well as the embeddinglayers, produce outputs of dimension dmodel = 512.The decoder is also composed of a stack of N = 6 identical layers.In addition to the twoDecoder:sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-headattention over the output of the encoder stack.Similar to the encoder, we employ residual connectionsaround each of the sub-layers, followed by layer normalization.We also modify the self-attentionsub-layer in the decoder stack to prevent positions from attending to subsequent positions.Thismasking, combined with fact that the output embeddings are offset by one position, ensures that the", "page_start": 2, "page_end": 3, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.5563942}, {"chunk_id": "d487540d-470c-4230-b88f-4a3e27e977d2", "text": "Similar to the encoder, we employ residual connectionsaround each of the sub-layers, followed by layer normalization.We also modify the self-attentionsub-layer in the decoder stack to prevent positions from attending to subsequent positions.Thismasking, combined with fact that the output embeddings are offset by one position, ensures that thepredictions for position i can depend only on the known outputs at positions less than i.3.2AttentionAn attention function can be described as mapping a query and a set of key-value pairs to an output,where the query, keys, values, and output are all vectors.The output is computed as a weighted sum3Scaled Dot-Product AttentionMulti-Head AttentionFigure 2: (left) Scaled Dot-Product Attention.(right) Multi-Head Attention consists of severalattention layers running in parallel.of the values, where the weight assigned to each value is computed by a compatibility function of thequery with the corresponding key.3.2.1Scaled Dot-Product AttentionWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2).The input consists ofqueries and keys of dimension dk, and values of dimension dv.We compute the dot products of thequery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on thevalues.In practice, we compute the attention function on a set of queries simultaneously, packed togetherinto a matrix Q.The keys and values are also packed together into matrices K and V .We computethe matrix of outputs as:Attention(Q, K, V ) = softmax(QKT√dk)V(1)The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention.Dot-product attention is identical to our algorithm, except for the scaling factor1of√dk .Additive attention computes the compatibility function using a feed-forward network witha single hidden layer.While the two are similar in theoretical complexity, dot-product attention ismuch faster and more space-efficient in practice, since it can be implemented using highly optimizedmatrix multiplication code.While for small values of dk the two mechanisms perform similarly, additive attention outperformsdot product attention without scaling for larger values of dk [3].We suspect that for large values ofdk, the dot products grow large in magnitude, pushing the softmax function into regions where it has1extremely small gradients 4.To counteract this effect, we scale the dot products by√dk .3.2.2Multi-Head AttentionInstead of performing a single attention function with dmodel-dimensional keys, values and queries,we found it beneficial to linearly project the queries, keys and values h times with different, learnedlinear projections to dk, dk and dv dimensions, respectively.On each of these projected versions ofqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional", "page_start": 3, "page_end": 4, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.54014}, {"chunk_id": "92be8a85-80ac-4327-a0b3-f2fff684bda4", "text": "the input sequence centered around the respective output position.This would increase the maximumpath length to O(n/r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k < n does not connect all pairs of input and outputpositions.Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest pathsbetween any two positions in the network.Convolutional layers are generally more expensive thanrecurrent layers, by a factor of k.Separable convolutions [6], however, decrease the complexityconsiderably, to O(k · n · d + n · d2).Even with k = n, however, the complexity of a separableconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,the approach we take in our model.As side benefit, self-attention could yield more interpretable models.We inspect attention distributionsfrom our models and present and discuss examples in the appendix.Not only do individual attentionheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntacticand semantic structure of the sentences.5TrainingThis section describes the training regime for our models.5.1Training Data and BatchingWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 millionsentence pairs.Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens.For English-French, we used the significantly larger WMT2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piecevocabulary [38].Sentence pairs were batched together by approximate sequence length.Each trainingbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000target tokens.5.2Hardware and ScheduleWe trained our models on one machine with 8 NVIDIA P100 GPUs.For our base models usingthe hyperparameters described throughout the paper, each training step took about 0.4 seconds.Wetrained the base models for a total of 100,000 steps or 12 hours.For our big models,(described on thebottom line of table 3), step time was 1.0 seconds.The big models were trained for 300,000 steps(3.5 days).5.3OptimizerWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9.We varied the learningrate over the course of training, according to the formula:lrate = d−0.5model · min(step_num−0.5, step_num · warmup_steps−1.5)(3)This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step number.We usedwarmup_steps = 4000.5.4RegularizationWe employ three types of regularization during training:7", "page_start": 7, "page_end": 7, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.5342657}, {"chunk_id": "58f1c685-b5d0-4ba4-8711-0ba5768f729c", "text": "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,we found it beneficial to linearly project the queries, keys and values h times with different, learnedlinear projections to dk, dk and dv dimensions, respectively.On each of these projected versions ofqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional4To illustrate why the dot products get large, assume that the components of q and k are independent randomvariables with mean 0 and variance 1.Then their dot product, q · k = Pdki=1 qiki, has mean 0 and variance dk.4output values.These are concatenated and once again projected, resulting in the final values, asdepicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representationsubspaces at different positions.With a single attention head, averaging inhibits this.MultiHead(Q, K, V ) = Concat(head1, ..., headh)W Owhere headi = Attention(QW Qi , KW Ki , V W Vi )Where the projections are parameter matrices W Q∈Rdmodel×dk, W V∈Rdmodel×dv∈Rdmodel×dk, W Kiiiand W O ∈Rhdv×dmodel.In this work we employ h = 8 parallel attention layers, or heads.For each of these we usedk = dv = dmodel/h = 64.Due to the reduced dimension of each head, the total computational costis similar to that of single-head attention with full dimensionality.3.2.3Applications of Attention in our ModelThe Transformer uses multi-head attention in three different ways:• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,and the memory keys and values come from the output of the encoder.This allows everyposition in the decoder to attend over all positions in the input sequence.This mimics thetypical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38, 2, 9].• The encoder contains self-attention layers.In a self-attention layer all of the keys, valuesand queries come from the same place, in this case, the output of the previous layer in theencoder.Each position in the encoder can attend to all positions in the previous layer of theencoder.• Similarly, self-attention layers in the decoder allow each position in the decoder to attend toall positions in the decoder up to and including that position.We need to prevent leftwardinformation flow in the decoder to preserve the auto-regressive property.We implement thisinside of scaled dot-product attention by masking out (setting to −∞) all values in the inputof the softmax which correspond to illegal connections.See Figure 2.3.3Position-wise Feed-Forward NetworksIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fullyconnected feed-forward network, which is applied to each position separately and identically.This", "page_start": 4, "page_end": 5, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.53122437}, {"chunk_id": "0b373c99-8c0e-44bf-8fc7-f6b6b009590d", "text": "Provided proper attribution is provided, Google hereby grants permission toreproduce the tables and figures in this paper solely for use in journalistic orscholarly works.Ashish Vaswani∗Noam Shazeer∗Niki Parmar∗Jakob Uszkoreit∗Google BrainGoogle BrainGoogle ResearchGoogle Researchavaswani@google.comnoam@google.comnikip@google.comusz@google.comLlion Jones∗Aidan N.Gomez∗†Łukasz Kaiser∗Google ResearchUniversity of TorontoGoogle Brainlukaszkaiser@google.comllion@google.comaidan@cs.toronto.eduIllia Polosukhin∗‡illia.polosukhin@gmail.comAbstractThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder.The bestperforming models also connect the encoder and decoder through an attentionmechanism.We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely.Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring significantlyless time to train.Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU.On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.8 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature.We show that the Transformer generalizes well toother tasks by applying it successfully to English constituency parsing both withlarge and limited training data.∗Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and startedthe effort to evaluate this idea.Ashish, with Illia, designed and implemented the first Transformer models andhas been crucially involved in every aspect of this work.Noam proposed scaled dot-product attention, multi-headattention and the parameter-free position representation and became the other person involved in nearly everydetail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase andtensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, andefficient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of andimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks", "page_start": 1, "page_end": 2, "section_title": "", "score": 0.46042842}], "multihop_nda_01": [{"chunk_id": "94d954fb-950b-4dcf-8049-c72deb8243c9", "text": "Date: 201[ ]Parties:[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]and[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]1.Each of the parties to this Agreement intends to disclose information (the ConfidentialInformation) to the other party for the purpose of [insert details e.g.discussing thepossibility of the parties entering into a joint venture] (the Purpose).2.Each party to this Agreement is referred to as ‘the Recipient’ when it receives or usesthe Confidential Information disclosed by the other party.3.The Recipient undertakes not to use the Confidential Information disclosed by the otherparty for any purpose except the Purpose, without first obtaining the written agreementof the other party.4.The Recipient undertakes to keep the Confidential Information disclosed by the otherparty secure and not to disclose it to any third party [except to its employees [andprofessional advisers] who need to know the same for the Purpose, who know theyowe a duty of confidence to the other party and who are bound by obligationsequivalent to those in clause 3 above and this clause 4.5.The undertakings in clauses 3 and 4 above apply to all of the information disclosed byeach of the parties to the other, regardless of the way or form in which it is disclosed orrecorded but they do not apply to:a) any information which is or in future comes into the public domain (unless as a result ofthe breach of this Agreement); orb) any information which is already known to the Recipient and which was not subject toany obligation of confidence before it was disclosed to the Recipient by the other party.6.Nothing in this Agreement will prevent the Recipient from making any disclosure of theConfidential Information required by law or by any competent authority.7.The Recipient will, on request from the other party, return all copies and records of theConfidential Information disclosed by the other party to the Recipient and will not retainany copies or records of the Confidential Information disclosed by the other party.8.Neither this Agreement nor the supply of any information grants the Recipient anylicence, interest or right in respect of any intellectual property rights of the other partyexcept the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.", "page_start": 1, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.5164414}, {"chunk_id": "6c64d139-fd3d-400d-b64b-3ddf0dea3473", "text": "except the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.The English Courts will have non-exclusive jurisdiction to deal with any dispute whichhas arisen or may arise out of, or in connection with, this Agreement.Signed [by [insert name]] OR [on behalf of][insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________PositionSigned [by [insert name]] OR [on behalf of] [insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________Position", "page_start": 2, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.4752222}], "negative_nvidia_01": [{"chunk_id": "a012dfff-19d8-49a4-b8a0-ec4462727f0c", "text": "markets with shared underlying technology by using a variety of software stacks developed either internally or by third-party developers and partners.We utilizethis platform approach in each of our target markets.Extending our technology and platform leadership in AI.We provide a complete, end-to-end accelerated computing platform for AI, addressing both trainingand inferencing.This includes full-stack data center-scale compute and networking solutions across processing units, interconnects, systems, and software.Ourcompute solutions include all three major processing units in AI servers – GPUs, CPUs, and DPUs.GPUs are uniquely suited to AI, and we will continue to addAI-specific features to our GPU architecture to further extend our leadership position.In addition, we offer DGX Cloud, an AI-training-as-a-service platform, andNeMo – a complete solution for building enterprise-ready Large Language Models, or LLMs, using open source and proprietary LLMs created by NVIDIA andthird parties.Our AI technology leadership is reinforced by our large and expanding ecosystem in a virtuous cycle.Our computing platforms are available fromvirtually every major server maker and CSP, as well as on our own AI supercomputers.There are over 4.7 million developers worldwide using CUDA and ourother software tools to help deploy our technology in our target markets.We evangelize AI through partnerships with hundreds of universities and thousands ofstartups through our Inception program.Additionally, our Deep Learning Institute provides instruction on the latest techniques on how to design, train, and deployneural networks in applications using our accelerated computing platform.Extending our technology and platform leadership in computer graphics.We believe that computer graphics infused with AI is fundamental to thecontinued expansion and evolution of computing.We apply our research and development resources to enhance the user experience for consumerentertainment and professional visualization applications and create new virtual world and simulation capabilities.Our technologies are instrumental in drivingthe gaming, design, and creative industries forward, as developers leverage our libraries and algorithms to deliver an optimized experience on our GeForce andNVIDIA RTX platforms.Our computer graphics platforms leverage AI end-to-end, from the developer tools and cloud services to the Tensor Cores included in allRTX-class GPUs.For example, NVIDIA Avatar Cloud Engine, or ACE, is a suite of technologies that help developers bring digital avatars to life with generativeAI, running in the cloud or locally on the PC.GeForce Experience enhances each gamer’s experience by optimizing their PC’s settings, as well as enabling therecording and sharing of gameplay.Our Studio drivers enhance and accelerate a number of popular creative applications.Omniverse is real-time 3D designcollaboration and virtual world simulation software that empowers artists, designers, and creators to connect and collaborate in leading design applications.Wealso enable interactive graphics applications - such as games, movie and photo editing and design software - to be accessed by almost any device, almostanywhere, through our cloud platforms such as vGPU for enterprise and GeForce NOW for gaming.Advancing the leading autonomous vehicle platform.", "page_start": 7, "page_end": 7, "section_title": "NVIDIA CORPORATION", "score": 0.5426532}, {"chunk_id": "ea4279b8-2d86-4d6d-8b49-3cae1b849f61", "text": "company with data-center-scale offerings that are reshaping industry.Our full-stack includes the foundational CUDA programming model that runs on all NVIDIA GPUs, as well as hundreds of domain-specific software libraries,software development kits, or SDKs, and Application Programming Interfaces, or APIs.This deep and broad software stack accelerates the performance andeases the deployment of NVIDIA accelerated computing for computationally intensive workloads such as artificial intelligence, or AI, model training andinference, data analytics, scientific computing, and 3D graphics, with vertical-specific optimizations to address industries ranging from healthcare and telecom toautomotive and manufacturing.Our data-center-scale offerings are comprised of compute and networking solutions that can scale to tens of thousands of GPU-accelerated serversinterconnected to function as a single giant computer; this type of data center architecture and scale is needed for the development and deployment of modernAI applications.The GPU was initially used to simulate human imagination, enabling the virtual worlds of video games and films.Today, it also simulates human intelligence,enabling a deeper understanding of the physical world.Its parallel processing capabilities, supported by thousands of computing cores, are essential for deeplearning algorithms.This form of AI, in which software writes itself by learning from large amounts of data, can serve as the brain of computers, robots and selfdriving cars that can perceive and understand the world.GPU-powered AI solutions are being developed by thousands of enterprises to deliver services andproducts that would have been immensely difficult or even impossible with traditional coding.Examples include generative AI, which can create new contentsuch as text, code, images, audio, video, and molecule structures, and recommendation systems, which can recommend highly relevant content such asproducts, services, media or ads using deep neural networks trained on vast datasets that capture the user preferences.NVIDIA has a platform strategy, bringing together hardware, systems, software, algorithms, libraries, and services to create unique value for the markets weserve.While the computing requirements of these end markets are diverse, we address them with a unified underlying architecture leveraging our GPUs andnetworking and software stacks.The programmable nature of our architecture allows us to support several multi-billion-dollar end markets with the sameunderlying technology by using a variety of software stacks developed either internally or by third-party developers and partners.The large and growing numberof developers and installed base across our platforms strengthens our ecosystem and increases the value of our platform to our customers.Innovation is at our core.We have invested over $45.3 billion in research and development since our inception, yielding inventions that are essential to moderncomputing.Our invention of the GPU in 1999 sparked the growth of the PC gaming market and redefined computer graphics.With our introduction of the CUDAprogramming model in 2006, we opened the parallel processing capabilities of our GPU to a broad range of compute-intensive applications, paving the way forthe emergence of modern AI.", "page_start": 4, "page_end": 4, "section_title": "NVIDIA CORPORATION", "score": 0.5423167}, {"chunk_id": "93b89655-c91e-4ed1-b998-77653bf71744", "text": "Our invention of the GPU in 1999 sparked the growth of the PC gaming market and redefined computer graphics.With our introduction of the CUDAprogramming model in 2006, we opened the parallel processing capabilities of our GPU to a broad range of compute-intensive applications, paving the way forthe emergence of modern AI.In 2012, the AlexNet neural network, trained on NVIDIA GPUs, won the ImageNet computer image recognition competition,marking the “Big Bang” moment of AI.We introduced our first Tensor Core GPU in 2017, built from the ground-up for the new era of AI, and our first autonomousdriving system-on-chips, or SoC, in 2018.Our acquisition of Mellanox in 2020 expanded our innovation canvas to include networking and led to the introductionof a new processor class – the data processing unit, or DPU.Over the past 5 years, we have built full software stacks that run on top of our GPUs and CUDA tobring AI to the world’s largest industries, including NVIDIA DRIVE stack for autonomous driving, Clara for healthcare, and Omniverse for industrial digitalization;and introduced the NVIDIA AI Enterprise software – essentially an operating system for enterprise AI applications.In 2023, we introduced our first data centerCPU, Grace, built for giant-scale AI and high-performance computing.With a strong engineering culture, we drive fast, yet harmonized, product and technologyinnovations in all dimensions of computing including silicon, systems, networking, software and algorithms.More than half of our engineers work on software.The world’s leading cloud service providers, or CSPs, and consumer internet companies use our data center-scale accelerated computing platforms to enable,accelerate or enrich the services they deliver to billions of end users, including AI solutions and assistants, search, recommendations, social networking, onlineshopping, live video, and translation.Enterprises and startups across a broad range of industries use our accelerated computing platforms to build new generative AI-enabled products and services,or to dramatically accelerate and reduce the costs of their workloads and workflows.The enterprise software industry uses them for new AI assistants andchatbots; the transportation industry for autonomous driving; the healthcare industry for accelerated and computer-aided drug discovery; and the financialservices industry for customer support and fraud detection.4Researchers and developers use our computing solutions to accelerate a wide range of important applications, from simulating molecular dynamics to climateforecasting.With support for more than 3,500 applications, NVIDIA computing enables some of the most promising areas of discovery, from climate prediction tomaterials science and from wind tunnel simulation to genomics.Including GPUs and networking, NVIDIA powers over 75% of the supercomputers on the globalTOP500 list, including 24 of the top 30 systems on the Green500 list.Gamers choose NVIDIA GPUs to enjoy immersive, increasingly cinematic virtual worlds.In addition to serving the growing number of gamers, the market for PCGPUs is expanding because of the burgeoning population of live streamers, broadcasters, artists, and creators.", "page_start": 4, "page_end": 5, "section_title": "NVIDIA CORPORATION", "score": 0.5392433}, {"chunk_id": "af665b9d-5bc9-4f58-8c86-b860d78f48e8", "text": "TOP500 list, including 24 of the top 30 systems on the Green500 list.Gamers choose NVIDIA GPUs to enjoy immersive, increasingly cinematic virtual worlds.In addition to serving the growing number of gamers, the market for PCGPUs is expanding because of the burgeoning population of live streamers, broadcasters, artists, and creators.With the advent of generative AI, we expect abroader set of PC users to choose NVIDIA GPUs for running generative AI applications locally on their PC, which is critical for privacy, latency, and cost-sensitiveAI applications.Professional artists, architects and designers use NVIDIA partner products accelerated with our GPUs and software platform for a range of creative and designuse cases, such as creating visual effects in movies or designing buildings and products.In addition, generative AI is expanding the market for our workstationclass GPUs, as more enterprise customers develop and deploy AI applications with their data on-premises.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Our BusinessesWe report our business results in two segments.The Compute & Networking segment is comprised of our Data Center accelerated computing platforms and end-to-end networking platforms including Quantumfor InfiniBand and Spectrum for Ethernet; our NVIDIA DRIVE automated-driving platform and automotive development agreements; Jetson robotics and otherembedded platforms; NVIDIA AI Enterprise and other software; and DGX Cloud software and services.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure; Quadro/NVIDIARTX GPUs for enterprise workstation graphics; virtual GPU, or vGPU, software for cloud-based visual and virtual computing; automotive platforms forinfotainment systems; and Omniverse Enterprise software for building and operating metaverse and 3D internet applications.Our MarketsWe specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Data CenterThe NVIDIA Data Center platform is focused on accelerating the most compute-intensive workloads, such as AI, data analytics, graphics and scientificcomputing, delivering significantly better performance and power efficiency relative to conventional CPU-only approaches.It is deployed in cloud, hyperscale,on-premises and edge data centers.The platform consists of compute and networking offerings typically delivered to customers as systems, subsystems, ormodules, along with software and services.Our compute offerings include supercomputing platforms and servers, bringing together our energy efficient GPUs, DPUs, interconnects, and fully optimized AIand high-performance computing, or HPC, software stacks.In addition, they include NVIDIA AI Enterprise software; our DGX Cloud service; and a growing bodyof acceleration libraries, APIs, SDKs, and domain-specific application frameworks.Our networking offerings include end-to-end platforms for InfiniBand and Ethernet, consisting of network adapters, cables, DPUs, and switch systems, as well asa full software stack.", "page_start": 5, "page_end": 5, "section_title": "NVIDIA CORPORATION", "score": 0.52703214}, {"chunk_id": "f2207e14-4c6c-421b-849b-76359737cca1", "text": "Germany, India, Israel, and Taiwan for fiscal years 2005 through 2023.Note 15 - Shareholders’ EquityCapital Return ProgramIn August 2023, our Board of Directors approved an increase to our share repurchase program of an additional $25.0 billion, without expiration.During fiscalyear 2024, we repurchased 21 million shares of our common stock for $9.7 billion.As of January 28, 2024, we were authorized, subject to certain specifications,to repurchase additional shares of our common stock up to $22.5 billion.From January 29, 2024 through February 16, 2024, we repurchased 2.8 million sharesfor $1.9 billion pursuant to a Rule 10b5-1 trading plan.Our share repurchase program aims to offset dilution from shares issued to employees.We may pursueadditional share repurchases as we weigh market factors and other investment opportunities.During fiscal years 2024, 2023, and 2022, we paid $395 million, $398 million, and $399 million in cash dividends to our shareholders, respectively.Our cashdividend program and the payment of future cash dividends under that program are subject to our Board of Directors' continuing determination that the dividendprogram and the declaration of dividends thereunder are in the best interests of our shareholders.In fiscal year 2022, we retired our existing 349 million treasury shares.These shares assumed the status of authorized and unissued shares upon retirement.The excess of repurchase price over par value was allocated between additional paid-in capital and retained earnings, resulting in a reduction in additional paidin capital by $20 million and retained earnings by $12.0 billion.Any future repurchased shares will assume the status of authorized and unissued shares.77NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Note 16 - Employee Retirement PlansWe provide tax-qualified defined contribution plans to eligible employees in the U.S.and certain other countries.Our contribution expense for fiscal years 2024,2023, and 2022 was $255 million, $227 million, and $168 million, respectively.Note 17 - Segment InformationOur Chief Executive Officer, who is considered to be our chief operating decision maker, or CODM, reviews financial information presented on an operatingsegment basis for purposes of making decisions and assessing financial performance.The Compute & Networking segment includes our Data Center accelerated computing platform; networking; automotive artificial intelligence, or AI, Cockpit,autonomous driving development agreements, and autonomous vehicle solutions; electric vehicle computing platforms; Jetson for robotics and other embeddedplatforms; NVIDIA AI Enterprise and other software; and DGX Cloud.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions forgaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across our", "page_start": 77, "page_end": 78, "section_title": "NVIDIA CORPORATION", "score": 0.518052}, {"chunk_id": "0758e052-9cde-4a60-9208-4a87e9eb14ce", "text": "robots and vehicles, powered by NVIDIA accelerated computing infrastructure on-premises and in the cloud.AutomotiveAutomotive market is comprised of platform solutions for automated driving and in-vehicle cockpit computing.Leveraging our technology leadership in AI andbuilding on our long-standing automotive relationships, we are delivering a complete end-to-end solution for the AV market under the DRIVE Hyperion brand.Wehave demonstrated multiple applications of AI within the car: AI can drive the car itself as a pilot in fully autonomous mode or it can also be a co-pilot, assistingthe human driver while creating a safer driving experience.6We are working with several hundred partners in the automotive ecosystem including automakers, truck makers, tier-one suppliers, sensor manufacturers,automotive research institutions, HD mapping companies, and startups to develop and deploy AI systems for self-driving vehicles.Our unified AI computingarchitecture starts with training deep neural networks using our Data Center computing solutions, and then running a full perception, fusion, planning, and controlstack within the vehicle on the NVIDIA DRIVE Hyperion platform.DRIVE Hyperion consists of the high-performance, energy efficient DRIVE AGX computinghardware, a reference sensor set that supports full self-driving capability as well as an open, modular DRIVE software platform for autonomous driving, mapping,and parking services, and intelligent in-vehicle experiences.In addition, we offer a scalable data center-based simulation solution, NVIDIA DRIVE Sim, based on NVIDIA Omniverse software, for digital cockpitdevelopment, as well as for testing and validating a self-driving platform.Our unique end-to-end, software-defined approach is designed for continuousinnovation and continuous development, enabling cars to receive over-the-air updates to add new features and capabilities throughout the life of a vehicle.Business StrategiesNVIDIA’s key strategies that shape our overall business approach include:Advancing the NVIDIA accelerated computing platform.Our accelerated computing platform can solve complex problems in significantly less time and withlower power consumption than alternative computational approaches.Indeed, it can help solve problems that were previously deemed unsolvable.We work todeliver continued performance leaps that outpace Moore’s Law by leveraging innovation across the architecture, chip design, system, interconnect, and softwarelayers.This full-stack innovation approach allows us to deliver order-of-magnitude performance advantages relative to legacy approaches in our target markets,which include Data Center, Gaming, Professional Visualization, and Automotive.While the computing requirements of these end markets are diverse, weaddress them with a unified underlying architecture leveraging our GPUs, CUDA and networking technologies as the fundamental building blocks.Theprogrammable nature of our architecture allows us to make leveraged investments in research and development: we can support several multi-billion-dollar endmarkets with shared underlying technology by using a variety of software stacks developed either internally or by third-party developers and partners.We utilizethis platform approach in each of our target markets.Extending our technology and platform leadership in AI.We provide a complete, end-to-end accelerated computing platform for AI, addressing both trainingand inferencing.", "page_start": 6, "page_end": 7, "section_title": "NVIDIA CORPORATION", "score": 0.51575506}, {"chunk_id": "0dd7fcd2-abcb-4469-bfdc-d6d5c6cb8eae", "text": "*$100 invested on 1/27/19 in stock and in indices, including reinvestment of dividends.Source: FactSet financial data and analytics.1/27/20191/26/20201/31/20211/30/20221/29/20231/28/2024NVIDIA Corporation$100.00$157.02$326.26$574.15$512.40$1,536.28S&P 500$100.00$126.17$144.83$175.25$163.63$199.83Nasdaq 100$100.00$136.15$194.20$218.68$185.67$268.13Item 6.[Reserved]33Item 7.Management's Discussion and Analysis of Financial Condition and Results of OperationsThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with “Item 1A.Risk Factors”, ourConsolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described elsewhere in this Annual Report onForm 10-K, before deciding to purchase, hold or sell shares of our common stock.OverviewOur Company and Our BusinessesNVIDIA pioneered accelerated computing to help solve the most challenging computational problems.Since our original focus on PC graphics, we haveexpanded to several other large and important computationally intensive fields.NVIDIA has leveraged its GPU architecture to create platforms for acceleratedcomputing, AI solutions, scientific computing, data science, AV, robotics, metaverse and 3D internet applications.Our two operating segments are \"Compute & Networking\" and \"Graphics.\" Refer to Note 17 of the Notes to the Consolidated Financial Statements in Part IV,Item 15 of this Annual Report on Form 10-K for additional information.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Recent Developments, Future Objectives and ChallengesDemand and Supply, Product Transitions, and New Products and Business ModelsDemand for our data center systems and products surged in fiscal year 2024.Entering fiscal year 2025, we are gathering customer demand indications acrossseveral product transitions.We have demand visibility for our new data center products ramping later in fiscal year 2025.We have increased our supply andcapacity purchases with existing suppliers, added new vendors and entered into prepaid manufacturing and capacity agreements.These increased purchasevolumes, the number of suppliers, and the integration of new vendors into our supply chain may create more complexity and execution risk.Our purchasecommitments and obligations for inventory and manufacturing capacity at the end of fiscal year 2024 were impacted by shortening lead times for certaincomponents.We may continue to enter into new supplier and capacity arrangements.Supply of Hopper architecture products is improving, and demand remainsvery strong.We expect our next-generation products to be supply-constrained based upon demand indications.", "page_start": 33, "page_end": 34, "section_title": "NVIDIA CORPORATION", "score": 0.5153957}, {"chunk_id": "f1afdb9e-f35c-4c22-a847-c7d9a35572e3", "text": "Many factors propel its growth, including new high production valuegames and franchises, the continued rise of competitive gaming, or eSports, social connectivity and the increasing popularity of game streamers, modders, orgamers who remaster games, and creators.Our gaming platforms leverage our GPUs and sophisticated software to enhance the gaming experience with smoother, higher quality graphics.We developedNVIDIA RTX to bring next generation graphics and AI to games.NVIDIA RTX features ray tracing technology for real-time, cinematic-quality rendering.Raytracing, which has long been used for special effects in the movie industry, is a computationally intensive technique that simulates the physical behavior of lightto achieve greater realism in computer-generated scenes.NVIDIA RTX also features deep learning super sampling, or NVIDIA DLSS, our AI technology thatboosts frame rates while generating beautiful, sharp images for games.RTX GPUs will also accelerate a new generation of AI applications.With an installedbase of over 100 million AI capable PCs, more than 500 RTX AI-enabled applications and games, and a robust suite of development tools, RTX is already the AIPC leader.Our products for the gaming market include GeForce RTX and GeForce GTX GPUs for gaming desktop and laptop PCs, GeForce NOW cloud gaming forplaying PC games on underpowered devices, as well as SoCs and development services for game consoles.Professional VisualizationWe serve the Professional Visualization market by working closely with independent software vendors, or ISVs, to optimize their offerings for NVIDIA GPUs.OurGPU computing platform enhances productivity and introduces new capabilities for critical workflows in many fields, such as design and manufacturing anddigital content creation.Design and manufacturing encompass computer-aided design, architectural design, consumer-products manufacturing, medicalinstrumentation, and aerospace.Digital content creation includes professional video editing and post-production, special effects for films, and broadcasttelevision graphics.The NVIDIA RTX platform makes it possible to render film-quality, photorealistic objects and environments with physically accurate shadows, reflections andrefractions using ray tracing in real-time.Many leading 3D design and content creation applications developed by our ecosystem partners now support RTX,allowing professionals to accelerate and transform their workflows with NVIDIA RTX GPUs and software.We offer NVIDIA Omniverse as a development platform and operating system for building virtual world simulation applications, available as a softwaresubscription for enterprise use and free for individual use.Industrial enterprises are adopting Omniverse’s 3D and simulation technologies to digitalize theircomplex physical assets, processes, and environments – building digital twins of factories, real time 3D product configurators, testing and validating autonomousrobots and vehicles, powered by NVIDIA accelerated computing infrastructure on-premises and in the cloud.AutomotiveAutomotive market is comprised of platform solutions for automated driving and in-vehicle cockpit computing.Leveraging our technology leadership in AI andbuilding on our long-standing automotive relationships, we are delivering a complete end-to-end solution for the AV market under the DRIVE Hyperion brand.We", "page_start": 6, "page_end": 6, "section_title": "NVIDIA CORPORATION", "score": 0.5113677}], "negative_nvidia_02": [{"chunk_id": "4d665fa7-dd83-464c-908e-2634e7e09406", "text": "10or generate enough renewable energy to match 100% of our global electricity usage for our offices and data centers.In fiscal year 2023, we increased thepercentage of our total electricity use matched by renewable energy purchases to 44%.By fiscal year 2026, we aim to engage manufacturing supplierscomprising at least 67% of NVIDIA’s scope 3 category 1 GHG emissions with goal of effecting supplier adoption of science-based targets.Whether it is creation of technology to power next-generation laptops or designs to support high-performance supercomputers, improving energy efficiency isimportant in our research, development, and design processes.GPU-accelerated computing is inherently more energy efficient than traditional computing formany workloads because it is optimized for throughput, performance per watt, and certain AI workloads.The energy efficiency of our products is evidenced byour continued strong presence on the Green500 list of the most energy-efficient systems.We powered 24 of the top 30 most energy efficient systems, includingthe top supercomputer, on the Green500 list.We plan to build Earth-2, a digital twin of the Earth on NVIDIA AI and NVIDIA Omniverse platforms.Earth-2 will enable scientists, companies, and policy makersto do ultra-high-resolution predictions of the impact of climate change and explore mitigation and adaptation strategies.Human Capital ManagementWe believe that our employees are our greatest assets, and they play a key role in creating long-term value for our stakeholders.As of the end of fiscal year2024, we had approximately 29,600 employees in 36 countries, 22,200 were engaged in research and development and 7,400 were engaged in sales,marketing, operations, and administrative positions.The Compensation Committee of our Board of Directors assists in the oversight of policies and strategiesrelating to human capital management.To be competitive and execute our business strategy successfully, we must recruit, develop, and retain talented employees, including qualified executives,scientists, engineers, and technical and non-technical staff.RecruitmentAs the demand for global technical talent continues to be competitive, we have grown our technical workforce and have been successful in attracting top talentto NVIDIA.We have attracted talent globally through our strong employer brand and differentiated hiring strategies for college, professional, and leadershiptalent.Our workforce is 83% technical and 49% hold advanced degrees.Additionally, we have increased focus on diversity recruiting, resulting in an increase inglobal female hiring in each channel.Our own employees help to surface top talent, with over 40% of our new hires in fiscal year 2024 coming from employeereferrals.Development and RetentionTo support employee development, we provide opportunities to learn on-the-job through training courses, targeted development programs, mentoring and peercoaching and ongoing feedback.We have a library of live and on-demand learning experiences that include workshops, panel discussions, and speaker forums.We create learning paths focused on our most common development needs and constantly upgrade our offerings to ensure that our employees are exposed tothe most current content and technologies available.", "page_start": 10, "page_end": 11, "section_title": "NVIDIA CORPORATION", "score": 0.6144456}, {"chunk_id": "f2207e14-4c6c-421b-849b-76359737cca1", "text": "Germany, India, Israel, and Taiwan for fiscal years 2005 through 2023.Note 15 - Shareholders’ EquityCapital Return ProgramIn August 2023, our Board of Directors approved an increase to our share repurchase program of an additional $25.0 billion, without expiration.During fiscalyear 2024, we repurchased 21 million shares of our common stock for $9.7 billion.As of January 28, 2024, we were authorized, subject to certain specifications,to repurchase additional shares of our common stock up to $22.5 billion.From January 29, 2024 through February 16, 2024, we repurchased 2.8 million sharesfor $1.9 billion pursuant to a Rule 10b5-1 trading plan.Our share repurchase program aims to offset dilution from shares issued to employees.We may pursueadditional share repurchases as we weigh market factors and other investment opportunities.During fiscal years 2024, 2023, and 2022, we paid $395 million, $398 million, and $399 million in cash dividends to our shareholders, respectively.Our cashdividend program and the payment of future cash dividends under that program are subject to our Board of Directors' continuing determination that the dividendprogram and the declaration of dividends thereunder are in the best interests of our shareholders.In fiscal year 2022, we retired our existing 349 million treasury shares.These shares assumed the status of authorized and unissued shares upon retirement.The excess of repurchase price over par value was allocated between additional paid-in capital and retained earnings, resulting in a reduction in additional paidin capital by $20 million and retained earnings by $12.0 billion.Any future repurchased shares will assume the status of authorized and unissued shares.77NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Note 16 - Employee Retirement PlansWe provide tax-qualified defined contribution plans to eligible employees in the U.S.and certain other countries.Our contribution expense for fiscal years 2024,2023, and 2022 was $255 million, $227 million, and $168 million, respectively.Note 17 - Segment InformationOur Chief Executive Officer, who is considered to be our chief operating decision maker, or CODM, reviews financial information presented on an operatingsegment basis for purposes of making decisions and assessing financial performance.The Compute & Networking segment includes our Data Center accelerated computing platform; networking; automotive artificial intelligence, or AI, Cockpit,autonomous driving development agreements, and autonomous vehicle solutions; electric vehicle computing platforms; Jetson for robotics and other embeddedplatforms; NVIDIA AI Enterprise and other software; and DGX Cloud.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions forgaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across our", "page_start": 77, "page_end": 78, "section_title": "NVIDIA CORPORATION", "score": 0.6141245}, {"chunk_id": "0dd7fcd2-abcb-4469-bfdc-d6d5c6cb8eae", "text": "*$100 invested on 1/27/19 in stock and in indices, including reinvestment of dividends.Source: FactSet financial data and analytics.1/27/20191/26/20201/31/20211/30/20221/29/20231/28/2024NVIDIA Corporation$100.00$157.02$326.26$574.15$512.40$1,536.28S&P 500$100.00$126.17$144.83$175.25$163.63$199.83Nasdaq 100$100.00$136.15$194.20$218.68$185.67$268.13Item 6.[Reserved]33Item 7.Management's Discussion and Analysis of Financial Condition and Results of OperationsThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with “Item 1A.Risk Factors”, ourConsolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described elsewhere in this Annual Report onForm 10-K, before deciding to purchase, hold or sell shares of our common stock.OverviewOur Company and Our BusinessesNVIDIA pioneered accelerated computing to help solve the most challenging computational problems.Since our original focus on PC graphics, we haveexpanded to several other large and important computationally intensive fields.NVIDIA has leveraged its GPU architecture to create platforms for acceleratedcomputing, AI solutions, scientific computing, data science, AV, robotics, metaverse and 3D internet applications.Our two operating segments are \"Compute & Networking\" and \"Graphics.\" Refer to Note 17 of the Notes to the Consolidated Financial Statements in Part IV,Item 15 of this Annual Report on Form 10-K for additional information.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Recent Developments, Future Objectives and ChallengesDemand and Supply, Product Transitions, and New Products and Business ModelsDemand for our data center systems and products surged in fiscal year 2024.Entering fiscal year 2025, we are gathering customer demand indications acrossseveral product transitions.We have demand visibility for our new data center products ramping later in fiscal year 2025.We have increased our supply andcapacity purchases with existing suppliers, added new vendors and entered into prepaid manufacturing and capacity agreements.These increased purchasevolumes, the number of suppliers, and the integration of new vendors into our supply chain may create more complexity and execution risk.Our purchasecommitments and obligations for inventory and manufacturing capacity at the end of fiscal year 2024 were impacted by shortening lead times for certaincomponents.We may continue to enter into new supplier and capacity arrangements.Supply of Hopper architecture products is improving, and demand remainsvery strong.We expect our next-generation products to be supply-constrained based upon demand indications.", "page_start": 33, "page_end": 34, "section_title": "NVIDIA CORPORATION", "score": 0.5958499}, {"chunk_id": "af665b9d-5bc9-4f58-8c86-b860d78f48e8", "text": "TOP500 list, including 24 of the top 30 systems on the Green500 list.Gamers choose NVIDIA GPUs to enjoy immersive, increasingly cinematic virtual worlds.In addition to serving the growing number of gamers, the market for PCGPUs is expanding because of the burgeoning population of live streamers, broadcasters, artists, and creators.With the advent of generative AI, we expect abroader set of PC users to choose NVIDIA GPUs for running generative AI applications locally on their PC, which is critical for privacy, latency, and cost-sensitiveAI applications.Professional artists, architects and designers use NVIDIA partner products accelerated with our GPUs and software platform for a range of creative and designuse cases, such as creating visual effects in movies or designing buildings and products.In addition, generative AI is expanding the market for our workstationclass GPUs, as more enterprise customers develop and deploy AI applications with their data on-premises.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Our BusinessesWe report our business results in two segments.The Compute & Networking segment is comprised of our Data Center accelerated computing platforms and end-to-end networking platforms including Quantumfor InfiniBand and Spectrum for Ethernet; our NVIDIA DRIVE automated-driving platform and automotive development agreements; Jetson robotics and otherembedded platforms; NVIDIA AI Enterprise and other software; and DGX Cloud software and services.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure; Quadro/NVIDIARTX GPUs for enterprise workstation graphics; virtual GPU, or vGPU, software for cloud-based visual and virtual computing; automotive platforms forinfotainment systems; and Omniverse Enterprise software for building and operating metaverse and 3D internet applications.Our MarketsWe specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Data CenterThe NVIDIA Data Center platform is focused on accelerating the most compute-intensive workloads, such as AI, data analytics, graphics and scientificcomputing, delivering significantly better performance and power efficiency relative to conventional CPU-only approaches.It is deployed in cloud, hyperscale,on-premises and edge data centers.The platform consists of compute and networking offerings typically delivered to customers as systems, subsystems, ormodules, along with software and services.Our compute offerings include supercomputing platforms and servers, bringing together our energy efficient GPUs, DPUs, interconnects, and fully optimized AIand high-performance computing, or HPC, software stacks.In addition, they include NVIDIA AI Enterprise software; our DGX Cloud service; and a growing bodyof acceleration libraries, APIs, SDKs, and domain-specific application frameworks.Our networking offerings include end-to-end platforms for InfiniBand and Ethernet, consisting of network adapters, cables, DPUs, and switch systems, as well asa full software stack.", "page_start": 5, "page_end": 5, "section_title": "NVIDIA CORPORATION", "score": 0.5907971}, {"chunk_id": "93b89655-c91e-4ed1-b998-77653bf71744", "text": "Our invention of the GPU in 1999 sparked the growth of the PC gaming market and redefined computer graphics.With our introduction of the CUDAprogramming model in 2006, we opened the parallel processing capabilities of our GPU to a broad range of compute-intensive applications, paving the way forthe emergence of modern AI.In 2012, the AlexNet neural network, trained on NVIDIA GPUs, won the ImageNet computer image recognition competition,marking the “Big Bang” moment of AI.We introduced our first Tensor Core GPU in 2017, built from the ground-up for the new era of AI, and our first autonomousdriving system-on-chips, or SoC, in 2018.Our acquisition of Mellanox in 2020 expanded our innovation canvas to include networking and led to the introductionof a new processor class – the data processing unit, or DPU.Over the past 5 years, we have built full software stacks that run on top of our GPUs and CUDA tobring AI to the world’s largest industries, including NVIDIA DRIVE stack for autonomous driving, Clara for healthcare, and Omniverse for industrial digitalization;and introduced the NVIDIA AI Enterprise software – essentially an operating system for enterprise AI applications.In 2023, we introduced our first data centerCPU, Grace, built for giant-scale AI and high-performance computing.With a strong engineering culture, we drive fast, yet harmonized, product and technologyinnovations in all dimensions of computing including silicon, systems, networking, software and algorithms.More than half of our engineers work on software.The world’s leading cloud service providers, or CSPs, and consumer internet companies use our data center-scale accelerated computing platforms to enable,accelerate or enrich the services they deliver to billions of end users, including AI solutions and assistants, search, recommendations, social networking, onlineshopping, live video, and translation.Enterprises and startups across a broad range of industries use our accelerated computing platforms to build new generative AI-enabled products and services,or to dramatically accelerate and reduce the costs of their workloads and workflows.The enterprise software industry uses them for new AI assistants andchatbots; the transportation industry for autonomous driving; the healthcare industry for accelerated and computer-aided drug discovery; and the financialservices industry for customer support and fraud detection.4Researchers and developers use our computing solutions to accelerate a wide range of important applications, from simulating molecular dynamics to climateforecasting.With support for more than 3,500 applications, NVIDIA computing enables some of the most promising areas of discovery, from climate prediction tomaterials science and from wind tunnel simulation to genomics.Including GPUs and networking, NVIDIA powers over 75% of the supercomputers on the globalTOP500 list, including 24 of the top 30 systems on the Green500 list.Gamers choose NVIDIA GPUs to enjoy immersive, increasingly cinematic virtual worlds.In addition to serving the growing number of gamers, the market for PCGPUs is expanding because of the burgeoning population of live streamers, broadcasters, artists, and creators.", "page_start": 4, "page_end": 5, "section_title": "NVIDIA CORPORATION", "score": 0.58294797}, {"chunk_id": "ea4279b8-2d86-4d6d-8b49-3cae1b849f61", "text": "company with data-center-scale offerings that are reshaping industry.Our full-stack includes the foundational CUDA programming model that runs on all NVIDIA GPUs, as well as hundreds of domain-specific software libraries,software development kits, or SDKs, and Application Programming Interfaces, or APIs.This deep and broad software stack accelerates the performance andeases the deployment of NVIDIA accelerated computing for computationally intensive workloads such as artificial intelligence, or AI, model training andinference, data analytics, scientific computing, and 3D graphics, with vertical-specific optimizations to address industries ranging from healthcare and telecom toautomotive and manufacturing.Our data-center-scale offerings are comprised of compute and networking solutions that can scale to tens of thousands of GPU-accelerated serversinterconnected to function as a single giant computer; this type of data center architecture and scale is needed for the development and deployment of modernAI applications.The GPU was initially used to simulate human imagination, enabling the virtual worlds of video games and films.Today, it also simulates human intelligence,enabling a deeper understanding of the physical world.Its parallel processing capabilities, supported by thousands of computing cores, are essential for deeplearning algorithms.This form of AI, in which software writes itself by learning from large amounts of data, can serve as the brain of computers, robots and selfdriving cars that can perceive and understand the world.GPU-powered AI solutions are being developed by thousands of enterprises to deliver services andproducts that would have been immensely difficult or even impossible with traditional coding.Examples include generative AI, which can create new contentsuch as text, code, images, audio, video, and molecule structures, and recommendation systems, which can recommend highly relevant content such asproducts, services, media or ads using deep neural networks trained on vast datasets that capture the user preferences.NVIDIA has a platform strategy, bringing together hardware, systems, software, algorithms, libraries, and services to create unique value for the markets weserve.While the computing requirements of these end markets are diverse, we address them with a unified underlying architecture leveraging our GPUs andnetworking and software stacks.The programmable nature of our architecture allows us to support several multi-billion-dollar end markets with the sameunderlying technology by using a variety of software stacks developed either internally or by third-party developers and partners.The large and growing numberof developers and installed base across our platforms strengthens our ecosystem and increases the value of our platform to our customers.Innovation is at our core.We have invested over $45.3 billion in research and development since our inception, yielding inventions that are essential to moderncomputing.Our invention of the GPU in 1999 sparked the growth of the PC gaming market and redefined computer graphics.With our introduction of the CUDAprogramming model in 2006, we opened the parallel processing capabilities of our GPU to a broad range of compute-intensive applications, paving the way forthe emergence of modern AI.", "page_start": 4, "page_end": 4, "section_title": "NVIDIA CORPORATION", "score": 0.5739196}, {"chunk_id": "823438dd-9ce8-4ae9-924a-70c6bbf5121f", "text": "Form 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38The following table sets forth, for the periods indicated, certain items in our Consolidated Statements of Income expressed as a percentage of revenue.Year EndedJan 28, 2024Jan 29, 2023Revenue100.0 %100.0 %Cost of revenue27.343.1Gross profit72.756.9Operating expensesResearch and development14.227.2Sales, general and administrative4.49.1Acquisition termination cost—5.0Total operating expenses18.641.3Operating income54.115.6Interest income1.41.0Interest expense(0.4)(1.0)0.4(0.1)Other, net1.4(0.1)Other income (expense), netIncome before income tax55.515.5Income tax expense (benefit)6.6(0.7)48.9 %16.2 %Net incomeReportable SegmentsRevenue by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$47,405$15,068$32,337215 %13,51711,9061,611Graphics14 %$60,922$26,974$33,948Total126 %Operating Income by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$32,016$5,083$26,933530 %Graphics5,8464,5521,29428 %(4,890)(5,411)521All Other(10)%$32,972$4,224$28,748Total681 %Compute & Networking revenue – The year-on-year increase was due to higher Data Center revenue.Compute grew 266% due to higher shipments of theNVIDIA Hopper GPU computing platform for the training and inference of LLMs, recommendation engines and generative AI applications.Networking was up133% due to higher shipments of InfiniBand.Graphics revenue – The year-on-year increase was led by growth in Gaming of 15% driven by higher sell-in to partners following the normalization of channelinventory levels.Reportable segment operating income – The year-on-year increase in Compute & Networking and Graphics operating income was driven by higher revenue.39All Other operating loss - The year-on-year decrease was due to the $1.4 billion Arm acquisition termination cost in fiscal year 2023, partially offset by a $839million increase in stock-based compensation expense in fiscal year 2024.Concentration of Revenue", "page_start": 38, "page_end": 40, "section_title": "NVIDIA CORPORATION", "score": 0.57207096}, {"chunk_id": "4f638c9d-79d6-4b84-9677-86448558b897", "text": "44,30115,35617,475Operating expensesResearch and development8,6757,3395,268Sales, general and administrative2,6542,4402,166—1,353—Acquisition termination cost11,32911,1327,434Total operating expensesOperating income32,9724,22410,041Interest income86626729Interest expense(257)(262)(236)237(48)107Other, net846(43)(100)Other income (expense), netIncome before income tax33,8184,1819,941Income tax expense (benefit)4,058(187)189$29,760$4,368$9,752Net incomeNet income per share:$12.05$1.76$3.91Basic$11.93$1.74$3.85DilutedWeighted average shares used in per share computation:2,4692,4872,496Basic2,4942,5072,535DilutedSee accompanying notes to the consolidated financial statements.50NVIDIA Corporation and SubsidiariesConsolidated Statements of Comprehensive Income(In millions)Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Net income$29,760$4,368$9,752Other comprehensive income (loss), net of taxAvailable-for-sale securities:Net change in unrealized gain (loss)80(31)(16)Reclassification adjustments for net realized gain included in net income—1—80(30)(16)Net change in unrealized gain (loss)Cash flow hedges:Net change in unrealized gain (loss)3847(43)(48)(49)29Reclassification adjustments for net realized gain (loss) included in net income(10)(2)(14)Net change in unrealized loss70(32)(30)Other comprehensive income (loss), net of tax$29,830$4,336$9,722Total comprehensive incomeSee accompanying notes to the consolidated financial statements.51NVIDIA Corporation and SubsidiariesConsolidated Balance Sheets(In millions, except par value)Jan 28, 2024Jan 29, 2023AssetsCurrent assets:Cash and cash equivalents$7,280$3,389Marketable securities18,7049,907Accounts receivable, net9,9993,827Inventories5,2825,1593,080791Prepaid expenses and other current assetsTotal current assets44,34523,073Property and equipment, net3,9143,807Operating lease assets1,3461,038Goodwill4,4304,372Intangible assets, net1,1121,676Deferred income tax assets6,0813,3964,5003,820Other assets$65,728$41,182Total assetsLiabilities and Shareholders' EquityCurrent liabilities:Accounts payable$2,699$1,193Accrued and other current liabilities6,682", "page_start": 50, "page_end": 52, "section_title": "NVIDIA CORPORATION", "score": 0.5700854}], "negative_attention_01": [{"chunk_id": "92be8a85-80ac-4327-a0b3-f2fff684bda4", "text": "the input sequence centered around the respective output position.This would increase the maximumpath length to O(n/r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k < n does not connect all pairs of input and outputpositions.Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest pathsbetween any two positions in the network.Convolutional layers are generally more expensive thanrecurrent layers, by a factor of k.Separable convolutions [6], however, decrease the complexityconsiderably, to O(k · n · d + n · d2).Even with k = n, however, the complexity of a separableconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,the approach we take in our model.As side benefit, self-attention could yield more interpretable models.We inspect attention distributionsfrom our models and present and discuss examples in the appendix.Not only do individual attentionheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntacticand semantic structure of the sentences.5TrainingThis section describes the training regime for our models.5.1Training Data and BatchingWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 millionsentence pairs.Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens.For English-French, we used the significantly larger WMT2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piecevocabulary [38].Sentence pairs were batched together by approximate sequence length.Each trainingbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000target tokens.5.2Hardware and ScheduleWe trained our models on one machine with 8 NVIDIA P100 GPUs.For our base models usingthe hyperparameters described throughout the paper, each training step took about 0.4 seconds.Wetrained the base models for a total of 100,000 steps or 12 hours.For our big models,(described on thebottom line of table 3), step time was 1.0 seconds.The big models were trained for 300,000 steps(3.5 days).5.3OptimizerWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9.We varied the learningrate over the course of training, according to the formula:lrate = d−0.5model · min(step_num−0.5, step_num · warmup_steps−1.5)(3)This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step number.We usedwarmup_steps = 4000.5.4RegularizationWe employ three types of regularization during training:7", "page_start": 7, "page_end": 7, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.343157}, {"chunk_id": "8d9a2784-c6bd-4675-a20b-80dd0948329f", "text": "25.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,keeping the amount of computation constant, as described in Section 3.2.2.While single-headattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality.Thissuggests that determining compatibility is not easy and that a more sophisticated compatibilityfunction than dot product may be beneficial.We further observe in rows (C) and (D) that, as expected,bigger models are better, and dropout is very helpful in avoiding over-fitting.In row (E) we replace oursinusoidal positional encoding with learned positional embeddings [9], and observe nearly identicalresults to the base model.6.3English Constituency ParsingTo evaluate if the Transformer can generalize to other tasks we performed experiments on Englishconstituency parsing.This task presents specific challenges: the output is subject to strong structuralconstraints and is significantly longer than the input.Furthermore, RNN sequence-to-sequencemodels have not been able to attain state-of-the-art results in small-data regimes [37].We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of thePenn Treebank [25], about 40K training sentences.We also trained it in a semi-supervised setting,using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences[37].We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokensfor the semi-supervised setting.We performed only a small number of experiments to select the dropout, both attention and residual(section 5.4), learning rates and beam size on the Section 22 development set, all other parametersremained unchanged from the English-to-German base translation model.During inference, we9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23of WSJ)ParserTrainingWSJ 23 F1Vinyals & Kaiser el al.(2014) [37]WSJ only, discriminative88.3Petrov et al.(2006) [29]WSJ only, discriminative90.4Zhu et al.(2013) [40]WSJ only, discriminative90.4Dyer et al.(2016) [8]WSJ only, discriminative91.7Transformer (4 layers)WSJ only, discriminative91.3Zhu et al.(2013) [40]semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised", "page_start": 9, "page_end": 10, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.3222505}, {"chunk_id": "1abdec62-7bd0-4192-b611-f148cee3a9b6", "text": "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, BowenZhou, and Yoshua Bengio.A structured self-attentive sentence embedding.arXiv preprintarXiv:1703.03130, 2017.[23] Minh-Thang Luong, Quoc V.Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.Multi-tasksequence to sequence learning.arXiv preprint arXiv:1511.06114, 2015.[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.Effective approaches to attentionbased neural machine translation.arXiv preprint arXiv:1508.04025, 2015.11[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.Building a large annotatedcorpus of english: The penn treebank.Computational linguistics, 19(2):313–330, 1993.[26] David McClosky, Eugene Charniak, and Mark Johnson.Effective self-training for parsing.InProceedings of the Human Language Technology Conference of the NAACL, Main Conference,pages 152–159.ACL, June 2006.[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit.A decomposable attentionmodel.In Empirical Methods in Natural Language Processing, 2016.[28] Romain Paulus, Caiming Xiong, and Richard Socher.A deep reinforced model for abstractivesummarization.arXiv preprint arXiv:1705.04304, 2017.[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.Learning accurate, compact,and interpretable tree annotation.In Proceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440.ACL, July2006.[30] Ofir Press and Lior Wolf.Using the output embedding to improve language models.arXivpreprint arXiv:1608.05859, 2016.[31] Rico Sennrich, Barry Haddow, and Alexandra Birch.Neural machine translation of rare wordswith subword units.arXiv preprint arXiv:1508.07909, 2015.[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,and Jeff Dean.Outrageously large neural networks: The sparsely-gated mixture-of-expertslayer.arXiv preprint arXiv:1701.06538, 2017.[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.Dropout: a simple way to prevent neural networks from overfitting.Journal of MachineLearning Research, 15(1):1929–1958, 2014.[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.End-to-end memorynetworks.In C.Cortes, N.D.Lawrence, D.D.Lee, M.Sugiyama, and R.Garnett, editors,Advances in Neural Information Processing Systems 28, pages 2440–2448.Curran Associates,Inc., 2015.[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.Sequence to sequence learning with neuralnetworks.In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.Rethinking the inception architecture for computer vision.CoRR, abs/1512.00567, 2015.[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton.Grammar as a foreign language.InAdvances in Neural Information Processing Systems, 2015.[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, WolfgangMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.Google’s neural machinetranslation system: Bridging the gap between human and machine translation.arXiv preprintarXiv:1609.08144, 2016.", "page_start": 11, "page_end": 12, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.30880353}, {"chunk_id": "0b373c99-8c0e-44bf-8fc7-f6b6b009590d", "text": "Provided proper attribution is provided, Google hereby grants permission toreproduce the tables and figures in this paper solely for use in journalistic orscholarly works.Ashish Vaswani∗Noam Shazeer∗Niki Parmar∗Jakob Uszkoreit∗Google BrainGoogle BrainGoogle ResearchGoogle Researchavaswani@google.comnoam@google.comnikip@google.comusz@google.comLlion Jones∗Aidan N.Gomez∗†Łukasz Kaiser∗Google ResearchUniversity of TorontoGoogle Brainlukaszkaiser@google.comllion@google.comaidan@cs.toronto.eduIllia Polosukhin∗‡illia.polosukhin@gmail.comAbstractThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder.The bestperforming models also connect the encoder and decoder through an attentionmechanism.We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely.Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring significantlyless time to train.Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU.On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.8 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature.We show that the Transformer generalizes well toother tasks by applying it successfully to English constituency parsing both withlarge and limited training data.∗Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and startedthe effort to evaluate this idea.Ashish, with Illia, designed and implemented the first Transformer models andhas been crucially involved in every aspect of this work.Noam proposed scaled dot-product attention, multi-headattention and the parameter-free position representation and became the other person involved in nearly everydetail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase andtensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, andefficient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of andimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks", "page_start": 1, "page_end": 2, "section_title": "", "score": 0.30061767}, {"chunk_id": "914cfb0b-3d4a-4988-8392-851b64c13e57", "text": "The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.Weused beam search with a beam size of 4 and length penalty α = 0.6 [38].These hyperparameterswere chosen after experimentation on the development set.We set the maximum output length duringinference to input length + 50, but terminate early when possible [38].Table 2 summarizes our results and compares our translation quality and training costs to other modelarchitectures from the literature.We estimate the number of floating point operations used to train amodel by multiplying the training time, the number of GPUs used, and an estimate of the sustainedsingle-precision floating-point capacity of each GPU 5.6.2Model VariationsTo evaluate the importance of different components of the Transformer, we varied our base modelin different ways, measuring the change in performance on English-to-German translation on the5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the basemodel.All metrics are on the English-to-German translation development set, newstest2013.Listedperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared toper-word perplexities.trainPPLBLEUparamsNdmodeldffhdkdvPdropϵls×106steps(dev)(dev)base65122048864640.10.1100K4.9225.86515125125.2924.941281285.0025.5(A)1632324.9125.83216165.0125.4165.1625.158(B)325.0125.46026.1123.73645.1925.35084.8825.580(C)25632325.7524.52810241281284.6626.016810245.1225.45340964.7526.2900.05.7724.60.24.9525.5(D)0.04.6725.30.25.4725.7(E)positional embedding instead of sinusoids4.9225.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.", "page_start": 8, "page_end": 9, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.29692876}, {"chunk_id": "222d0339-104c-4f96-bec5-f86a9511af81", "text": "[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V.Le.Massive exploration of neuralmachine translation architectures.CoRR, abs/1703.03906, 2017.[4] Jianpeng Cheng, Li Dong, and Mirella Lapata.Long short-term memory-networks for machinereading.arXiv preprint arXiv:1601.06733, 2016.10[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,and Yoshua Bengio.Learning phrase representations using rnn encoder-decoder for statisticalmachine translation.CoRR, abs/1406.1078, 2014.[6] Francois Chollet.Xception: Deep learning with depthwise separable convolutions.arXivpreprint arXiv:1610.02357, 2016.[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio.Empirical evaluationof gated recurrent neural networks on sequence modeling.CoRR, abs/1412.3555, 2014.[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A.Smith.Recurrent neuralnetwork grammars.In Proc.of NAACL, 2016.[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N.Dauphin.Convolutional sequence to sequence learning.arXiv preprint arXiv:1705.03122v2, 2017.arXiv preprint[10] Alex Graves.Generating sequences with recurrent neural networks.arXiv:1308.0850, 2013.[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition.In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pages 770–778, 2016.[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber.Gradient flow inrecurrent nets: the difficulty of learning long-term dependencies, 2001.[13] Sepp Hochreiter and Jürgen Schmidhuber.Long short-term memory.Neural computation,9(8):1735–1780, 1997.[14] Zhongqiang Huang and Mary Harper.Self-training PCFG grammars with latent annotationsacross languages.In Proceedings of the 2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 832–841.ACL, August 2009.[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.Exploringthe limits of language modeling.arXiv preprint arXiv:1602.02410, 2016.[16] Łukasz Kaiser and Samy Bengio.Can active memory replace attention?In Advances in NeuralInformation Processing Systems, (NIPS), 2016.[17] Łukasz Kaiser and Ilya Sutskever.Neural GPUs learn algorithms.In International Conferenceon Learning Representations (ICLR), 2016.[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu.Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,2017.[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M.Rush.Structured attention networks.In International Conference on Learning Representations, 2017.[20] Diederik Kingma and Jimmy Ba.Adam: A method for stochastic optimization.In ICLR, 2015.[21] Oleksii Kuchaiev and Boris Ginsburg.Factorization tricks for LSTM networks.arXiv preprintarXiv:1703.10722, 2017.[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, BowenZhou, and Yoshua Bengio.A structured self-attentive sentence embedding.arXiv preprintarXiv:1703.03130, 2017.[23] Minh-Thang Luong, Quoc V.Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.Multi-tasksequence to sequence learning.arXiv preprint arXiv:1511.", "page_start": 10, "page_end": 11, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.29512104}, {"chunk_id": "160dd6c9-8965-4dcd-8d74-e0dd157c4fd1", "text": "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step number.We usedwarmup_steps = 4000.5.4RegularizationWe employ three types of regularization during training:7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on theEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.BLEUTraining Cost (FLOPs)ModelEN-DEEN-FREN-DEEN-FRByteNet [18]23.751.0 · 1020Deep-Att + PosUnk [39]39.22.3 · 10191.4 · 1020GNMT + RL [38]24.639.929.6 · 10181.5 · 1020ConvS2S [9]25.1640.462.0 · 10191.2 · 1020MoE [32]26.0340.568.0 · 1020Deep-Att + PosUnk Ensemble [39]40.41.8 · 10201.1 · 1021GNMT + RL Ensemble [38]26.3041.167.7 · 10191.2 · 102141.29ConvS2S Ensemble [9]26.363.3 · 1018Transformer (base model)27.338.12.3 · 101928.441.8Transformer (big)Residual DropoutWe apply dropout [33] to the output of each sub-layer, before it is added to thesub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and thepositional encodings in both the encoder and decoder stacks.For the base model, we use a rate ofPdrop = 0.1.During training, we employed label smoothing of value ϵls = 0.1 [36].ThisLabel Smoothinghurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6Results6.1Machine TranslationOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0BLEU, establishing a new state-of-the-art BLEU score of 28.4.The configuration of this model islisted in the bottom line of Table 3.Training took 3.5 days on 8 P100 GPUs.Even our base modelsurpasses all previously published models and ensembles, at a fraction of the training cost of any ofthe competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,outperforming all of the previously published single models, at less than 1/4 the training cost of theprevious state-of-the-art model.The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We", "page_start": 7, "page_end": 8, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.29386362}, {"chunk_id": "bb77d3cc-59f9-4b97-a21b-132b30215259", "text": "semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised92.1Transformer (4 layers)semi-supervised92.7Luong et al.(2015) [23]multi-task93.0Dyer et al.(2016) [8]generative93.3increased the maximum output length to input length + 300.We used a beam size of 21 and α = 0.3for both WSJ only and the semi-supervised setting.Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of theRecurrent Neural Network Grammar [8].In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.7ConclusionIn this work, we presented the Transformer, the first sequence transduction model based entirely onattention, replacing the recurrent layers most commonly used in encoder-decoder architectures withmulti-headed self-attention.For translation tasks, the Transformer can be trained significantly faster than architectures basedon recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014English-to-French translation tasks, we achieve a new state of the art.In the former task our bestmodel outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.Weplan to extend the Transformer to problems involving input and output modalities other than text andto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputssuch as images, audio and video.Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.AcknowledgementsWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitfulcomments, corrections and inspiration.References[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprintarXiv:1607.06450, 2016.[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointlylearning to align and translate.CoRR, abs/1409.0473, 2014.[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V.Le.Massive exploration of neuralmachine translation architectures.CoRR, abs/1703.03906, 2017.[4] Jianpeng Cheng, Li Dong, and Mirella Lapata.Long short-term memory-networks for machinereading.arXiv preprint arXiv:1601.06733, 2016.10[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,", "page_start": 10, "page_end": 11, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.28706747}], "negative_attention_02": [{"chunk_id": "5151352b-ae46-44f3-83b0-8b8748547941", "text": "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networksin particular, have been firmly established as state of the art approaches in sequence modeling andtransduction problems such as language modeling and machine translation [35, 2, 5].Numerousefforts have since continued to push the boundaries of recurrent language models and encoder-decoderarchitectures [38, 24, 15].Recurrent models typically factor computation along the symbol positions of the input and outputsequences.Aligning the positions to steps in computation time, they generate a sequence of hiddenstates ht, as a function of the previous hidden state ht−1 and the input for position t.This inherentlysequential nature precludes parallelization within training examples, which becomes critical at longersequence lengths, as memory constraints limit batching across examples.Recent work has achievedsignificant improvements in computational efficiency through factorization tricks [21] and conditionalcomputation [32], while also improving model performance in case of the latter.The fundamentalconstraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance inthe input or output sequences [2, 19].In all but a few cases [27], however, such attention mechanismsare used in conjunction with a recurrent network.In this work we propose the Transformer, a model architecture eschewing recurrence and insteadrelying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for significantly more parallelization and can reach a new state of the art intranslation quality after being trained for as little as twelve hours on eight P100 GPUs.2BackgroundThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic buildingblock, computing hidden representations in parallel for all input and output positions.In these models,the number of operations required to relate signals from two arbitrary input or output positions growsin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.This makesit more difficult to learn dependencies between distant positions [12].In the Transformer this isreduced to a constant number of operations, albeit at the cost of reduced effective resolution dueto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,", "page_start": 1, "page_end": 2, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.5479867}, {"chunk_id": "0b373c99-8c0e-44bf-8fc7-f6b6b009590d", "text": "Provided proper attribution is provided, Google hereby grants permission toreproduce the tables and figures in this paper solely for use in journalistic orscholarly works.Ashish Vaswani∗Noam Shazeer∗Niki Parmar∗Jakob Uszkoreit∗Google BrainGoogle BrainGoogle ResearchGoogle Researchavaswani@google.comnoam@google.comnikip@google.comusz@google.comLlion Jones∗Aidan N.Gomez∗†Łukasz Kaiser∗Google ResearchUniversity of TorontoGoogle Brainlukaszkaiser@google.comllion@google.comaidan@cs.toronto.eduIllia Polosukhin∗‡illia.polosukhin@gmail.comAbstractThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder.The bestperforming models also connect the encoder and decoder through an attentionmechanism.We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely.Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring significantlyless time to train.Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU.On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.8 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature.We show that the Transformer generalizes well toother tasks by applying it successfully to English constituency parsing both withlarge and limited training data.∗Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and startedthe effort to evaluate this idea.Ashish, with Illia, designed and implemented the first Transformer models andhas been crucially involved in every aspect of this work.Noam proposed scaled dot-product attention, multi-headattention and the parameter-free position representation and became the other person involved in nearly everydetail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase andtensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, andefficient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of andimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks", "page_start": 1, "page_end": 2, "section_title": "", "score": 0.5139911}, {"chunk_id": "160dd6c9-8965-4dcd-8d74-e0dd157c4fd1", "text": "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step number.We usedwarmup_steps = 4000.5.4RegularizationWe employ three types of regularization during training:7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on theEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.BLEUTraining Cost (FLOPs)ModelEN-DEEN-FREN-DEEN-FRByteNet [18]23.751.0 · 1020Deep-Att + PosUnk [39]39.22.3 · 10191.4 · 1020GNMT + RL [38]24.639.929.6 · 10181.5 · 1020ConvS2S [9]25.1640.462.0 · 10191.2 · 1020MoE [32]26.0340.568.0 · 1020Deep-Att + PosUnk Ensemble [39]40.41.8 · 10201.1 · 1021GNMT + RL Ensemble [38]26.3041.167.7 · 10191.2 · 102141.29ConvS2S Ensemble [9]26.363.3 · 1018Transformer (base model)27.338.12.3 · 101928.441.8Transformer (big)Residual DropoutWe apply dropout [33] to the output of each sub-layer, before it is added to thesub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and thepositional encodings in both the encoder and decoder stacks.For the base model, we use a rate ofPdrop = 0.1.During training, we employed label smoothing of value ϵls = 0.1 [36].ThisLabel Smoothinghurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6Results6.1Machine TranslationOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0BLEU, establishing a new state-of-the-art BLEU score of 28.4.The configuration of this model islisted in the bottom line of Table 3.Training took 3.5 days on 8 P100 GPUs.Even our base modelsurpasses all previously published models and ensembles, at a fraction of the training cost of any ofthe competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,outperforming all of the previously published single models, at less than 1/4 the training cost of theprevious state-of-the-art model.The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We", "page_start": 7, "page_end": 8, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.5090487}, {"chunk_id": "914cfb0b-3d4a-4988-8392-851b64c13e57", "text": "The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.Weused beam search with a beam size of 4 and length penalty α = 0.6 [38].These hyperparameterswere chosen after experimentation on the development set.We set the maximum output length duringinference to input length + 50, but terminate early when possible [38].Table 2 summarizes our results and compares our translation quality and training costs to other modelarchitectures from the literature.We estimate the number of floating point operations used to train amodel by multiplying the training time, the number of GPUs used, and an estimate of the sustainedsingle-precision floating-point capacity of each GPU 5.6.2Model VariationsTo evaluate the importance of different components of the Transformer, we varied our base modelin different ways, measuring the change in performance on English-to-German translation on the5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the basemodel.All metrics are on the English-to-German translation development set, newstest2013.Listedperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared toper-word perplexities.trainPPLBLEUparamsNdmodeldffhdkdvPdropϵls×106steps(dev)(dev)base65122048864640.10.1100K4.9225.86515125125.2924.941281285.0025.5(A)1632324.9125.83216165.0125.4165.1625.158(B)325.0125.46026.1123.73645.1925.35084.8825.580(C)25632325.7524.52810241281284.6626.016810245.1225.45340964.7526.2900.05.7724.60.24.9525.5(D)0.04.6725.30.25.4725.7(E)positional embedding instead of sinusoids4.9225.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.", "page_start": 8, "page_end": 9, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.5084184}, {"chunk_id": "6ecb21e7-c073-4d90-9d8a-41fdfda06afb", "text": "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,textual entailment and learning task-independent sentence representations [4, 27, 28, 22].End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering andlanguage modeling tasks [34].To the best of our knowledge, however, the Transformer is the first transduction model relyingentirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.In the following sections, we will describe the Transformer, motivateself-attention and discuss its advantages over models such as [17, 18] and [9].3Model ArchitectureMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequenceof continuous representations z = (z1, ..., zn).Given z, the decoder then generates an outputsequence (y1, ..., ym) of symbols one element at a time.At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next.2Figure 1: The Transformer - model architecture.The Transformer follows this overall architecture using stacked self-attention and point-wise, fullyconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,respectively.3.1Encoder and Decoder StacksThe encoder is composed of a stack of N = 6 identical layers.Each layer has twoEncoder:sub-layers.The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network.We employ a residual connection [11] around each ofthe two sub-layers, followed by layer normalization [1].That is, the output of each sub-layer isLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layeritself.To facilitate these residual connections, all sub-layers in the model, as well as the embeddinglayers, produce outputs of dimension dmodel = 512.The decoder is also composed of a stack of N = 6 identical layers.In addition to the twoDecoder:sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-headattention over the output of the encoder stack.Similar to the encoder, we employ residual connectionsaround each of the sub-layers, followed by layer normalization.We also modify the self-attentionsub-layer in the decoder stack to prevent positions from attending to subsequent positions.Thismasking, combined with fact that the output embeddings are offset by one position, ensures that the", "page_start": 2, "page_end": 3, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.46284074}, {"chunk_id": "bb77d3cc-59f9-4b97-a21b-132b30215259", "text": "semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised92.1Transformer (4 layers)semi-supervised92.7Luong et al.(2015) [23]multi-task93.0Dyer et al.(2016) [8]generative93.3increased the maximum output length to input length + 300.We used a beam size of 21 and α = 0.3for both WSJ only and the semi-supervised setting.Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of theRecurrent Neural Network Grammar [8].In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.7ConclusionIn this work, we presented the Transformer, the first sequence transduction model based entirely onattention, replacing the recurrent layers most commonly used in encoder-decoder architectures withmulti-headed self-attention.For translation tasks, the Transformer can be trained significantly faster than architectures basedon recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014English-to-French translation tasks, we achieve a new state of the art.In the former task our bestmodel outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.Weplan to extend the Transformer to problems involving input and output modalities other than text andto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputssuch as images, audio and video.Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.AcknowledgementsWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitfulcomments, corrections and inspiration.References[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprintarXiv:1607.06450, 2016.[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointlylearning to align and translate.CoRR, abs/1409.0473, 2014.[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V.Le.Massive exploration of neuralmachine translation architectures.CoRR, abs/1703.03906, 2017.[4] Jianpeng Cheng, Li Dong, and Mirella Lapata.Long short-term memory-networks for machinereading.arXiv preprint arXiv:1601.06733, 2016.10[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,", "page_start": 10, "page_end": 11, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4427572}, {"chunk_id": "8d9a2784-c6bd-4675-a20b-80dd0948329f", "text": "25.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,keeping the amount of computation constant, as described in Section 3.2.2.While single-headattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality.Thissuggests that determining compatibility is not easy and that a more sophisticated compatibilityfunction than dot product may be beneficial.We further observe in rows (C) and (D) that, as expected,bigger models are better, and dropout is very helpful in avoiding over-fitting.In row (E) we replace oursinusoidal positional encoding with learned positional embeddings [9], and observe nearly identicalresults to the base model.6.3English Constituency ParsingTo evaluate if the Transformer can generalize to other tasks we performed experiments on Englishconstituency parsing.This task presents specific challenges: the output is subject to strong structuralconstraints and is significantly longer than the input.Furthermore, RNN sequence-to-sequencemodels have not been able to attain state-of-the-art results in small-data regimes [37].We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of thePenn Treebank [25], about 40K training sentences.We also trained it in a semi-supervised setting,using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences[37].We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokensfor the semi-supervised setting.We performed only a small number of experiments to select the dropout, both attention and residual(section 5.4), learning rates and beam size on the Section 22 development set, all other parametersremained unchanged from the English-to-German base translation model.During inference, we9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23of WSJ)ParserTrainingWSJ 23 F1Vinyals & Kaiser el al.(2014) [37]WSJ only, discriminative88.3Petrov et al.(2006) [29]WSJ only, discriminative90.4Zhu et al.(2013) [40]WSJ only, discriminative90.4Dyer et al.(2016) [8]WSJ only, discriminative91.7Transformer (4 layers)WSJ only, discriminative91.3Zhu et al.(2013) [40]semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised", "page_start": 9, "page_end": 10, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.41879356}, {"chunk_id": "92be8a85-80ac-4327-a0b3-f2fff684bda4", "text": "the input sequence centered around the respective output position.This would increase the maximumpath length to O(n/r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k < n does not connect all pairs of input and outputpositions.Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest pathsbetween any two positions in the network.Convolutional layers are generally more expensive thanrecurrent layers, by a factor of k.Separable convolutions [6], however, decrease the complexityconsiderably, to O(k · n · d + n · d2).Even with k = n, however, the complexity of a separableconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,the approach we take in our model.As side benefit, self-attention could yield more interpretable models.We inspect attention distributionsfrom our models and present and discuss examples in the appendix.Not only do individual attentionheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntacticand semantic structure of the sentences.5TrainingThis section describes the training regime for our models.5.1Training Data and BatchingWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 millionsentence pairs.Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens.For English-French, we used the significantly larger WMT2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piecevocabulary [38].Sentence pairs were batched together by approximate sequence length.Each trainingbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000target tokens.5.2Hardware and ScheduleWe trained our models on one machine with 8 NVIDIA P100 GPUs.For our base models usingthe hyperparameters described throughout the paper, each training step took about 0.4 seconds.Wetrained the base models for a total of 100,000 steps or 12 hours.For our big models,(described on thebottom line of table 3), step time was 1.0 seconds.The big models were trained for 300,000 steps(3.5 days).5.3OptimizerWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9.We varied the learningrate over the course of training, according to the formula:lrate = d−0.5model · min(step_num−0.5, step_num · warmup_steps−1.5)(3)This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step number.We usedwarmup_steps = 4000.5.4RegularizationWe employ three types of regularization during training:7", "page_start": 7, "page_end": 7, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.41212302}], "negative_nda_01": [{"chunk_id": "94d954fb-950b-4dcf-8049-c72deb8243c9", "text": "Date: 201[ ]Parties:[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]and[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]1.Each of the parties to this Agreement intends to disclose information (the ConfidentialInformation) to the other party for the purpose of [insert details e.g.discussing thepossibility of the parties entering into a joint venture] (the Purpose).2.Each party to this Agreement is referred to as ‘the Recipient’ when it receives or usesthe Confidential Information disclosed by the other party.3.The Recipient undertakes not to use the Confidential Information disclosed by the otherparty for any purpose except the Purpose, without first obtaining the written agreementof the other party.4.The Recipient undertakes to keep the Confidential Information disclosed by the otherparty secure and not to disclose it to any third party [except to its employees [andprofessional advisers] who need to know the same for the Purpose, who know theyowe a duty of confidence to the other party and who are bound by obligationsequivalent to those in clause 3 above and this clause 4.5.The undertakings in clauses 3 and 4 above apply to all of the information disclosed byeach of the parties to the other, regardless of the way or form in which it is disclosed orrecorded but they do not apply to:a) any information which is or in future comes into the public domain (unless as a result ofthe breach of this Agreement); orb) any information which is already known to the Recipient and which was not subject toany obligation of confidence before it was disclosed to the Recipient by the other party.6.Nothing in this Agreement will prevent the Recipient from making any disclosure of theConfidential Information required by law or by any competent authority.7.The Recipient will, on request from the other party, return all copies and records of theConfidential Information disclosed by the other party to the Recipient and will not retainany copies or records of the Confidential Information disclosed by the other party.8.Neither this Agreement nor the supply of any information grants the Recipient anylicence, interest or right in respect of any intellectual property rights of the other partyexcept the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.", "page_start": 1, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.41675818}, {"chunk_id": "6c64d139-fd3d-400d-b64b-3ddf0dea3473", "text": "except the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.The English Courts will have non-exclusive jurisdiction to deal with any dispute whichhas arisen or may arise out of, or in connection with, this Agreement.Signed [by [insert name]] OR [on behalf of][insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________PositionSigned [by [insert name]] OR [on behalf of] [insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________Position", "page_start": 2, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.39659417}], "negative_nda_02": [{"chunk_id": "94d954fb-950b-4dcf-8049-c72deb8243c9", "text": "Date: 201[ ]Parties:[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]and[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]1.Each of the parties to this Agreement intends to disclose information (the ConfidentialInformation) to the other party for the purpose of [insert details e.g.discussing thepossibility of the parties entering into a joint venture] (the Purpose).2.Each party to this Agreement is referred to as ‘the Recipient’ when it receives or usesthe Confidential Information disclosed by the other party.3.The Recipient undertakes not to use the Confidential Information disclosed by the otherparty for any purpose except the Purpose, without first obtaining the written agreementof the other party.4.The Recipient undertakes to keep the Confidential Information disclosed by the otherparty secure and not to disclose it to any third party [except to its employees [andprofessional advisers] who need to know the same for the Purpose, who know theyowe a duty of confidence to the other party and who are bound by obligationsequivalent to those in clause 3 above and this clause 4.5.The undertakings in clauses 3 and 4 above apply to all of the information disclosed byeach of the parties to the other, regardless of the way or form in which it is disclosed orrecorded but they do not apply to:a) any information which is or in future comes into the public domain (unless as a result ofthe breach of this Agreement); orb) any information which is already known to the Recipient and which was not subject toany obligation of confidence before it was disclosed to the Recipient by the other party.6.Nothing in this Agreement will prevent the Recipient from making any disclosure of theConfidential Information required by law or by any competent authority.7.The Recipient will, on request from the other party, return all copies and records of theConfidential Information disclosed by the other party to the Recipient and will not retainany copies or records of the Confidential Information disclosed by the other party.8.Neither this Agreement nor the supply of any information grants the Recipient anylicence, interest or right in respect of any intellectual property rights of the other partyexcept the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.", "page_start": 1, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.43780625}, {"chunk_id": "6c64d139-fd3d-400d-b64b-3ddf0dea3473", "text": "except the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.The English Courts will have non-exclusive jurisdiction to deal with any dispute whichhas arisen or may arise out of, or in connection with, this Agreement.Signed [by [insert name]] OR [on behalf of][insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________PositionSigned [by [insert name]] OR [on behalf of] [insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________Position", "page_start": 2, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.39407843}], "multilingual_nvidia_zh": [{"chunk_id": "823438dd-9ce8-4ae9-924a-70c6bbf5121f", "text": "Form 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38The following table sets forth, for the periods indicated, certain items in our Consolidated Statements of Income expressed as a percentage of revenue.Year EndedJan 28, 2024Jan 29, 2023Revenue100.0 %100.0 %Cost of revenue27.343.1Gross profit72.756.9Operating expensesResearch and development14.227.2Sales, general and administrative4.49.1Acquisition termination cost—5.0Total operating expenses18.641.3Operating income54.115.6Interest income1.41.0Interest expense(0.4)(1.0)0.4(0.1)Other, net1.4(0.1)Other income (expense), netIncome before income tax55.515.5Income tax expense (benefit)6.6(0.7)48.9 %16.2 %Net incomeReportable SegmentsRevenue by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$47,405$15,068$32,337215 %13,51711,9061,611Graphics14 %$60,922$26,974$33,948Total126 %Operating Income by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$32,016$5,083$26,933530 %Graphics5,8464,5521,29428 %(4,890)(5,411)521All Other(10)%$32,972$4,224$28,748Total681 %Compute & Networking revenue – The year-on-year increase was due to higher Data Center revenue.Compute grew 266% due to higher shipments of theNVIDIA Hopper GPU computing platform for the training and inference of LLMs, recommendation engines and generative AI applications.Networking was up133% due to higher shipments of InfiniBand.Graphics revenue – The year-on-year increase was led by growth in Gaming of 15% driven by higher sell-in to partners following the normalization of channelinventory levels.Reportable segment operating income – The year-on-year increase in Compute & Networking and Graphics operating income was driven by higher revenue.39All Other operating loss - The year-on-year decrease was due to the $1.4 billion Arm acquisition termination cost in fiscal year 2023, partially offset by a $839million increase in stock-based compensation expense in fiscal year 2024.Concentration of Revenue", "page_start": 38, "page_end": 40, "section_title": "NVIDIA CORPORATION", "score": 0.6508163}, {"chunk_id": "ddc05d6e-79f6-4ed4-a29b-876d9e76fd48", "text": "We did not experience any significant impact or expense toour business; however, if the conflict is further extended, it could impact future product development, operations, and revenue or create other uncertainty for ourbusiness.35Fiscal Year 2024 SummaryYear EndedJan 28, 2024Jan 29, 2023Change($ in millions, except per share data)Revenue$60,922$26,974Up 126%Gross margin72.7 %56.9 %Up 15.8 ptsOperating expenses$11,329$11,132Up 2%Operating income$32,972$4,224Up 681%Net income$29,760$4,368Up 581%Net income per diluted share$11.93$1.74Up 586%We specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Revenue for fiscal year 2024 was $60.9 billion, up 126% from a year ago.Data Center revenue for fiscal year 2024 was up 217%.Strong demand was driven by enterprise software and consumer internet applications, and multipleindustry verticals including automotive, financial services, and healthcare.Customers across industry verticals access NVIDIA AI infrastructure both through thecloud and on-premises.Data Center compute revenue was up 244% in the fiscal year.Networking revenue was up 133% in the fiscal year.Gaming revenue for fiscal year 2024 was up 15%.The increase reflects higher sell-in to partners following the normalization of channel inventory levels andgrowing demand.Professional Visualization revenue for fiscal year 2024 was up 1%.Automotive revenue for the fiscal year 2024 was up 21%.The increase primarily reflected growth in self-driving platforms.Gross margin increased in fiscal year 2024, primarily driven by Data Center revenue growth and lower net inventory provisions as a percentage of revenue.Operating expenses increased for fiscal year 2024, driven by growth in employees and compensation increases.Fiscal year 2023 also included a $1.4 billionacquisition termination charge related to the proposed Arm transaction.Market Platform HighlightsData Center revenue for fiscal year 2024 was $47.5 billion, up 217% from fiscal year 2023.In Data Center, we launched AI inference platforms that combine ourfull-stack inference software with NVIDIA Ada, NVIDIA Hopper and NVIDIA Grace Hopper processors optimized for generative AI, LLMs and other AI workloads.We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.", "page_start": 35, "page_end": 36, "section_title": "NVIDIA CORPORATION", "score": 0.6320236}, {"chunk_id": "f2207e14-4c6c-421b-849b-76359737cca1", "text": "Germany, India, Israel, and Taiwan for fiscal years 2005 through 2023.Note 15 - Shareholders’ EquityCapital Return ProgramIn August 2023, our Board of Directors approved an increase to our share repurchase program of an additional $25.0 billion, without expiration.During fiscalyear 2024, we repurchased 21 million shares of our common stock for $9.7 billion.As of January 28, 2024, we were authorized, subject to certain specifications,to repurchase additional shares of our common stock up to $22.5 billion.From January 29, 2024 through February 16, 2024, we repurchased 2.8 million sharesfor $1.9 billion pursuant to a Rule 10b5-1 trading plan.Our share repurchase program aims to offset dilution from shares issued to employees.We may pursueadditional share repurchases as we weigh market factors and other investment opportunities.During fiscal years 2024, 2023, and 2022, we paid $395 million, $398 million, and $399 million in cash dividends to our shareholders, respectively.Our cashdividend program and the payment of future cash dividends under that program are subject to our Board of Directors' continuing determination that the dividendprogram and the declaration of dividends thereunder are in the best interests of our shareholders.In fiscal year 2022, we retired our existing 349 million treasury shares.These shares assumed the status of authorized and unissued shares upon retirement.The excess of repurchase price over par value was allocated between additional paid-in capital and retained earnings, resulting in a reduction in additional paidin capital by $20 million and retained earnings by $12.0 billion.Any future repurchased shares will assume the status of authorized and unissued shares.77NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Note 16 - Employee Retirement PlansWe provide tax-qualified defined contribution plans to eligible employees in the U.S.and certain other countries.Our contribution expense for fiscal years 2024,2023, and 2022 was $255 million, $227 million, and $168 million, respectively.Note 17 - Segment InformationOur Chief Executive Officer, who is considered to be our chief operating decision maker, or CODM, reviews financial information presented on an operatingsegment basis for purposes of making decisions and assessing financial performance.The Compute & Networking segment includes our Data Center accelerated computing platform; networking; automotive artificial intelligence, or AI, Cockpit,autonomous driving development agreements, and autonomous vehicle solutions; electric vehicle computing platforms; Jetson for robotics and other embeddedplatforms; NVIDIA AI Enterprise and other software; and DGX Cloud.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions forgaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across our", "page_start": 77, "page_end": 78, "section_title": "NVIDIA CORPORATION", "score": 0.60024536}, {"chunk_id": "4f638c9d-79d6-4b84-9677-86448558b897", "text": "44,30115,35617,475Operating expensesResearch and development8,6757,3395,268Sales, general and administrative2,6542,4402,166—1,353—Acquisition termination cost11,32911,1327,434Total operating expensesOperating income32,9724,22410,041Interest income86626729Interest expense(257)(262)(236)237(48)107Other, net846(43)(100)Other income (expense), netIncome before income tax33,8184,1819,941Income tax expense (benefit)4,058(187)189$29,760$4,368$9,752Net incomeNet income per share:$12.05$1.76$3.91Basic$11.93$1.74$3.85DilutedWeighted average shares used in per share computation:2,4692,4872,496Basic2,4942,5072,535DilutedSee accompanying notes to the consolidated financial statements.50NVIDIA Corporation and SubsidiariesConsolidated Statements of Comprehensive Income(In millions)Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Net income$29,760$4,368$9,752Other comprehensive income (loss), net of taxAvailable-for-sale securities:Net change in unrealized gain (loss)80(31)(16)Reclassification adjustments for net realized gain included in net income—1—80(30)(16)Net change in unrealized gain (loss)Cash flow hedges:Net change in unrealized gain (loss)3847(43)(48)(49)29Reclassification adjustments for net realized gain (loss) included in net income(10)(2)(14)Net change in unrealized loss70(32)(30)Other comprehensive income (loss), net of tax$29,830$4,336$9,722Total comprehensive incomeSee accompanying notes to the consolidated financial statements.51NVIDIA Corporation and SubsidiariesConsolidated Balance Sheets(In millions, except par value)Jan 28, 2024Jan 29, 2023AssetsCurrent assets:Cash and cash equivalents$7,280$3,389Marketable securities18,7049,907Accounts receivable, net9,9993,827Inventories5,2825,1593,080791Prepaid expenses and other current assetsTotal current assets44,34523,073Property and equipment, net3,9143,807Operating lease assets1,3461,038Goodwill4,4304,372Intangible assets, net1,1121,676Deferred income tax assets6,0813,3964,5003,820Other assets$65,728$41,182Total assetsLiabilities and Shareholders' EquityCurrent liabilities:Accounts payable$2,699$1,193Accrued and other current liabilities6,682", "page_start": 50, "page_end": 52, "section_title": "NVIDIA CORPORATION", "score": 0.5888245}, {"chunk_id": "8d7f8306-8d55-40f5-b383-f4914775e7ee", "text": "(10)Restructuring costs and other—(54)—Acquisition termination cost—(1,353)—10(2)—Other$(4,890)$(5,411)$(3,049)TotalRevenue by geographic areas is designated based upon the billing location of the customer.End customer location may be different than our customer’s billinglocation.Revenue by geographic areas was as follows:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Revenue:(In millions)United States$26,966$8,292$4,349Taiwan13,4056,9868,544China (including Hong Kong)10,3065,7857,11110,2455,9116,910Other countries$60,922$26,974$26,914Total revenueRevenue from sales to customers outside of the United States accounted for 56%, 69%, and 84% of total revenue for fiscal years 2024, 2023, and 2022,respectively.The increase in revenue to the United States for fiscal year 2024 was primarily due to higher U.S.-based Compute & Networking segment demand.Sales to one customer represented 13% of total revenue for fiscal year 2024, which was attributable to the Compute & Networking segment.No customerrepresented 10% or more of total revenue for fiscal years 2023 and 2022.The following table summarizes information pertaining to our revenue by each of the specialized markets we serve:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Revenue:(In millions)Data Center$47,525$15,005$10,613Gaming10,4479,06712,462Professional Visualization1,5531,5442,111Automotive1,0919035663064551,162OEM and Other$60,922$26,974$26,914Total revenue79NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)The following table presents summarized information for long-lived assets by country.Long-lived assets consist of property and equipment and exclude otherassets, operating lease assets, goodwill, and intangible assets.Jan 28, 2024Jan 29, 2023Long-lived assets:(In millions)United States$2,595$2,587Taiwan773702Israel325283Other countries221235$3,914$3,807Total long-lived assets80NVIDIA Corporation and SubsidiariesSchedule II – Valuation and Qualifying AccountsBalance atBeginning ofBalance atDescriptionPeriodAdditionsDeductionsEnd of Period(In millions)Fiscal year 2024Allowance for doubtful accounts$4$— (1)$— (1)$4Sales return allowance$26$213 (2)$", "page_start": 79, "page_end": 81, "section_title": "NVIDIA CORPORATION", "score": 0.5870222}, {"chunk_id": "c69190e8-be56-40d7-9e3f-a1b1a8224354", "text": "Estimates for customer program accrualsinclude a combination of historical attainment and claim rates and may be adjusted based on relevant internal and external factors.License and Development ArrangementsRevenue from License and Development Arrangements is recognized over the period in which the development services are performed.Each fiscal reportingperiod, we measure progress to completion based on actual cost incurred to date as a percentage of the estimated total cost required to complete each project.Estimated total cost for each project includes a forecast of internal engineer personnel time expected to be incurred and other third-party costs as applicable.Contracts with Multiple Performance ObligationsOur contracts may contain more than one performance obligation.Judgement is required in determining whether each performance obligation within a customercontract is distinct.Except for License and Development Arrangements, NVIDIA products and services function on a standalone basis and do not require asignificant amount of integration or interdependency.Therefore, multiple performance obligations contained within a customer contract are considered distinctand are not combined for revenue recognition purposes.We allocate the total transaction price to each distinct performance obligation in a multiple performance obligations arrangement on a relative standalone sellingprice basis.In certain cases, we can establish standalone selling price based on directly observable prices of products or services sold separately in comparablecircumstances to similar customers.If standalone selling price is not directly observable, such as when we do not sell a product or service separately, wedetermine standalone selling price based on market data and other observable inputs.Change in Accounting EstimateIn February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of a majority of the server, storage, and network equipment from three years to a range of four to five years, and assembly and testequipment from five years to seven years.The estimated effect of this change for fiscal year 2024 was a benefit of $33 million and $102 million for cost ofrevenue and operating expenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or$0.05 per both basic and diluted share.Results of OperationsA discussion regarding our financial condition and results of operations for fiscal year 2024 compared to fiscal year 2023 is presented below.A discussionregarding our financial condition and results of operations for fiscal year 2023 compared to fiscal year 2022 can be found under Item 7 in our Annual Report onForm 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38", "page_start": 38, "page_end": 38, "section_title": "NVIDIA CORPORATION", "score": 0.58214337}, {"chunk_id": "0dd7fcd2-abcb-4469-bfdc-d6d5c6cb8eae", "text": "*$100 invested on 1/27/19 in stock and in indices, including reinvestment of dividends.Source: FactSet financial data and analytics.1/27/20191/26/20201/31/20211/30/20221/29/20231/28/2024NVIDIA Corporation$100.00$157.02$326.26$574.15$512.40$1,536.28S&P 500$100.00$126.17$144.83$175.25$163.63$199.83Nasdaq 100$100.00$136.15$194.20$218.68$185.67$268.13Item 6.[Reserved]33Item 7.Management's Discussion and Analysis of Financial Condition and Results of OperationsThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with “Item 1A.Risk Factors”, ourConsolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described elsewhere in this Annual Report onForm 10-K, before deciding to purchase, hold or sell shares of our common stock.OverviewOur Company and Our BusinessesNVIDIA pioneered accelerated computing to help solve the most challenging computational problems.Since our original focus on PC graphics, we haveexpanded to several other large and important computationally intensive fields.NVIDIA has leveraged its GPU architecture to create platforms for acceleratedcomputing, AI solutions, scientific computing, data science, AV, robotics, metaverse and 3D internet applications.Our two operating segments are \"Compute & Networking\" and \"Graphics.\" Refer to Note 17 of the Notes to the Consolidated Financial Statements in Part IV,Item 15 of this Annual Report on Form 10-K for additional information.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Recent Developments, Future Objectives and ChallengesDemand and Supply, Product Transitions, and New Products and Business ModelsDemand for our data center systems and products surged in fiscal year 2024.Entering fiscal year 2025, we are gathering customer demand indications acrossseveral product transitions.We have demand visibility for our new data center products ramping later in fiscal year 2025.We have increased our supply andcapacity purchases with existing suppliers, added new vendors and entered into prepaid manufacturing and capacity agreements.These increased purchasevolumes, the number of suppliers, and the integration of new vendors into our supply chain may create more complexity and execution risk.Our purchasecommitments and obligations for inventory and manufacturing capacity at the end of fiscal year 2024 were impacted by shortening lead times for certaincomponents.We may continue to enter into new supplier and capacity arrangements.Supply of Hopper architecture products is improving, and demand remainsvery strong.We expect our next-generation products to be supply-constrained based upon demand indications.", "page_start": 33, "page_end": 34, "section_title": "NVIDIA CORPORATION", "score": 0.580955}, {"chunk_id": "9f7fc279-f10b-4a78-a0be-9566e7368d5c", "text": "programs, and can be made in one or more larger repurchases, in compliance with Rule 10b-18 of the Exchange Act, subject to market conditions, applicablelegal requirements, and other factors.The program does not obligate NVIDIA to acquire any particular amount of common stock and the program may besuspended at any time at our discretion.In fiscal year 2024, we paid $395 million in quarterly cash dividends.Our cash dividend program and the payment of future cash dividends under that programare subject to our Board of Directors' continuing determination that the dividend program and the declaration of dividends thereunder are in the best interests ofour shareholders.The following table presents details of our share repurchase transactions during the fourth quarter of fiscal year 2024:Total NumberTotal Number of SharesApproximate Dollar Valueof SharesPurchased as Part ofof Shares that May Yet BePurchasedAverage Price PaidPublicly AnnouncedPurchased Under thePeriod(In millions)per ShareProgram (In millions)Program (In billions)October 30, 2023 - November 26, 20230.9$464.390.9$24.8November 27, 2023 - December 24, 20231.1$477.261.1$24.33.33.3December 25, 2023 - January 28, 2024$540.85$22.55.35.3TotalFrom January 29, 2024 to February 16, 2024, we repurchased 2.8 million shares for $1.9 billion pursuant to a Rule 10b5-1 trading plan.Restricted Stock Unit Share WithholdingWe withhold common stock shares associated with net share settlements to cover tax withholding obligations upon the vesting of RSU awards under ouremployee equity incentive program.During fiscal year 2024, we withheld32approximately 7 million shares for a total value of $2.8 billion through net share settlements.Refer to Note 4 of the Notes to the Consolidated FinancialStatements in Part IV, Item 15 of this Annual Report on Form 10-K for further discussion regarding our equity incentive plans.Stock Performance GraphsThe following graph compares the cumulative total shareholder return for our common stock, the S&P 500 Index, and the Nasdaq 100 Index for the five yearsended January 28, 2024.The graph assumes that $100 was invested on January 27, 2019 in our common stock and in each of the S&P 500 Index and theNasdaq 100 Index.Our common stock is a component of each of the presented indices.Total return assumes reinvestment of dividends in each of the indicesindicated.Total return is based on historical results and is not intended to indicate future performance.*$100 invested on 1/27/19 in stock and in indices, including reinvestment of dividends.Source: FactSet financial data and analytics.1/27/20191/26/20201/31/20211/30/20221/29/20231/28/2024NVIDIA Corporation$100.00$", "page_start": 32, "page_end": 33, "section_title": "NVIDIA CORPORATION", "score": 0.5730454}], "multilingual_nvidia_es": [{"chunk_id": "823438dd-9ce8-4ae9-924a-70c6bbf5121f", "text": "Form 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38The following table sets forth, for the periods indicated, certain items in our Consolidated Statements of Income expressed as a percentage of revenue.Year EndedJan 28, 2024Jan 29, 2023Revenue100.0 %100.0 %Cost of revenue27.343.1Gross profit72.756.9Operating expensesResearch and development14.227.2Sales, general and administrative4.49.1Acquisition termination cost—5.0Total operating expenses18.641.3Operating income54.115.6Interest income1.41.0Interest expense(0.4)(1.0)0.4(0.1)Other, net1.4(0.1)Other income (expense), netIncome before income tax55.515.5Income tax expense (benefit)6.6(0.7)48.9 %16.2 %Net incomeReportable SegmentsRevenue by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$47,405$15,068$32,337215 %13,51711,9061,611Graphics14 %$60,922$26,974$33,948Total126 %Operating Income by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$32,016$5,083$26,933530 %Graphics5,8464,5521,29428 %(4,890)(5,411)521All Other(10)%$32,972$4,224$28,748Total681 %Compute & Networking revenue – The year-on-year increase was due to higher Data Center revenue.Compute grew 266% due to higher shipments of theNVIDIA Hopper GPU computing platform for the training and inference of LLMs, recommendation engines and generative AI applications.Networking was up133% due to higher shipments of InfiniBand.Graphics revenue – The year-on-year increase was led by growth in Gaming of 15% driven by higher sell-in to partners following the normalization of channelinventory levels.Reportable segment operating income – The year-on-year increase in Compute & Networking and Graphics operating income was driven by higher revenue.39All Other operating loss - The year-on-year decrease was due to the $1.4 billion Arm acquisition termination cost in fiscal year 2023, partially offset by a $839million increase in stock-based compensation expense in fiscal year 2024.Concentration of Revenue", "page_start": 38, "page_end": 40, "section_title": "NVIDIA CORPORATION", "score": 0.7292737}, {"chunk_id": "ddc05d6e-79f6-4ed4-a29b-876d9e76fd48", "text": "We did not experience any significant impact or expense toour business; however, if the conflict is further extended, it could impact future product development, operations, and revenue or create other uncertainty for ourbusiness.35Fiscal Year 2024 SummaryYear EndedJan 28, 2024Jan 29, 2023Change($ in millions, except per share data)Revenue$60,922$26,974Up 126%Gross margin72.7 %56.9 %Up 15.8 ptsOperating expenses$11,329$11,132Up 2%Operating income$32,972$4,224Up 681%Net income$29,760$4,368Up 581%Net income per diluted share$11.93$1.74Up 586%We specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Revenue for fiscal year 2024 was $60.9 billion, up 126% from a year ago.Data Center revenue for fiscal year 2024 was up 217%.Strong demand was driven by enterprise software and consumer internet applications, and multipleindustry verticals including automotive, financial services, and healthcare.Customers across industry verticals access NVIDIA AI infrastructure both through thecloud and on-premises.Data Center compute revenue was up 244% in the fiscal year.Networking revenue was up 133% in the fiscal year.Gaming revenue for fiscal year 2024 was up 15%.The increase reflects higher sell-in to partners following the normalization of channel inventory levels andgrowing demand.Professional Visualization revenue for fiscal year 2024 was up 1%.Automotive revenue for the fiscal year 2024 was up 21%.The increase primarily reflected growth in self-driving platforms.Gross margin increased in fiscal year 2024, primarily driven by Data Center revenue growth and lower net inventory provisions as a percentage of revenue.Operating expenses increased for fiscal year 2024, driven by growth in employees and compensation increases.Fiscal year 2023 also included a $1.4 billionacquisition termination charge related to the proposed Arm transaction.Market Platform HighlightsData Center revenue for fiscal year 2024 was $47.5 billion, up 217% from fiscal year 2023.In Data Center, we launched AI inference platforms that combine ourfull-stack inference software with NVIDIA Ada, NVIDIA Hopper and NVIDIA Grace Hopper processors optimized for generative AI, LLMs and other AI workloads.We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.", "page_start": 35, "page_end": 36, "section_title": "NVIDIA CORPORATION", "score": 0.7168038}, {"chunk_id": "0dd7fcd2-abcb-4469-bfdc-d6d5c6cb8eae", "text": "*$100 invested on 1/27/19 in stock and in indices, including reinvestment of dividends.Source: FactSet financial data and analytics.1/27/20191/26/20201/31/20211/30/20221/29/20231/28/2024NVIDIA Corporation$100.00$157.02$326.26$574.15$512.40$1,536.28S&P 500$100.00$126.17$144.83$175.25$163.63$199.83Nasdaq 100$100.00$136.15$194.20$218.68$185.67$268.13Item 6.[Reserved]33Item 7.Management's Discussion and Analysis of Financial Condition and Results of OperationsThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with “Item 1A.Risk Factors”, ourConsolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described elsewhere in this Annual Report onForm 10-K, before deciding to purchase, hold or sell shares of our common stock.OverviewOur Company and Our BusinessesNVIDIA pioneered accelerated computing to help solve the most challenging computational problems.Since our original focus on PC graphics, we haveexpanded to several other large and important computationally intensive fields.NVIDIA has leveraged its GPU architecture to create platforms for acceleratedcomputing, AI solutions, scientific computing, data science, AV, robotics, metaverse and 3D internet applications.Our two operating segments are \"Compute & Networking\" and \"Graphics.\" Refer to Note 17 of the Notes to the Consolidated Financial Statements in Part IV,Item 15 of this Annual Report on Form 10-K for additional information.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Recent Developments, Future Objectives and ChallengesDemand and Supply, Product Transitions, and New Products and Business ModelsDemand for our data center systems and products surged in fiscal year 2024.Entering fiscal year 2025, we are gathering customer demand indications acrossseveral product transitions.We have demand visibility for our new data center products ramping later in fiscal year 2025.We have increased our supply andcapacity purchases with existing suppliers, added new vendors and entered into prepaid manufacturing and capacity agreements.These increased purchasevolumes, the number of suppliers, and the integration of new vendors into our supply chain may create more complexity and execution risk.Our purchasecommitments and obligations for inventory and manufacturing capacity at the end of fiscal year 2024 were impacted by shortening lead times for certaincomponents.We may continue to enter into new supplier and capacity arrangements.Supply of Hopper architecture products is improving, and demand remainsvery strong.We expect our next-generation products to be supply-constrained based upon demand indications.", "page_start": 33, "page_end": 34, "section_title": "NVIDIA CORPORATION", "score": 0.67670184}, {"chunk_id": "f2207e14-4c6c-421b-849b-76359737cca1", "text": "Germany, India, Israel, and Taiwan for fiscal years 2005 through 2023.Note 15 - Shareholders’ EquityCapital Return ProgramIn August 2023, our Board of Directors approved an increase to our share repurchase program of an additional $25.0 billion, without expiration.During fiscalyear 2024, we repurchased 21 million shares of our common stock for $9.7 billion.As of January 28, 2024, we were authorized, subject to certain specifications,to repurchase additional shares of our common stock up to $22.5 billion.From January 29, 2024 through February 16, 2024, we repurchased 2.8 million sharesfor $1.9 billion pursuant to a Rule 10b5-1 trading plan.Our share repurchase program aims to offset dilution from shares issued to employees.We may pursueadditional share repurchases as we weigh market factors and other investment opportunities.During fiscal years 2024, 2023, and 2022, we paid $395 million, $398 million, and $399 million in cash dividends to our shareholders, respectively.Our cashdividend program and the payment of future cash dividends under that program are subject to our Board of Directors' continuing determination that the dividendprogram and the declaration of dividends thereunder are in the best interests of our shareholders.In fiscal year 2022, we retired our existing 349 million treasury shares.These shares assumed the status of authorized and unissued shares upon retirement.The excess of repurchase price over par value was allocated between additional paid-in capital and retained earnings, resulting in a reduction in additional paidin capital by $20 million and retained earnings by $12.0 billion.Any future repurchased shares will assume the status of authorized and unissued shares.77NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Note 16 - Employee Retirement PlansWe provide tax-qualified defined contribution plans to eligible employees in the U.S.and certain other countries.Our contribution expense for fiscal years 2024,2023, and 2022 was $255 million, $227 million, and $168 million, respectively.Note 17 - Segment InformationOur Chief Executive Officer, who is considered to be our chief operating decision maker, or CODM, reviews financial information presented on an operatingsegment basis for purposes of making decisions and assessing financial performance.The Compute & Networking segment includes our Data Center accelerated computing platform; networking; automotive artificial intelligence, or AI, Cockpit,autonomous driving development agreements, and autonomous vehicle solutions; electric vehicle computing platforms; Jetson for robotics and other embeddedplatforms; NVIDIA AI Enterprise and other software; and DGX Cloud.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions forgaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across our", "page_start": 77, "page_end": 78, "section_title": "NVIDIA CORPORATION", "score": 0.67091626}, {"chunk_id": "c69190e8-be56-40d7-9e3f-a1b1a8224354", "text": "Estimates for customer program accrualsinclude a combination of historical attainment and claim rates and may be adjusted based on relevant internal and external factors.License and Development ArrangementsRevenue from License and Development Arrangements is recognized over the period in which the development services are performed.Each fiscal reportingperiod, we measure progress to completion based on actual cost incurred to date as a percentage of the estimated total cost required to complete each project.Estimated total cost for each project includes a forecast of internal engineer personnel time expected to be incurred and other third-party costs as applicable.Contracts with Multiple Performance ObligationsOur contracts may contain more than one performance obligation.Judgement is required in determining whether each performance obligation within a customercontract is distinct.Except for License and Development Arrangements, NVIDIA products and services function on a standalone basis and do not require asignificant amount of integration or interdependency.Therefore, multiple performance obligations contained within a customer contract are considered distinctand are not combined for revenue recognition purposes.We allocate the total transaction price to each distinct performance obligation in a multiple performance obligations arrangement on a relative standalone sellingprice basis.In certain cases, we can establish standalone selling price based on directly observable prices of products or services sold separately in comparablecircumstances to similar customers.If standalone selling price is not directly observable, such as when we do not sell a product or service separately, wedetermine standalone selling price based on market data and other observable inputs.Change in Accounting EstimateIn February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of a majority of the server, storage, and network equipment from three years to a range of four to five years, and assembly and testequipment from five years to seven years.The estimated effect of this change for fiscal year 2024 was a benefit of $33 million and $102 million for cost ofrevenue and operating expenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or$0.05 per both basic and diluted share.Results of OperationsA discussion regarding our financial condition and results of operations for fiscal year 2024 compared to fiscal year 2023 is presented below.A discussionregarding our financial condition and results of operations for fiscal year 2023 compared to fiscal year 2022 can be found under Item 7 in our Annual Report onForm 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38", "page_start": 38, "page_end": 38, "section_title": "NVIDIA CORPORATION", "score": 0.6697141}, {"chunk_id": "4f638c9d-79d6-4b84-9677-86448558b897", "text": "44,30115,35617,475Operating expensesResearch and development8,6757,3395,268Sales, general and administrative2,6542,4402,166—1,353—Acquisition termination cost11,32911,1327,434Total operating expensesOperating income32,9724,22410,041Interest income86626729Interest expense(257)(262)(236)237(48)107Other, net846(43)(100)Other income (expense), netIncome before income tax33,8184,1819,941Income tax expense (benefit)4,058(187)189$29,760$4,368$9,752Net incomeNet income per share:$12.05$1.76$3.91Basic$11.93$1.74$3.85DilutedWeighted average shares used in per share computation:2,4692,4872,496Basic2,4942,5072,535DilutedSee accompanying notes to the consolidated financial statements.50NVIDIA Corporation and SubsidiariesConsolidated Statements of Comprehensive Income(In millions)Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Net income$29,760$4,368$9,752Other comprehensive income (loss), net of taxAvailable-for-sale securities:Net change in unrealized gain (loss)80(31)(16)Reclassification adjustments for net realized gain included in net income—1—80(30)(16)Net change in unrealized gain (loss)Cash flow hedges:Net change in unrealized gain (loss)3847(43)(48)(49)29Reclassification adjustments for net realized gain (loss) included in net income(10)(2)(14)Net change in unrealized loss70(32)(30)Other comprehensive income (loss), net of tax$29,830$4,336$9,722Total comprehensive incomeSee accompanying notes to the consolidated financial statements.51NVIDIA Corporation and SubsidiariesConsolidated Balance Sheets(In millions, except par value)Jan 28, 2024Jan 29, 2023AssetsCurrent assets:Cash and cash equivalents$7,280$3,389Marketable securities18,7049,907Accounts receivable, net9,9993,827Inventories5,2825,1593,080791Prepaid expenses and other current assetsTotal current assets44,34523,073Property and equipment, net3,9143,807Operating lease assets1,3461,038Goodwill4,4304,372Intangible assets, net1,1121,676Deferred income tax assets6,0813,3964,5003,820Other assets$65,728$41,182Total assetsLiabilities and Shareholders' EquityCurrent liabilities:Accounts payable$2,699$1,193Accrued and other current liabilities6,682", "page_start": 50, "page_end": 52, "section_title": "NVIDIA CORPORATION", "score": 0.6581472}, {"chunk_id": "16846335-f2af-4650-bf60-d4b5485698d4", "text": "Principles of ConsolidationOur consolidated financial statements include the accounts of NVIDIA Corporation and our wholly-owned subsidiaries.All intercompany balances andtransactions have been eliminated in consolidation.Use of EstimatesThe preparation of financial statements in conformity with U.S.GAAP requires management to make estimates and assumptions that affect the reportedamounts of assets and liabilities and disclosures of contingent assets and liabilities at the date of the financial statements and the reported amounts of revenueand expenses during the reporting period.Actual results could differ materially from our estimates.On an on-going basis, we evaluate our estimates, includingthose related to revenue recognition, cash equivalents and marketable securities, accounts receivable, inventories and product purchase commitments, incometaxes, goodwill, stock-based compensation, litigation, investigation and settlement costs, restructuring and other charges, property, plant, and equipment, andother contingencies.These estimates are based on historical facts and various other assumptions that we believe are reasonable.In February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of most of our server, storage, and network equipment from three to four or five years, and our assembly and test equipment from five toseven years.The effect of this change for the fiscal year ended January 28, 2024 was a benefit of $33 million and $102 million for cost of revenue and operatingexpenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or $0.05 per both basic anddiluted share.Revenue RecognitionWe derive our revenue from product sales, including hardware and systems, license and development arrangements, software licensing, and cloud services.Wedetermine revenue recognition through the following steps: (1) identification of the contract with a customer; (2) identification of the performance obligations inthe contract; (3) determination of the transaction price; (4) allocation of the transaction price to the performance obligations in the contract (where revenue isallocated on a relative standalone selling price basis by maximizing the use of observable inputs to determine the standalone selling price for each performanceobligation); and (5) recognition of revenue when, or as, we satisfy a performance obligation.Product Sales RevenueRevenue from product sales is recognized upon transfer of control of products to customers in an amount that reflects the consideration we expect to receive inexchange for those products.Certain products are sold with support or an extended warranty for the incorporated system, hardware, and/or software.Supportand extended warranty revenue are recognized ratably over the service period, or as services are performed.Revenue is recognized net of allowances forreturns, customer programs and any taxes collected from customers.For products sold with a right of return, we record a reduction to revenue by establishing a sales return allowance for estimated product returns at the timerevenue is recognized, based primarily on historical return rates.", "page_start": 55, "page_end": 55, "section_title": "NVIDIA CORPORATION", "score": 0.6577978}, {"chunk_id": "b54b5cad-85c6-477b-a80b-27989017a705", "text": "Fair value is based upon observable inputs in an inactivemarket and the valuation requires our judgment due to the absence of market prices and inherent lack of liquidity.All gains and losses on these investments,realized and unrealized, are recognized in other income (expense), net on our Consolidated Statements of Income.We assess whether an impairment loss has occurred on our investments in non-marketable equity securities, accounted for under the measurement alternativebased on quantitative and qualitative factors.If any impairment is identified for non-marketable equity securities, we write down the investment to its fair valueand record the corresponding charge through other income (expense), net on our Consolidated Statements of Income.59NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Recently Issued Accounting PronouncementsRecent Accounting Pronouncements Not Yet AdoptedIn November 2023, the Financial Accounting Standards Board, or FASB, issued a new accounting standard to provide for additional disclosures about significantexpenses in operating segments.The standard is effective for our annual reporting for fiscal year 2025 and for interim period reporting starting in fiscal year 2026retrospectively.We are currently evaluating the impact of this standard on our Consolidated Financial Statements.In December 2023, the FASB issued a new accounting standard which provides for new and changes to income tax disclosures including disaggregation of therate reconciliation and income taxes paid disclosures.The amendments in the standard are effective for annual periods beginning after December 15, 2024.Early adoption is permitted and should be applied prospectively, with retrospective application permitted.We expect to adopt this standard in our annual periodbeginning fiscal year 2026.We are currently evaluating the impact of this standard on our Consolidated Financial Statements.Note 2 - Business CombinationTermination of the Arm Share Purchase AgreementIn February 2022, NVIDIA and SoftBank Group Corp, or SoftBank, announced the termination of the Share Purchase Agreement whereby NVIDIA would haveacquired Arm from SoftBank.The parties agreed to terminate it due to significant regulatory challenges preventing the completion of the transaction.Werecorded an acquisition termination cost of $1.4 billion in fiscal year 2023 reflecting the write-off of the prepayment provided at signing.Note 3 - LeasesOur lease obligations primarily consist of operating leases for our headquarters complex, domestic and international office facilities, and data center space, withlease periods expiring between fiscal years 2025 and 2035.Futureminimumleasepaymentsunderournon-cancelableoperatingleasesasofJanuary28,2024,areasfollows:Operating Lease Obligations(In millions)Fiscal Year:2025$29020262702027253202823620292022030 and thereafter288Total1,539192Less imputed interestPresent value of net future minimum lease payments1,347228", "page_start": 59, "page_end": 60, "section_title": "NVIDIA CORPORATION", "score": 0.6452376}], "multilingual_nvidia_ja": [{"chunk_id": "ddc05d6e-79f6-4ed4-a29b-876d9e76fd48", "text": "We did not experience any significant impact or expense toour business; however, if the conflict is further extended, it could impact future product development, operations, and revenue or create other uncertainty for ourbusiness.35Fiscal Year 2024 SummaryYear EndedJan 28, 2024Jan 29, 2023Change($ in millions, except per share data)Revenue$60,922$26,974Up 126%Gross margin72.7 %56.9 %Up 15.8 ptsOperating expenses$11,329$11,132Up 2%Operating income$32,972$4,224Up 681%Net income$29,760$4,368Up 581%Net income per diluted share$11.93$1.74Up 586%We specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Revenue for fiscal year 2024 was $60.9 billion, up 126% from a year ago.Data Center revenue for fiscal year 2024 was up 217%.Strong demand was driven by enterprise software and consumer internet applications, and multipleindustry verticals including automotive, financial services, and healthcare.Customers across industry verticals access NVIDIA AI infrastructure both through thecloud and on-premises.Data Center compute revenue was up 244% in the fiscal year.Networking revenue was up 133% in the fiscal year.Gaming revenue for fiscal year 2024 was up 15%.The increase reflects higher sell-in to partners following the normalization of channel inventory levels andgrowing demand.Professional Visualization revenue for fiscal year 2024 was up 1%.Automotive revenue for the fiscal year 2024 was up 21%.The increase primarily reflected growth in self-driving platforms.Gross margin increased in fiscal year 2024, primarily driven by Data Center revenue growth and lower net inventory provisions as a percentage of revenue.Operating expenses increased for fiscal year 2024, driven by growth in employees and compensation increases.Fiscal year 2023 also included a $1.4 billionacquisition termination charge related to the proposed Arm transaction.Market Platform HighlightsData Center revenue for fiscal year 2024 was $47.5 billion, up 217% from fiscal year 2023.In Data Center, we launched AI inference platforms that combine ourfull-stack inference software with NVIDIA Ada, NVIDIA Hopper and NVIDIA Grace Hopper processors optimized for generative AI, LLMs and other AI workloads.We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.", "page_start": 35, "page_end": 36, "section_title": "NVIDIA CORPORATION", "score": 0.65774894}, {"chunk_id": "823438dd-9ce8-4ae9-924a-70c6bbf5121f", "text": "Form 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38The following table sets forth, for the periods indicated, certain items in our Consolidated Statements of Income expressed as a percentage of revenue.Year EndedJan 28, 2024Jan 29, 2023Revenue100.0 %100.0 %Cost of revenue27.343.1Gross profit72.756.9Operating expensesResearch and development14.227.2Sales, general and administrative4.49.1Acquisition termination cost—5.0Total operating expenses18.641.3Operating income54.115.6Interest income1.41.0Interest expense(0.4)(1.0)0.4(0.1)Other, net1.4(0.1)Other income (expense), netIncome before income tax55.515.5Income tax expense (benefit)6.6(0.7)48.9 %16.2 %Net incomeReportable SegmentsRevenue by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$47,405$15,068$32,337215 %13,51711,9061,611Graphics14 %$60,922$26,974$33,948Total126 %Operating Income by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$32,016$5,083$26,933530 %Graphics5,8464,5521,29428 %(4,890)(5,411)521All Other(10)%$32,972$4,224$28,748Total681 %Compute & Networking revenue – The year-on-year increase was due to higher Data Center revenue.Compute grew 266% due to higher shipments of theNVIDIA Hopper GPU computing platform for the training and inference of LLMs, recommendation engines and generative AI applications.Networking was up133% due to higher shipments of InfiniBand.Graphics revenue – The year-on-year increase was led by growth in Gaming of 15% driven by higher sell-in to partners following the normalization of channelinventory levels.Reportable segment operating income – The year-on-year increase in Compute & Networking and Graphics operating income was driven by higher revenue.39All Other operating loss - The year-on-year decrease was due to the $1.4 billion Arm acquisition termination cost in fiscal year 2023, partially offset by a $839million increase in stock-based compensation expense in fiscal year 2024.Concentration of Revenue", "page_start": 38, "page_end": 40, "section_title": "NVIDIA CORPORATION", "score": 0.64945424}, {"chunk_id": "c69190e8-be56-40d7-9e3f-a1b1a8224354", "text": "Estimates for customer program accrualsinclude a combination of historical attainment and claim rates and may be adjusted based on relevant internal and external factors.License and Development ArrangementsRevenue from License and Development Arrangements is recognized over the period in which the development services are performed.Each fiscal reportingperiod, we measure progress to completion based on actual cost incurred to date as a percentage of the estimated total cost required to complete each project.Estimated total cost for each project includes a forecast of internal engineer personnel time expected to be incurred and other third-party costs as applicable.Contracts with Multiple Performance ObligationsOur contracts may contain more than one performance obligation.Judgement is required in determining whether each performance obligation within a customercontract is distinct.Except for License and Development Arrangements, NVIDIA products and services function on a standalone basis and do not require asignificant amount of integration or interdependency.Therefore, multiple performance obligations contained within a customer contract are considered distinctand are not combined for revenue recognition purposes.We allocate the total transaction price to each distinct performance obligation in a multiple performance obligations arrangement on a relative standalone sellingprice basis.In certain cases, we can establish standalone selling price based on directly observable prices of products or services sold separately in comparablecircumstances to similar customers.If standalone selling price is not directly observable, such as when we do not sell a product or service separately, wedetermine standalone selling price based on market data and other observable inputs.Change in Accounting EstimateIn February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of a majority of the server, storage, and network equipment from three years to a range of four to five years, and assembly and testequipment from five years to seven years.The estimated effect of this change for fiscal year 2024 was a benefit of $33 million and $102 million for cost ofrevenue and operating expenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or$0.05 per both basic and diluted share.Results of OperationsA discussion regarding our financial condition and results of operations for fiscal year 2024 compared to fiscal year 2023 is presented below.A discussionregarding our financial condition and results of operations for fiscal year 2023 compared to fiscal year 2022 can be found under Item 7 in our Annual Report onForm 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38", "page_start": 38, "page_end": 38, "section_title": "NVIDIA CORPORATION", "score": 0.61510146}, {"chunk_id": "8d7f8306-8d55-40f5-b383-f4914775e7ee", "text": "(10)Restructuring costs and other—(54)—Acquisition termination cost—(1,353)—10(2)—Other$(4,890)$(5,411)$(3,049)TotalRevenue by geographic areas is designated based upon the billing location of the customer.End customer location may be different than our customer’s billinglocation.Revenue by geographic areas was as follows:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Revenue:(In millions)United States$26,966$8,292$4,349Taiwan13,4056,9868,544China (including Hong Kong)10,3065,7857,11110,2455,9116,910Other countries$60,922$26,974$26,914Total revenueRevenue from sales to customers outside of the United States accounted for 56%, 69%, and 84% of total revenue for fiscal years 2024, 2023, and 2022,respectively.The increase in revenue to the United States for fiscal year 2024 was primarily due to higher U.S.-based Compute & Networking segment demand.Sales to one customer represented 13% of total revenue for fiscal year 2024, which was attributable to the Compute & Networking segment.No customerrepresented 10% or more of total revenue for fiscal years 2023 and 2022.The following table summarizes information pertaining to our revenue by each of the specialized markets we serve:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Revenue:(In millions)Data Center$47,525$15,005$10,613Gaming10,4479,06712,462Professional Visualization1,5531,5442,111Automotive1,0919035663064551,162OEM and Other$60,922$26,974$26,914Total revenue79NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)The following table presents summarized information for long-lived assets by country.Long-lived assets consist of property and equipment and exclude otherassets, operating lease assets, goodwill, and intangible assets.Jan 28, 2024Jan 29, 2023Long-lived assets:(In millions)United States$2,595$2,587Taiwan773702Israel325283Other countries221235$3,914$3,807Total long-lived assets80NVIDIA Corporation and SubsidiariesSchedule II – Valuation and Qualifying AccountsBalance atBeginning ofBalance atDescriptionPeriodAdditionsDeductionsEnd of Period(In millions)Fiscal year 2024Allowance for doubtful accounts$4$— (1)$— (1)$4Sales return allowance$26$213 (2)$", "page_start": 79, "page_end": 81, "section_title": "NVIDIA CORPORATION", "score": 0.5967095}, {"chunk_id": "f2207e14-4c6c-421b-849b-76359737cca1", "text": "Germany, India, Israel, and Taiwan for fiscal years 2005 through 2023.Note 15 - Shareholders’ EquityCapital Return ProgramIn August 2023, our Board of Directors approved an increase to our share repurchase program of an additional $25.0 billion, without expiration.During fiscalyear 2024, we repurchased 21 million shares of our common stock for $9.7 billion.As of January 28, 2024, we were authorized, subject to certain specifications,to repurchase additional shares of our common stock up to $22.5 billion.From January 29, 2024 through February 16, 2024, we repurchased 2.8 million sharesfor $1.9 billion pursuant to a Rule 10b5-1 trading plan.Our share repurchase program aims to offset dilution from shares issued to employees.We may pursueadditional share repurchases as we weigh market factors and other investment opportunities.During fiscal years 2024, 2023, and 2022, we paid $395 million, $398 million, and $399 million in cash dividends to our shareholders, respectively.Our cashdividend program and the payment of future cash dividends under that program are subject to our Board of Directors' continuing determination that the dividendprogram and the declaration of dividends thereunder are in the best interests of our shareholders.In fiscal year 2022, we retired our existing 349 million treasury shares.These shares assumed the status of authorized and unissued shares upon retirement.The excess of repurchase price over par value was allocated between additional paid-in capital and retained earnings, resulting in a reduction in additional paidin capital by $20 million and retained earnings by $12.0 billion.Any future repurchased shares will assume the status of authorized and unissued shares.77NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Note 16 - Employee Retirement PlansWe provide tax-qualified defined contribution plans to eligible employees in the U.S.and certain other countries.Our contribution expense for fiscal years 2024,2023, and 2022 was $255 million, $227 million, and $168 million, respectively.Note 17 - Segment InformationOur Chief Executive Officer, who is considered to be our chief operating decision maker, or CODM, reviews financial information presented on an operatingsegment basis for purposes of making decisions and assessing financial performance.The Compute & Networking segment includes our Data Center accelerated computing platform; networking; automotive artificial intelligence, or AI, Cockpit,autonomous driving development agreements, and autonomous vehicle solutions; electric vehicle computing platforms; Jetson for robotics and other embeddedplatforms; NVIDIA AI Enterprise and other software; and DGX Cloud.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions forgaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across our", "page_start": 77, "page_end": 78, "section_title": "NVIDIA CORPORATION", "score": 0.59651005}, {"chunk_id": "0dd7fcd2-abcb-4469-bfdc-d6d5c6cb8eae", "text": "*$100 invested on 1/27/19 in stock and in indices, including reinvestment of dividends.Source: FactSet financial data and analytics.1/27/20191/26/20201/31/20211/30/20221/29/20231/28/2024NVIDIA Corporation$100.00$157.02$326.26$574.15$512.40$1,536.28S&P 500$100.00$126.17$144.83$175.25$163.63$199.83Nasdaq 100$100.00$136.15$194.20$218.68$185.67$268.13Item 6.[Reserved]33Item 7.Management's Discussion and Analysis of Financial Condition and Results of OperationsThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with “Item 1A.Risk Factors”, ourConsolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described elsewhere in this Annual Report onForm 10-K, before deciding to purchase, hold or sell shares of our common stock.OverviewOur Company and Our BusinessesNVIDIA pioneered accelerated computing to help solve the most challenging computational problems.Since our original focus on PC graphics, we haveexpanded to several other large and important computationally intensive fields.NVIDIA has leveraged its GPU architecture to create platforms for acceleratedcomputing, AI solutions, scientific computing, data science, AV, robotics, metaverse and 3D internet applications.Our two operating segments are \"Compute & Networking\" and \"Graphics.\" Refer to Note 17 of the Notes to the Consolidated Financial Statements in Part IV,Item 15 of this Annual Report on Form 10-K for additional information.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Recent Developments, Future Objectives and ChallengesDemand and Supply, Product Transitions, and New Products and Business ModelsDemand for our data center systems and products surged in fiscal year 2024.Entering fiscal year 2025, we are gathering customer demand indications acrossseveral product transitions.We have demand visibility for our new data center products ramping later in fiscal year 2025.We have increased our supply andcapacity purchases with existing suppliers, added new vendors and entered into prepaid manufacturing and capacity agreements.These increased purchasevolumes, the number of suppliers, and the integration of new vendors into our supply chain may create more complexity and execution risk.Our purchasecommitments and obligations for inventory and manufacturing capacity at the end of fiscal year 2024 were impacted by shortening lead times for certaincomponents.We may continue to enter into new supplier and capacity arrangements.Supply of Hopper architecture products is improving, and demand remainsvery strong.We expect our next-generation products to be supply-constrained based upon demand indications.", "page_start": 33, "page_end": 34, "section_title": "NVIDIA CORPORATION", "score": 0.5918457}, {"chunk_id": "4f638c9d-79d6-4b84-9677-86448558b897", "text": "44,30115,35617,475Operating expensesResearch and development8,6757,3395,268Sales, general and administrative2,6542,4402,166—1,353—Acquisition termination cost11,32911,1327,434Total operating expensesOperating income32,9724,22410,041Interest income86626729Interest expense(257)(262)(236)237(48)107Other, net846(43)(100)Other income (expense), netIncome before income tax33,8184,1819,941Income tax expense (benefit)4,058(187)189$29,760$4,368$9,752Net incomeNet income per share:$12.05$1.76$3.91Basic$11.93$1.74$3.85DilutedWeighted average shares used in per share computation:2,4692,4872,496Basic2,4942,5072,535DilutedSee accompanying notes to the consolidated financial statements.50NVIDIA Corporation and SubsidiariesConsolidated Statements of Comprehensive Income(In millions)Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Net income$29,760$4,368$9,752Other comprehensive income (loss), net of taxAvailable-for-sale securities:Net change in unrealized gain (loss)80(31)(16)Reclassification adjustments for net realized gain included in net income—1—80(30)(16)Net change in unrealized gain (loss)Cash flow hedges:Net change in unrealized gain (loss)3847(43)(48)(49)29Reclassification adjustments for net realized gain (loss) included in net income(10)(2)(14)Net change in unrealized loss70(32)(30)Other comprehensive income (loss), net of tax$29,830$4,336$9,722Total comprehensive incomeSee accompanying notes to the consolidated financial statements.51NVIDIA Corporation and SubsidiariesConsolidated Balance Sheets(In millions, except par value)Jan 28, 2024Jan 29, 2023AssetsCurrent assets:Cash and cash equivalents$7,280$3,389Marketable securities18,7049,907Accounts receivable, net9,9993,827Inventories5,2825,1593,080791Prepaid expenses and other current assetsTotal current assets44,34523,073Property and equipment, net3,9143,807Operating lease assets1,3461,038Goodwill4,4304,372Intangible assets, net1,1121,676Deferred income tax assets6,0813,3964,5003,820Other assets$65,728$41,182Total assetsLiabilities and Shareholders' EquityCurrent liabilities:Accounts payable$2,699$1,193Accrued and other current liabilities6,682", "page_start": 50, "page_end": 52, "section_title": "NVIDIA CORPORATION", "score": 0.5902442}, {"chunk_id": "16846335-f2af-4650-bf60-d4b5485698d4", "text": "Principles of ConsolidationOur consolidated financial statements include the accounts of NVIDIA Corporation and our wholly-owned subsidiaries.All intercompany balances andtransactions have been eliminated in consolidation.Use of EstimatesThe preparation of financial statements in conformity with U.S.GAAP requires management to make estimates and assumptions that affect the reportedamounts of assets and liabilities and disclosures of contingent assets and liabilities at the date of the financial statements and the reported amounts of revenueand expenses during the reporting period.Actual results could differ materially from our estimates.On an on-going basis, we evaluate our estimates, includingthose related to revenue recognition, cash equivalents and marketable securities, accounts receivable, inventories and product purchase commitments, incometaxes, goodwill, stock-based compensation, litigation, investigation and settlement costs, restructuring and other charges, property, plant, and equipment, andother contingencies.These estimates are based on historical facts and various other assumptions that we believe are reasonable.In February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of most of our server, storage, and network equipment from three to four or five years, and our assembly and test equipment from five toseven years.The effect of this change for the fiscal year ended January 28, 2024 was a benefit of $33 million and $102 million for cost of revenue and operatingexpenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or $0.05 per both basic anddiluted share.Revenue RecognitionWe derive our revenue from product sales, including hardware and systems, license and development arrangements, software licensing, and cloud services.Wedetermine revenue recognition through the following steps: (1) identification of the contract with a customer; (2) identification of the performance obligations inthe contract; (3) determination of the transaction price; (4) allocation of the transaction price to the performance obligations in the contract (where revenue isallocated on a relative standalone selling price basis by maximizing the use of observable inputs to determine the standalone selling price for each performanceobligation); and (5) recognition of revenue when, or as, we satisfy a performance obligation.Product Sales RevenueRevenue from product sales is recognized upon transfer of control of products to customers in an amount that reflects the consideration we expect to receive inexchange for those products.Certain products are sold with support or an extended warranty for the incorporated system, hardware, and/or software.Supportand extended warranty revenue are recognized ratably over the service period, or as services are performed.Revenue is recognized net of allowances forreturns, customer programs and any taxes collected from customers.For products sold with a right of return, we record a reduction to revenue by establishing a sales return allowance for estimated product returns at the timerevenue is recognized, based primarily on historical return rates.", "page_start": 55, "page_end": 55, "section_title": "NVIDIA CORPORATION", "score": 0.5826086}], "multilingual_nvidia_ar": [{"chunk_id": "823438dd-9ce8-4ae9-924a-70c6bbf5121f", "text": "Form 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38The following table sets forth, for the periods indicated, certain items in our Consolidated Statements of Income expressed as a percentage of revenue.Year EndedJan 28, 2024Jan 29, 2023Revenue100.0 %100.0 %Cost of revenue27.343.1Gross profit72.756.9Operating expensesResearch and development14.227.2Sales, general and administrative4.49.1Acquisition termination cost—5.0Total operating expenses18.641.3Operating income54.115.6Interest income1.41.0Interest expense(0.4)(1.0)0.4(0.1)Other, net1.4(0.1)Other income (expense), netIncome before income tax55.515.5Income tax expense (benefit)6.6(0.7)48.9 %16.2 %Net incomeReportable SegmentsRevenue by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$47,405$15,068$32,337215 %13,51711,9061,611Graphics14 %$60,922$26,974$33,948Total126 %Operating Income by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$32,016$5,083$26,933530 %Graphics5,8464,5521,29428 %(4,890)(5,411)521All Other(10)%$32,972$4,224$28,748Total681 %Compute & Networking revenue – The year-on-year increase was due to higher Data Center revenue.Compute grew 266% due to higher shipments of theNVIDIA Hopper GPU computing platform for the training and inference of LLMs, recommendation engines and generative AI applications.Networking was up133% due to higher shipments of InfiniBand.Graphics revenue – The year-on-year increase was led by growth in Gaming of 15% driven by higher sell-in to partners following the normalization of channelinventory levels.Reportable segment operating income – The year-on-year increase in Compute & Networking and Graphics operating income was driven by higher revenue.39All Other operating loss - The year-on-year decrease was due to the $1.4 billion Arm acquisition termination cost in fiscal year 2023, partially offset by a $839million increase in stock-based compensation expense in fiscal year 2024.Concentration of Revenue", "page_start": 38, "page_end": 40, "section_title": "NVIDIA CORPORATION", "score": 0.6038776}, {"chunk_id": "ddc05d6e-79f6-4ed4-a29b-876d9e76fd48", "text": "We did not experience any significant impact or expense toour business; however, if the conflict is further extended, it could impact future product development, operations, and revenue or create other uncertainty for ourbusiness.35Fiscal Year 2024 SummaryYear EndedJan 28, 2024Jan 29, 2023Change($ in millions, except per share data)Revenue$60,922$26,974Up 126%Gross margin72.7 %56.9 %Up 15.8 ptsOperating expenses$11,329$11,132Up 2%Operating income$32,972$4,224Up 681%Net income$29,760$4,368Up 581%Net income per diluted share$11.93$1.74Up 586%We specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Revenue for fiscal year 2024 was $60.9 billion, up 126% from a year ago.Data Center revenue for fiscal year 2024 was up 217%.Strong demand was driven by enterprise software and consumer internet applications, and multipleindustry verticals including automotive, financial services, and healthcare.Customers across industry verticals access NVIDIA AI infrastructure both through thecloud and on-premises.Data Center compute revenue was up 244% in the fiscal year.Networking revenue was up 133% in the fiscal year.Gaming revenue for fiscal year 2024 was up 15%.The increase reflects higher sell-in to partners following the normalization of channel inventory levels andgrowing demand.Professional Visualization revenue for fiscal year 2024 was up 1%.Automotive revenue for the fiscal year 2024 was up 21%.The increase primarily reflected growth in self-driving platforms.Gross margin increased in fiscal year 2024, primarily driven by Data Center revenue growth and lower net inventory provisions as a percentage of revenue.Operating expenses increased for fiscal year 2024, driven by growth in employees and compensation increases.Fiscal year 2023 also included a $1.4 billionacquisition termination charge related to the proposed Arm transaction.Market Platform HighlightsData Center revenue for fiscal year 2024 was $47.5 billion, up 217% from fiscal year 2023.In Data Center, we launched AI inference platforms that combine ourfull-stack inference software with NVIDIA Ada, NVIDIA Hopper and NVIDIA Grace Hopper processors optimized for generative AI, LLMs and other AI workloads.We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.", "page_start": 35, "page_end": 36, "section_title": "NVIDIA CORPORATION", "score": 0.5862895}, {"chunk_id": "c69190e8-be56-40d7-9e3f-a1b1a8224354", "text": "Estimates for customer program accrualsinclude a combination of historical attainment and claim rates and may be adjusted based on relevant internal and external factors.License and Development ArrangementsRevenue from License and Development Arrangements is recognized over the period in which the development services are performed.Each fiscal reportingperiod, we measure progress to completion based on actual cost incurred to date as a percentage of the estimated total cost required to complete each project.Estimated total cost for each project includes a forecast of internal engineer personnel time expected to be incurred and other third-party costs as applicable.Contracts with Multiple Performance ObligationsOur contracts may contain more than one performance obligation.Judgement is required in determining whether each performance obligation within a customercontract is distinct.Except for License and Development Arrangements, NVIDIA products and services function on a standalone basis and do not require asignificant amount of integration or interdependency.Therefore, multiple performance obligations contained within a customer contract are considered distinctand are not combined for revenue recognition purposes.We allocate the total transaction price to each distinct performance obligation in a multiple performance obligations arrangement on a relative standalone sellingprice basis.In certain cases, we can establish standalone selling price based on directly observable prices of products or services sold separately in comparablecircumstances to similar customers.If standalone selling price is not directly observable, such as when we do not sell a product or service separately, wedetermine standalone selling price based on market data and other observable inputs.Change in Accounting EstimateIn February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of a majority of the server, storage, and network equipment from three years to a range of four to five years, and assembly and testequipment from five years to seven years.The estimated effect of this change for fiscal year 2024 was a benefit of $33 million and $102 million for cost ofrevenue and operating expenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or$0.05 per both basic and diluted share.Results of OperationsA discussion regarding our financial condition and results of operations for fiscal year 2024 compared to fiscal year 2023 is presented below.A discussionregarding our financial condition and results of operations for fiscal year 2023 compared to fiscal year 2022 can be found under Item 7 in our Annual Report onForm 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38", "page_start": 38, "page_end": 38, "section_title": "NVIDIA CORPORATION", "score": 0.5683618}, {"chunk_id": "4f638c9d-79d6-4b84-9677-86448558b897", "text": "44,30115,35617,475Operating expensesResearch and development8,6757,3395,268Sales, general and administrative2,6542,4402,166—1,353—Acquisition termination cost11,32911,1327,434Total operating expensesOperating income32,9724,22410,041Interest income86626729Interest expense(257)(262)(236)237(48)107Other, net846(43)(100)Other income (expense), netIncome before income tax33,8184,1819,941Income tax expense (benefit)4,058(187)189$29,760$4,368$9,752Net incomeNet income per share:$12.05$1.76$3.91Basic$11.93$1.74$3.85DilutedWeighted average shares used in per share computation:2,4692,4872,496Basic2,4942,5072,535DilutedSee accompanying notes to the consolidated financial statements.50NVIDIA Corporation and SubsidiariesConsolidated Statements of Comprehensive Income(In millions)Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Net income$29,760$4,368$9,752Other comprehensive income (loss), net of taxAvailable-for-sale securities:Net change in unrealized gain (loss)80(31)(16)Reclassification adjustments for net realized gain included in net income—1—80(30)(16)Net change in unrealized gain (loss)Cash flow hedges:Net change in unrealized gain (loss)3847(43)(48)(49)29Reclassification adjustments for net realized gain (loss) included in net income(10)(2)(14)Net change in unrealized loss70(32)(30)Other comprehensive income (loss), net of tax$29,830$4,336$9,722Total comprehensive incomeSee accompanying notes to the consolidated financial statements.51NVIDIA Corporation and SubsidiariesConsolidated Balance Sheets(In millions, except par value)Jan 28, 2024Jan 29, 2023AssetsCurrent assets:Cash and cash equivalents$7,280$3,389Marketable securities18,7049,907Accounts receivable, net9,9993,827Inventories5,2825,1593,080791Prepaid expenses and other current assetsTotal current assets44,34523,073Property and equipment, net3,9143,807Operating lease assets1,3461,038Goodwill4,4304,372Intangible assets, net1,1121,676Deferred income tax assets6,0813,3964,5003,820Other assets$65,728$41,182Total assetsLiabilities and Shareholders' EquityCurrent liabilities:Accounts payable$2,699$1,193Accrued and other current liabilities6,682", "page_start": 50, "page_end": 52, "section_title": "NVIDIA CORPORATION", "score": 0.567471}, {"chunk_id": "b54b5cad-85c6-477b-a80b-27989017a705", "text": "Fair value is based upon observable inputs in an inactivemarket and the valuation requires our judgment due to the absence of market prices and inherent lack of liquidity.All gains and losses on these investments,realized and unrealized, are recognized in other income (expense), net on our Consolidated Statements of Income.We assess whether an impairment loss has occurred on our investments in non-marketable equity securities, accounted for under the measurement alternativebased on quantitative and qualitative factors.If any impairment is identified for non-marketable equity securities, we write down the investment to its fair valueand record the corresponding charge through other income (expense), net on our Consolidated Statements of Income.59NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Recently Issued Accounting PronouncementsRecent Accounting Pronouncements Not Yet AdoptedIn November 2023, the Financial Accounting Standards Board, or FASB, issued a new accounting standard to provide for additional disclosures about significantexpenses in operating segments.The standard is effective for our annual reporting for fiscal year 2025 and for interim period reporting starting in fiscal year 2026retrospectively.We are currently evaluating the impact of this standard on our Consolidated Financial Statements.In December 2023, the FASB issued a new accounting standard which provides for new and changes to income tax disclosures including disaggregation of therate reconciliation and income taxes paid disclosures.The amendments in the standard are effective for annual periods beginning after December 15, 2024.Early adoption is permitted and should be applied prospectively, with retrospective application permitted.We expect to adopt this standard in our annual periodbeginning fiscal year 2026.We are currently evaluating the impact of this standard on our Consolidated Financial Statements.Note 2 - Business CombinationTermination of the Arm Share Purchase AgreementIn February 2022, NVIDIA and SoftBank Group Corp, or SoftBank, announced the termination of the Share Purchase Agreement whereby NVIDIA would haveacquired Arm from SoftBank.The parties agreed to terminate it due to significant regulatory challenges preventing the completion of the transaction.Werecorded an acquisition termination cost of $1.4 billion in fiscal year 2023 reflecting the write-off of the prepayment provided at signing.Note 3 - LeasesOur lease obligations primarily consist of operating leases for our headquarters complex, domestic and international office facilities, and data center space, withlease periods expiring between fiscal years 2025 and 2035.Futureminimumleasepaymentsunderournon-cancelableoperatingleasesasofJanuary28,2024,areasfollows:Operating Lease Obligations(In millions)Fiscal Year:2025$29020262702027253202823620292022030 and thereafter288Total1,539192Less imputed interestPresent value of net future minimum lease payments1,347228", "page_start": 59, "page_end": 60, "section_title": "NVIDIA CORPORATION", "score": 0.5633031}, {"chunk_id": "f2207e14-4c6c-421b-849b-76359737cca1", "text": "Germany, India, Israel, and Taiwan for fiscal years 2005 through 2023.Note 15 - Shareholders’ EquityCapital Return ProgramIn August 2023, our Board of Directors approved an increase to our share repurchase program of an additional $25.0 billion, without expiration.During fiscalyear 2024, we repurchased 21 million shares of our common stock for $9.7 billion.As of January 28, 2024, we were authorized, subject to certain specifications,to repurchase additional shares of our common stock up to $22.5 billion.From January 29, 2024 through February 16, 2024, we repurchased 2.8 million sharesfor $1.9 billion pursuant to a Rule 10b5-1 trading plan.Our share repurchase program aims to offset dilution from shares issued to employees.We may pursueadditional share repurchases as we weigh market factors and other investment opportunities.During fiscal years 2024, 2023, and 2022, we paid $395 million, $398 million, and $399 million in cash dividends to our shareholders, respectively.Our cashdividend program and the payment of future cash dividends under that program are subject to our Board of Directors' continuing determination that the dividendprogram and the declaration of dividends thereunder are in the best interests of our shareholders.In fiscal year 2022, we retired our existing 349 million treasury shares.These shares assumed the status of authorized and unissued shares upon retirement.The excess of repurchase price over par value was allocated between additional paid-in capital and retained earnings, resulting in a reduction in additional paidin capital by $20 million and retained earnings by $12.0 billion.Any future repurchased shares will assume the status of authorized and unissued shares.77NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Note 16 - Employee Retirement PlansWe provide tax-qualified defined contribution plans to eligible employees in the U.S.and certain other countries.Our contribution expense for fiscal years 2024,2023, and 2022 was $255 million, $227 million, and $168 million, respectively.Note 17 - Segment InformationOur Chief Executive Officer, who is considered to be our chief operating decision maker, or CODM, reviews financial information presented on an operatingsegment basis for purposes of making decisions and assessing financial performance.The Compute & Networking segment includes our Data Center accelerated computing platform; networking; automotive artificial intelligence, or AI, Cockpit,autonomous driving development agreements, and autonomous vehicle solutions; electric vehicle computing platforms; Jetson for robotics and other embeddedplatforms; NVIDIA AI Enterprise and other software; and DGX Cloud.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions forgaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across our", "page_start": 77, "page_end": 78, "section_title": "NVIDIA CORPORATION", "score": 0.56154346}, {"chunk_id": "d44f53d4-29ff-496c-a1d8-772d2a857abb", "text": "follows:Operating Lease Obligations(In millions)Fiscal Year:2025$29020262702027253202823620292022030 and thereafter288Total1,539192Less imputed interestPresent value of net future minimum lease payments1,347228Less short-term operating lease liabilities$1,119Long-term operating lease liabilitiesIn addition, we have operating leases, primarily for our data centers, that are expected to commence within fiscal year 2025 with lease terms of 1 to 10 years for$1.1 billion.Operating lease expenses for fiscal years 2024, 2023, and 2022 were $269 million, $193 million, $168 million, respectively.Short-term and variable leaseexpenses for fiscal years 2024, 2023, and 2022 were not significant.60NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Other information related to leases was as follows:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022(In millions)Supplemental cash flows informationOperating cash flows used for operating leases$286$184$154Operating lease assets obtained in exchange for leaseobligations$531$358$266As of January 28, 2024, our operating leases had a weighted average remaining lease term of 6.1 years and a weighted average discount rate of 3.76%.As ofJanuary 29, 2023, our operating leases had a weighted average remaining lease term of 6.8 years and a weighted average discount rate of 3.21%.Note 4 - Stock-Based CompensationOur stock-based compensation expense is associated with RSUs, performance stock units based on our corporate financial performance targets, or PSUs,performance stock units based on market conditions, or market-based PSUs, and our ESPP.Our Consolidated Statements of Income include stock-based compensation expense, net of amounts allocated to inventory, as follows:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022(In millions)Cost of revenue$141$138$141Research and development2,5321,8921,298876680565Sales, general and administrative$3,549$2,710$2,004TotalStock-based compensation capitalized in inventories was not significant during fiscal years 2024, 2023, and 2022.The following is a summary of equity awards granted under our equity incentive plans:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022(In millions, except per share data)RSUs, PSUs and Market-based PSUsAwards granted142518Estimated total grant-date fair value$5,316$4,505$3,492Weighted average grant-date fair value per share$374.08$183.72$190.69ESPPShares purchased335Weighted average price per share$158.07$122.54$56.36Weighted average grant-date fair value per share$69.90$51.87$23.24As of January 28, 2024, there was $8.", "page_start": 60, "page_end": 61, "section_title": "NVIDIA CORPORATION", "score": 0.55935025}, {"chunk_id": "0dd7fcd2-abcb-4469-bfdc-d6d5c6cb8eae", "text": "*$100 invested on 1/27/19 in stock and in indices, including reinvestment of dividends.Source: FactSet financial data and analytics.1/27/20191/26/20201/31/20211/30/20221/29/20231/28/2024NVIDIA Corporation$100.00$157.02$326.26$574.15$512.40$1,536.28S&P 500$100.00$126.17$144.83$175.25$163.63$199.83Nasdaq 100$100.00$136.15$194.20$218.68$185.67$268.13Item 6.[Reserved]33Item 7.Management's Discussion and Analysis of Financial Condition and Results of OperationsThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with “Item 1A.Risk Factors”, ourConsolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described elsewhere in this Annual Report onForm 10-K, before deciding to purchase, hold or sell shares of our common stock.OverviewOur Company and Our BusinessesNVIDIA pioneered accelerated computing to help solve the most challenging computational problems.Since our original focus on PC graphics, we haveexpanded to several other large and important computationally intensive fields.NVIDIA has leveraged its GPU architecture to create platforms for acceleratedcomputing, AI solutions, scientific computing, data science, AV, robotics, metaverse and 3D internet applications.Our two operating segments are \"Compute & Networking\" and \"Graphics.\" Refer to Note 17 of the Notes to the Consolidated Financial Statements in Part IV,Item 15 of this Annual Report on Form 10-K for additional information.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Recent Developments, Future Objectives and ChallengesDemand and Supply, Product Transitions, and New Products and Business ModelsDemand for our data center systems and products surged in fiscal year 2024.Entering fiscal year 2025, we are gathering customer demand indications acrossseveral product transitions.We have demand visibility for our new data center products ramping later in fiscal year 2025.We have increased our supply andcapacity purchases with existing suppliers, added new vendors and entered into prepaid manufacturing and capacity agreements.These increased purchasevolumes, the number of suppliers, and the integration of new vendors into our supply chain may create more complexity and execution risk.Our purchasecommitments and obligations for inventory and manufacturing capacity at the end of fiscal year 2024 were impacted by shortening lead times for certaincomponents.We may continue to enter into new supplier and capacity arrangements.Supply of Hopper architecture products is improving, and demand remainsvery strong.We expect our next-generation products to be supply-constrained based upon demand indications.", "page_start": 33, "page_end": 34, "section_title": "NVIDIA CORPORATION", "score": 0.5563196}], "multilingual_attention_zh": [{"chunk_id": "914cfb0b-3d4a-4988-8392-851b64c13e57", "text": "The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.Weused beam search with a beam size of 4 and length penalty α = 0.6 [38].These hyperparameterswere chosen after experimentation on the development set.We set the maximum output length duringinference to input length + 50, but terminate early when possible [38].Table 2 summarizes our results and compares our translation quality and training costs to other modelarchitectures from the literature.We estimate the number of floating point operations used to train amodel by multiplying the training time, the number of GPUs used, and an estimate of the sustainedsingle-precision floating-point capacity of each GPU 5.6.2Model VariationsTo evaluate the importance of different components of the Transformer, we varied our base modelin different ways, measuring the change in performance on English-to-German translation on the5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the basemodel.All metrics are on the English-to-German translation development set, newstest2013.Listedperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared toper-word perplexities.trainPPLBLEUparamsNdmodeldffhdkdvPdropϵls×106steps(dev)(dev)base65122048864640.10.1100K4.9225.86515125125.2924.941281285.0025.5(A)1632324.9125.83216165.0125.4165.1625.158(B)325.0125.46026.1123.73645.1925.35084.8825.580(C)25632325.7524.52810241281284.6626.016810245.1225.45340964.7526.2900.05.7724.60.24.9525.5(D)0.04.6725.30.25.4725.7(E)positional embedding instead of sinusoids4.9225.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.", "page_start": 8, "page_end": 9, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.44254246}, {"chunk_id": "160dd6c9-8965-4dcd-8d74-e0dd157c4fd1", "text": "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step number.We usedwarmup_steps = 4000.5.4RegularizationWe employ three types of regularization during training:7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on theEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.BLEUTraining Cost (FLOPs)ModelEN-DEEN-FREN-DEEN-FRByteNet [18]23.751.0 · 1020Deep-Att + PosUnk [39]39.22.3 · 10191.4 · 1020GNMT + RL [38]24.639.929.6 · 10181.5 · 1020ConvS2S [9]25.1640.462.0 · 10191.2 · 1020MoE [32]26.0340.568.0 · 1020Deep-Att + PosUnk Ensemble [39]40.41.8 · 10201.1 · 1021GNMT + RL Ensemble [38]26.3041.167.7 · 10191.2 · 102141.29ConvS2S Ensemble [9]26.363.3 · 1018Transformer (base model)27.338.12.3 · 101928.441.8Transformer (big)Residual DropoutWe apply dropout [33] to the output of each sub-layer, before it is added to thesub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and thepositional encodings in both the encoder and decoder stacks.For the base model, we use a rate ofPdrop = 0.1.During training, we employed label smoothing of value ϵls = 0.1 [36].ThisLabel Smoothinghurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6Results6.1Machine TranslationOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0BLEU, establishing a new state-of-the-art BLEU score of 28.4.The configuration of this model islisted in the bottom line of Table 3.Training took 3.5 days on 8 P100 GPUs.Even our base modelsurpasses all previously published models and ensembles, at a fraction of the training cost of any ofthe competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,outperforming all of the previously published single models, at less than 1/4 the training cost of theprevious state-of-the-art model.The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We", "page_start": 7, "page_end": 8, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.3878727}, {"chunk_id": "8d9a2784-c6bd-4675-a20b-80dd0948329f", "text": "25.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,keeping the amount of computation constant, as described in Section 3.2.2.While single-headattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality.Thissuggests that determining compatibility is not easy and that a more sophisticated compatibilityfunction than dot product may be beneficial.We further observe in rows (C) and (D) that, as expected,bigger models are better, and dropout is very helpful in avoiding over-fitting.In row (E) we replace oursinusoidal positional encoding with learned positional embeddings [9], and observe nearly identicalresults to the base model.6.3English Constituency ParsingTo evaluate if the Transformer can generalize to other tasks we performed experiments on Englishconstituency parsing.This task presents specific challenges: the output is subject to strong structuralconstraints and is significantly longer than the input.Furthermore, RNN sequence-to-sequencemodels have not been able to attain state-of-the-art results in small-data regimes [37].We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of thePenn Treebank [25], about 40K training sentences.We also trained it in a semi-supervised setting,using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences[37].We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokensfor the semi-supervised setting.We performed only a small number of experiments to select the dropout, both attention and residual(section 5.4), learning rates and beam size on the Section 22 development set, all other parametersremained unchanged from the English-to-German base translation model.During inference, we9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23of WSJ)ParserTrainingWSJ 23 F1Vinyals & Kaiser el al.(2014) [37]WSJ only, discriminative88.3Petrov et al.(2006) [29]WSJ only, discriminative90.4Zhu et al.(2013) [40]WSJ only, discriminative90.4Dyer et al.(2016) [8]WSJ only, discriminative91.7Transformer (4 layers)WSJ only, discriminative91.3Zhu et al.(2013) [40]semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised", "page_start": 9, "page_end": 10, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.37585017}, {"chunk_id": "6ecb21e7-c073-4d90-9d8a-41fdfda06afb", "text": "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,textual entailment and learning task-independent sentence representations [4, 27, 28, 22].End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering andlanguage modeling tasks [34].To the best of our knowledge, however, the Transformer is the first transduction model relyingentirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.In the following sections, we will describe the Transformer, motivateself-attention and discuss its advantages over models such as [17, 18] and [9].3Model ArchitectureMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequenceof continuous representations z = (z1, ..., zn).Given z, the decoder then generates an outputsequence (y1, ..., ym) of symbols one element at a time.At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next.2Figure 1: The Transformer - model architecture.The Transformer follows this overall architecture using stacked self-attention and point-wise, fullyconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,respectively.3.1Encoder and Decoder StacksThe encoder is composed of a stack of N = 6 identical layers.Each layer has twoEncoder:sub-layers.The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network.We employ a residual connection [11] around each ofthe two sub-layers, followed by layer normalization [1].That is, the output of each sub-layer isLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layeritself.To facilitate these residual connections, all sub-layers in the model, as well as the embeddinglayers, produce outputs of dimension dmodel = 512.The decoder is also composed of a stack of N = 6 identical layers.In addition to the twoDecoder:sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-headattention over the output of the encoder stack.Similar to the encoder, we employ residual connectionsaround each of the sub-layers, followed by layer normalization.We also modify the self-attentionsub-layer in the decoder stack to prevent positions from attending to subsequent positions.Thismasking, combined with fact that the output embeddings are offset by one position, ensures that the", "page_start": 2, "page_end": 3, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.3480861}, {"chunk_id": "0b373c99-8c0e-44bf-8fc7-f6b6b009590d", "text": "Provided proper attribution is provided, Google hereby grants permission toreproduce the tables and figures in this paper solely for use in journalistic orscholarly works.Ashish Vaswani∗Noam Shazeer∗Niki Parmar∗Jakob Uszkoreit∗Google BrainGoogle BrainGoogle ResearchGoogle Researchavaswani@google.comnoam@google.comnikip@google.comusz@google.comLlion Jones∗Aidan N.Gomez∗†Łukasz Kaiser∗Google ResearchUniversity of TorontoGoogle Brainlukaszkaiser@google.comllion@google.comaidan@cs.toronto.eduIllia Polosukhin∗‡illia.polosukhin@gmail.comAbstractThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder.The bestperforming models also connect the encoder and decoder through an attentionmechanism.We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely.Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring significantlyless time to train.Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU.On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.8 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature.We show that the Transformer generalizes well toother tasks by applying it successfully to English constituency parsing both withlarge and limited training data.∗Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and startedthe effort to evaluate this idea.Ashish, with Illia, designed and implemented the first Transformer models andhas been crucially involved in every aspect of this work.Noam proposed scaled dot-product attention, multi-headattention and the parameter-free position representation and became the other person involved in nearly everydetail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase andtensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, andefficient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of andimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks", "page_start": 1, "page_end": 2, "section_title": "", "score": 0.31461966}, {"chunk_id": "58f1c685-b5d0-4ba4-8711-0ba5768f729c", "text": "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,we found it beneficial to linearly project the queries, keys and values h times with different, learnedlinear projections to dk, dk and dv dimensions, respectively.On each of these projected versions ofqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional4To illustrate why the dot products get large, assume that the components of q and k are independent randomvariables with mean 0 and variance 1.Then their dot product, q · k = Pdki=1 qiki, has mean 0 and variance dk.4output values.These are concatenated and once again projected, resulting in the final values, asdepicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representationsubspaces at different positions.With a single attention head, averaging inhibits this.MultiHead(Q, K, V ) = Concat(head1, ..., headh)W Owhere headi = Attention(QW Qi , KW Ki , V W Vi )Where the projections are parameter matrices W Q∈Rdmodel×dk, W V∈Rdmodel×dv∈Rdmodel×dk, W Kiiiand W O ∈Rhdv×dmodel.In this work we employ h = 8 parallel attention layers, or heads.For each of these we usedk = dv = dmodel/h = 64.Due to the reduced dimension of each head, the total computational costis similar to that of single-head attention with full dimensionality.3.2.3Applications of Attention in our ModelThe Transformer uses multi-head attention in three different ways:• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,and the memory keys and values come from the output of the encoder.This allows everyposition in the decoder to attend over all positions in the input sequence.This mimics thetypical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38, 2, 9].• The encoder contains self-attention layers.In a self-attention layer all of the keys, valuesand queries come from the same place, in this case, the output of the previous layer in theencoder.Each position in the encoder can attend to all positions in the previous layer of theencoder.• Similarly, self-attention layers in the decoder allow each position in the decoder to attend toall positions in the decoder up to and including that position.We need to prevent leftwardinformation flow in the decoder to preserve the auto-regressive property.We implement thisinside of scaled dot-product attention by masking out (setting to −∞) all values in the inputof the softmax which correspond to illegal connections.See Figure 2.3.3Position-wise Feed-Forward NetworksIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fullyconnected feed-forward network, which is applied to each position separately and identically.This", "page_start": 4, "page_end": 5, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.31268725}, {"chunk_id": "d18429f8-020c-433c-b74a-eb6e9b76b836", "text": "inside of scaled dot-product attention by masking out (setting to −∞) all values in the inputof the softmax which correspond to illegal connections.See Figure 2.3.3Position-wise Feed-Forward NetworksIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fullyconnected feed-forward network, which is applied to each position separately and identically.Thisconsists of two linear transformations with a ReLU activation in between.FFN(x) = max(0, xW1 + b1)W2 + b2(2)While the linear transformations are the same across different positions, they use different parametersfrom layer to layer.Another way of describing this is as two convolutions with kernel size 1.The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionalitydff = 2048.3.4Embeddings and SoftmaxSimilarly to other sequence transduction models, we use learned embeddings to convert the inputtokens and output tokens to vectors of dimension dmodel.We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.Inour model, we share the same weight matrix between the two embedding layers and the pre-softmaxlinear transformation, similar to [30].In the embedding layers, we multiply those weights by √dmodel.5Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operationsfor different layer types.n is the sequence length, d is the representation dimension, k is the kernelsize of convolutions and r the size of the neighborhood in restricted self-attention.Layer TypeComplexity per LayerSequentialMaximum Path LengthOperationsO(n2 · d)O(1)O(1)Self-AttentionO(n · d2)O(n)O(n)RecurrentO(k · n · d2)O(1)O(logk(n))ConvolutionalO(r · n · d)O(1)O(n/r)Self-Attention (restricted)3.5Positional EncodingSince our model contains no recurrence and no convolution, in order for the model to make use of theorder of the sequence, we must inject some information about the relative or absolute position of thetokens in the sequence.To this end, we add \"positional encodings\" to the input embeddings at thebottoms of the encoder and decoder stacks.The positional encodings have the same dimension dmodelas the embeddings, so that the two can be summed.There are many choices of positional encodings,learned and fixed [9].In this work, we use sine and cosine functions of different frequencies:PE(pos,2i) = sin(pos/100002i/dmodel)PE(pos,2i+1) = cos(pos/100002i/dmodel)where pos is the position and i is the dimension.That is, each dimension of the positional encodingcorresponds to a sinusoid.The wavelengths form a geometric progression from 2π to 10000 · 2π.We", "page_start": 5, "page_end": 6, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.30545065}, {"chunk_id": "bb77d3cc-59f9-4b97-a21b-132b30215259", "text": "semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised92.1Transformer (4 layers)semi-supervised92.7Luong et al.(2015) [23]multi-task93.0Dyer et al.(2016) [8]generative93.3increased the maximum output length to input length + 300.We used a beam size of 21 and α = 0.3for both WSJ only and the semi-supervised setting.Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of theRecurrent Neural Network Grammar [8].In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.7ConclusionIn this work, we presented the Transformer, the first sequence transduction model based entirely onattention, replacing the recurrent layers most commonly used in encoder-decoder architectures withmulti-headed self-attention.For translation tasks, the Transformer can be trained significantly faster than architectures basedon recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014English-to-French translation tasks, we achieve a new state of the art.In the former task our bestmodel outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.Weplan to extend the Transformer to problems involving input and output modalities other than text andto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputssuch as images, audio and video.Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.AcknowledgementsWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitfulcomments, corrections and inspiration.References[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprintarXiv:1607.06450, 2016.[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointlylearning to align and translate.CoRR, abs/1409.0473, 2014.[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V.Le.Massive exploration of neuralmachine translation architectures.CoRR, abs/1703.03906, 2017.[4] Jianpeng Cheng, Li Dong, and Mirella Lapata.Long short-term memory-networks for machinereading.arXiv preprint arXiv:1601.06733, 2016.10[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,", "page_start": 10, "page_end": 11, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.2966255}], "multilingual_attention_es": [{"chunk_id": "6ecb21e7-c073-4d90-9d8a-41fdfda06afb", "text": "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,textual entailment and learning task-independent sentence representations [4, 27, 28, 22].End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering andlanguage modeling tasks [34].To the best of our knowledge, however, the Transformer is the first transduction model relyingentirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.In the following sections, we will describe the Transformer, motivateself-attention and discuss its advantages over models such as [17, 18] and [9].3Model ArchitectureMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequenceof continuous representations z = (z1, ..., zn).Given z, the decoder then generates an outputsequence (y1, ..., ym) of symbols one element at a time.At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next.2Figure 1: The Transformer - model architecture.The Transformer follows this overall architecture using stacked self-attention and point-wise, fullyconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,respectively.3.1Encoder and Decoder StacksThe encoder is composed of a stack of N = 6 identical layers.Each layer has twoEncoder:sub-layers.The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network.We employ a residual connection [11] around each ofthe two sub-layers, followed by layer normalization [1].That is, the output of each sub-layer isLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layeritself.To facilitate these residual connections, all sub-layers in the model, as well as the embeddinglayers, produce outputs of dimension dmodel = 512.The decoder is also composed of a stack of N = 6 identical layers.In addition to the twoDecoder:sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-headattention over the output of the encoder stack.Similar to the encoder, we employ residual connectionsaround each of the sub-layers, followed by layer normalization.We also modify the self-attentionsub-layer in the decoder stack to prevent positions from attending to subsequent positions.Thismasking, combined with fact that the output embeddings are offset by one position, ensures that the", "page_start": 2, "page_end": 3, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.54004383}, {"chunk_id": "914cfb0b-3d4a-4988-8392-851b64c13e57", "text": "The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.Weused beam search with a beam size of 4 and length penalty α = 0.6 [38].These hyperparameterswere chosen after experimentation on the development set.We set the maximum output length duringinference to input length + 50, but terminate early when possible [38].Table 2 summarizes our results and compares our translation quality and training costs to other modelarchitectures from the literature.We estimate the number of floating point operations used to train amodel by multiplying the training time, the number of GPUs used, and an estimate of the sustainedsingle-precision floating-point capacity of each GPU 5.6.2Model VariationsTo evaluate the importance of different components of the Transformer, we varied our base modelin different ways, measuring the change in performance on English-to-German translation on the5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the basemodel.All metrics are on the English-to-German translation development set, newstest2013.Listedperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared toper-word perplexities.trainPPLBLEUparamsNdmodeldffhdkdvPdropϵls×106steps(dev)(dev)base65122048864640.10.1100K4.9225.86515125125.2924.941281285.0025.5(A)1632324.9125.83216165.0125.4165.1625.158(B)325.0125.46026.1123.73645.1925.35084.8825.580(C)25632325.7524.52810241281284.6626.016810245.1225.45340964.7526.2900.05.7724.60.24.9525.5(D)0.04.6725.30.25.4725.7(E)positional embedding instead of sinusoids4.9225.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.", "page_start": 8, "page_end": 9, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.48337233}, {"chunk_id": "0b373c99-8c0e-44bf-8fc7-f6b6b009590d", "text": "Provided proper attribution is provided, Google hereby grants permission toreproduce the tables and figures in this paper solely for use in journalistic orscholarly works.Ashish Vaswani∗Noam Shazeer∗Niki Parmar∗Jakob Uszkoreit∗Google BrainGoogle BrainGoogle ResearchGoogle Researchavaswani@google.comnoam@google.comnikip@google.comusz@google.comLlion Jones∗Aidan N.Gomez∗†Łukasz Kaiser∗Google ResearchUniversity of TorontoGoogle Brainlukaszkaiser@google.comllion@google.comaidan@cs.toronto.eduIllia Polosukhin∗‡illia.polosukhin@gmail.comAbstractThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder.The bestperforming models also connect the encoder and decoder through an attentionmechanism.We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely.Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring significantlyless time to train.Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU.On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.8 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature.We show that the Transformer generalizes well toother tasks by applying it successfully to English constituency parsing both withlarge and limited training data.∗Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and startedthe effort to evaluate this idea.Ashish, with Illia, designed and implemented the first Transformer models andhas been crucially involved in every aspect of this work.Noam proposed scaled dot-product attention, multi-headattention and the parameter-free position representation and became the other person involved in nearly everydetail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase andtensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, andefficient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of andimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks", "page_start": 1, "page_end": 2, "section_title": "", "score": 0.4711917}, {"chunk_id": "58f1c685-b5d0-4ba4-8711-0ba5768f729c", "text": "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,we found it beneficial to linearly project the queries, keys and values h times with different, learnedlinear projections to dk, dk and dv dimensions, respectively.On each of these projected versions ofqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional4To illustrate why the dot products get large, assume that the components of q and k are independent randomvariables with mean 0 and variance 1.Then their dot product, q · k = Pdki=1 qiki, has mean 0 and variance dk.4output values.These are concatenated and once again projected, resulting in the final values, asdepicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representationsubspaces at different positions.With a single attention head, averaging inhibits this.MultiHead(Q, K, V ) = Concat(head1, ..., headh)W Owhere headi = Attention(QW Qi , KW Ki , V W Vi )Where the projections are parameter matrices W Q∈Rdmodel×dk, W V∈Rdmodel×dv∈Rdmodel×dk, W Kiiiand W O ∈Rhdv×dmodel.In this work we employ h = 8 parallel attention layers, or heads.For each of these we usedk = dv = dmodel/h = 64.Due to the reduced dimension of each head, the total computational costis similar to that of single-head attention with full dimensionality.3.2.3Applications of Attention in our ModelThe Transformer uses multi-head attention in three different ways:• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,and the memory keys and values come from the output of the encoder.This allows everyposition in the decoder to attend over all positions in the input sequence.This mimics thetypical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38, 2, 9].• The encoder contains self-attention layers.In a self-attention layer all of the keys, valuesand queries come from the same place, in this case, the output of the previous layer in theencoder.Each position in the encoder can attend to all positions in the previous layer of theencoder.• Similarly, self-attention layers in the decoder allow each position in the decoder to attend toall positions in the decoder up to and including that position.We need to prevent leftwardinformation flow in the decoder to preserve the auto-regressive property.We implement thisinside of scaled dot-product attention by masking out (setting to −∞) all values in the inputof the softmax which correspond to illegal connections.See Figure 2.3.3Position-wise Feed-Forward NetworksIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fullyconnected feed-forward network, which is applied to each position separately and identically.This", "page_start": 4, "page_end": 5, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4586857}, {"chunk_id": "160dd6c9-8965-4dcd-8d74-e0dd157c4fd1", "text": "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step number.We usedwarmup_steps = 4000.5.4RegularizationWe employ three types of regularization during training:7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on theEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.BLEUTraining Cost (FLOPs)ModelEN-DEEN-FREN-DEEN-FRByteNet [18]23.751.0 · 1020Deep-Att + PosUnk [39]39.22.3 · 10191.4 · 1020GNMT + RL [38]24.639.929.6 · 10181.5 · 1020ConvS2S [9]25.1640.462.0 · 10191.2 · 1020MoE [32]26.0340.568.0 · 1020Deep-Att + PosUnk Ensemble [39]40.41.8 · 10201.1 · 1021GNMT + RL Ensemble [38]26.3041.167.7 · 10191.2 · 102141.29ConvS2S Ensemble [9]26.363.3 · 1018Transformer (base model)27.338.12.3 · 101928.441.8Transformer (big)Residual DropoutWe apply dropout [33] to the output of each sub-layer, before it is added to thesub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and thepositional encodings in both the encoder and decoder stacks.For the base model, we use a rate ofPdrop = 0.1.During training, we employed label smoothing of value ϵls = 0.1 [36].ThisLabel Smoothinghurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6Results6.1Machine TranslationOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0BLEU, establishing a new state-of-the-art BLEU score of 28.4.The configuration of this model islisted in the bottom line of Table 3.Training took 3.5 days on 8 P100 GPUs.Even our base modelsurpasses all previously published models and ensembles, at a fraction of the training cost of any ofthe competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,outperforming all of the previously published single models, at less than 1/4 the training cost of theprevious state-of-the-art model.The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We", "page_start": 7, "page_end": 8, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4579233}, {"chunk_id": "5151352b-ae46-44f3-83b0-8b8748547941", "text": "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networksin particular, have been firmly established as state of the art approaches in sequence modeling andtransduction problems such as language modeling and machine translation [35, 2, 5].Numerousefforts have since continued to push the boundaries of recurrent language models and encoder-decoderarchitectures [38, 24, 15].Recurrent models typically factor computation along the symbol positions of the input and outputsequences.Aligning the positions to steps in computation time, they generate a sequence of hiddenstates ht, as a function of the previous hidden state ht−1 and the input for position t.This inherentlysequential nature precludes parallelization within training examples, which becomes critical at longersequence lengths, as memory constraints limit batching across examples.Recent work has achievedsignificant improvements in computational efficiency through factorization tricks [21] and conditionalcomputation [32], while also improving model performance in case of the latter.The fundamentalconstraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance inthe input or output sequences [2, 19].In all but a few cases [27], however, such attention mechanismsare used in conjunction with a recurrent network.In this work we propose the Transformer, a model architecture eschewing recurrence and insteadrelying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for significantly more parallelization and can reach a new state of the art intranslation quality after being trained for as little as twelve hours on eight P100 GPUs.2BackgroundThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic buildingblock, computing hidden representations in parallel for all input and output positions.In these models,the number of operations required to relate signals from two arbitrary input or output positions growsin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.This makesit more difficult to learn dependencies between distant positions [12].In the Transformer this isreduced to a constant number of operations, albeit at the cost of reduced effective resolution dueto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,", "page_start": 1, "page_end": 2, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.45426846}, {"chunk_id": "8d9a2784-c6bd-4675-a20b-80dd0948329f", "text": "25.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,keeping the amount of computation constant, as described in Section 3.2.2.While single-headattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality.Thissuggests that determining compatibility is not easy and that a more sophisticated compatibilityfunction than dot product may be beneficial.We further observe in rows (C) and (D) that, as expected,bigger models are better, and dropout is very helpful in avoiding over-fitting.In row (E) we replace oursinusoidal positional encoding with learned positional embeddings [9], and observe nearly identicalresults to the base model.6.3English Constituency ParsingTo evaluate if the Transformer can generalize to other tasks we performed experiments on Englishconstituency parsing.This task presents specific challenges: the output is subject to strong structuralconstraints and is significantly longer than the input.Furthermore, RNN sequence-to-sequencemodels have not been able to attain state-of-the-art results in small-data regimes [37].We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of thePenn Treebank [25], about 40K training sentences.We also trained it in a semi-supervised setting,using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences[37].We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokensfor the semi-supervised setting.We performed only a small number of experiments to select the dropout, both attention and residual(section 5.4), learning rates and beam size on the Section 22 development set, all other parametersremained unchanged from the English-to-German base translation model.During inference, we9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23of WSJ)ParserTrainingWSJ 23 F1Vinyals & Kaiser el al.(2014) [37]WSJ only, discriminative88.3Petrov et al.(2006) [29]WSJ only, discriminative90.4Zhu et al.(2013) [40]WSJ only, discriminative90.4Dyer et al.(2016) [8]WSJ only, discriminative91.7Transformer (4 layers)WSJ only, discriminative91.3Zhu et al.(2013) [40]semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised", "page_start": 9, "page_end": 10, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.43318275}, {"chunk_id": "bb77d3cc-59f9-4b97-a21b-132b30215259", "text": "semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised92.1Transformer (4 layers)semi-supervised92.7Luong et al.(2015) [23]multi-task93.0Dyer et al.(2016) [8]generative93.3increased the maximum output length to input length + 300.We used a beam size of 21 and α = 0.3for both WSJ only and the semi-supervised setting.Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of theRecurrent Neural Network Grammar [8].In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.7ConclusionIn this work, we presented the Transformer, the first sequence transduction model based entirely onattention, replacing the recurrent layers most commonly used in encoder-decoder architectures withmulti-headed self-attention.For translation tasks, the Transformer can be trained significantly faster than architectures basedon recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014English-to-French translation tasks, we achieve a new state of the art.In the former task our bestmodel outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.Weplan to extend the Transformer to problems involving input and output modalities other than text andto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputssuch as images, audio and video.Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.AcknowledgementsWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitfulcomments, corrections and inspiration.References[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprintarXiv:1607.06450, 2016.[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointlylearning to align and translate.CoRR, abs/1409.0473, 2014.[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V.Le.Massive exploration of neuralmachine translation architectures.CoRR, abs/1703.03906, 2017.[4] Jianpeng Cheng, Li Dong, and Mirella Lapata.Long short-term memory-networks for machinereading.arXiv preprint arXiv:1601.06733, 2016.10[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,", "page_start": 10, "page_end": 11, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4227041}], "multilingual_attention_ja": [{"chunk_id": "914cfb0b-3d4a-4988-8392-851b64c13e57", "text": "The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.Weused beam search with a beam size of 4 and length penalty α = 0.6 [38].These hyperparameterswere chosen after experimentation on the development set.We set the maximum output length duringinference to input length + 50, but terminate early when possible [38].Table 2 summarizes our results and compares our translation quality and training costs to other modelarchitectures from the literature.We estimate the number of floating point operations used to train amodel by multiplying the training time, the number of GPUs used, and an estimate of the sustainedsingle-precision floating-point capacity of each GPU 5.6.2Model VariationsTo evaluate the importance of different components of the Transformer, we varied our base modelin different ways, measuring the change in performance on English-to-German translation on the5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the basemodel.All metrics are on the English-to-German translation development set, newstest2013.Listedperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared toper-word perplexities.trainPPLBLEUparamsNdmodeldffhdkdvPdropϵls×106steps(dev)(dev)base65122048864640.10.1100K4.9225.86515125125.2924.941281285.0025.5(A)1632324.9125.83216165.0125.4165.1625.158(B)325.0125.46026.1123.73645.1925.35084.8825.580(C)25632325.7524.52810241281284.6626.016810245.1225.45340964.7526.2900.05.7724.60.24.9525.5(D)0.04.6725.30.25.4725.7(E)positional embedding instead of sinusoids4.9225.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.", "page_start": 8, "page_end": 9, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.41441312}, {"chunk_id": "6ecb21e7-c073-4d90-9d8a-41fdfda06afb", "text": "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,textual entailment and learning task-independent sentence representations [4, 27, 28, 22].End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering andlanguage modeling tasks [34].To the best of our knowledge, however, the Transformer is the first transduction model relyingentirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.In the following sections, we will describe the Transformer, motivateself-attention and discuss its advantages over models such as [17, 18] and [9].3Model ArchitectureMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequenceof continuous representations z = (z1, ..., zn).Given z, the decoder then generates an outputsequence (y1, ..., ym) of symbols one element at a time.At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next.2Figure 1: The Transformer - model architecture.The Transformer follows this overall architecture using stacked self-attention and point-wise, fullyconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,respectively.3.1Encoder and Decoder StacksThe encoder is composed of a stack of N = 6 identical layers.Each layer has twoEncoder:sub-layers.The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network.We employ a residual connection [11] around each ofthe two sub-layers, followed by layer normalization [1].That is, the output of each sub-layer isLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layeritself.To facilitate these residual connections, all sub-layers in the model, as well as the embeddinglayers, produce outputs of dimension dmodel = 512.The decoder is also composed of a stack of N = 6 identical layers.In addition to the twoDecoder:sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-headattention over the output of the encoder stack.Similar to the encoder, we employ residual connectionsaround each of the sub-layers, followed by layer normalization.We also modify the self-attentionsub-layer in the decoder stack to prevent positions from attending to subsequent positions.Thismasking, combined with fact that the output embeddings are offset by one position, ensures that the", "page_start": 2, "page_end": 3, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4010426}, {"chunk_id": "160dd6c9-8965-4dcd-8d74-e0dd157c4fd1", "text": "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step number.We usedwarmup_steps = 4000.5.4RegularizationWe employ three types of regularization during training:7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on theEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.BLEUTraining Cost (FLOPs)ModelEN-DEEN-FREN-DEEN-FRByteNet [18]23.751.0 · 1020Deep-Att + PosUnk [39]39.22.3 · 10191.4 · 1020GNMT + RL [38]24.639.929.6 · 10181.5 · 1020ConvS2S [9]25.1640.462.0 · 10191.2 · 1020MoE [32]26.0340.568.0 · 1020Deep-Att + PosUnk Ensemble [39]40.41.8 · 10201.1 · 1021GNMT + RL Ensemble [38]26.3041.167.7 · 10191.2 · 102141.29ConvS2S Ensemble [9]26.363.3 · 1018Transformer (base model)27.338.12.3 · 101928.441.8Transformer (big)Residual DropoutWe apply dropout [33] to the output of each sub-layer, before it is added to thesub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and thepositional encodings in both the encoder and decoder stacks.For the base model, we use a rate ofPdrop = 0.1.During training, we employed label smoothing of value ϵls = 0.1 [36].ThisLabel Smoothinghurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6Results6.1Machine TranslationOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0BLEU, establishing a new state-of-the-art BLEU score of 28.4.The configuration of this model islisted in the bottom line of Table 3.Training took 3.5 days on 8 P100 GPUs.Even our base modelsurpasses all previously published models and ensembles, at a fraction of the training cost of any ofthe competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,outperforming all of the previously published single models, at less than 1/4 the training cost of theprevious state-of-the-art model.The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We", "page_start": 7, "page_end": 8, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.37281162}, {"chunk_id": "5151352b-ae46-44f3-83b0-8b8748547941", "text": "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networksin particular, have been firmly established as state of the art approaches in sequence modeling andtransduction problems such as language modeling and machine translation [35, 2, 5].Numerousefforts have since continued to push the boundaries of recurrent language models and encoder-decoderarchitectures [38, 24, 15].Recurrent models typically factor computation along the symbol positions of the input and outputsequences.Aligning the positions to steps in computation time, they generate a sequence of hiddenstates ht, as a function of the previous hidden state ht−1 and the input for position t.This inherentlysequential nature precludes parallelization within training examples, which becomes critical at longersequence lengths, as memory constraints limit batching across examples.Recent work has achievedsignificant improvements in computational efficiency through factorization tricks [21] and conditionalcomputation [32], while also improving model performance in case of the latter.The fundamentalconstraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance inthe input or output sequences [2, 19].In all but a few cases [27], however, such attention mechanismsare used in conjunction with a recurrent network.In this work we propose the Transformer, a model architecture eschewing recurrence and insteadrelying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for significantly more parallelization and can reach a new state of the art intranslation quality after being trained for as little as twelve hours on eight P100 GPUs.2BackgroundThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic buildingblock, computing hidden representations in parallel for all input and output positions.In these models,the number of operations required to relate signals from two arbitrary input or output positions growsin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.This makesit more difficult to learn dependencies between distant positions [12].In the Transformer this isreduced to a constant number of operations, albeit at the cost of reduced effective resolution dueto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,", "page_start": 1, "page_end": 2, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.3681395}, {"chunk_id": "0b373c99-8c0e-44bf-8fc7-f6b6b009590d", "text": "Provided proper attribution is provided, Google hereby grants permission toreproduce the tables and figures in this paper solely for use in journalistic orscholarly works.Ashish Vaswani∗Noam Shazeer∗Niki Parmar∗Jakob Uszkoreit∗Google BrainGoogle BrainGoogle ResearchGoogle Researchavaswani@google.comnoam@google.comnikip@google.comusz@google.comLlion Jones∗Aidan N.Gomez∗†Łukasz Kaiser∗Google ResearchUniversity of TorontoGoogle Brainlukaszkaiser@google.comllion@google.comaidan@cs.toronto.eduIllia Polosukhin∗‡illia.polosukhin@gmail.comAbstractThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder.The bestperforming models also connect the encoder and decoder through an attentionmechanism.We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely.Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring significantlyless time to train.Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU.On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.8 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature.We show that the Transformer generalizes well toother tasks by applying it successfully to English constituency parsing both withlarge and limited training data.∗Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and startedthe effort to evaluate this idea.Ashish, with Illia, designed and implemented the first Transformer models andhas been crucially involved in every aspect of this work.Noam proposed scaled dot-product attention, multi-headattention and the parameter-free position representation and became the other person involved in nearly everydetail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase andtensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, andefficient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of andimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks", "page_start": 1, "page_end": 2, "section_title": "", "score": 0.364723}, {"chunk_id": "8d9a2784-c6bd-4675-a20b-80dd0948329f", "text": "25.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,keeping the amount of computation constant, as described in Section 3.2.2.While single-headattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality.Thissuggests that determining compatibility is not easy and that a more sophisticated compatibilityfunction than dot product may be beneficial.We further observe in rows (C) and (D) that, as expected,bigger models are better, and dropout is very helpful in avoiding over-fitting.In row (E) we replace oursinusoidal positional encoding with learned positional embeddings [9], and observe nearly identicalresults to the base model.6.3English Constituency ParsingTo evaluate if the Transformer can generalize to other tasks we performed experiments on Englishconstituency parsing.This task presents specific challenges: the output is subject to strong structuralconstraints and is significantly longer than the input.Furthermore, RNN sequence-to-sequencemodels have not been able to attain state-of-the-art results in small-data regimes [37].We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of thePenn Treebank [25], about 40K training sentences.We also trained it in a semi-supervised setting,using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences[37].We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokensfor the semi-supervised setting.We performed only a small number of experiments to select the dropout, both attention and residual(section 5.4), learning rates and beam size on the Section 22 development set, all other parametersremained unchanged from the English-to-German base translation model.During inference, we9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23of WSJ)ParserTrainingWSJ 23 F1Vinyals & Kaiser el al.(2014) [37]WSJ only, discriminative88.3Petrov et al.(2006) [29]WSJ only, discriminative90.4Zhu et al.(2013) [40]WSJ only, discriminative90.4Dyer et al.(2016) [8]WSJ only, discriminative91.7Transformer (4 layers)WSJ only, discriminative91.3Zhu et al.(2013) [40]semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised", "page_start": 9, "page_end": 10, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.35949966}, {"chunk_id": "58f1c685-b5d0-4ba4-8711-0ba5768f729c", "text": "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,we found it beneficial to linearly project the queries, keys and values h times with different, learnedlinear projections to dk, dk and dv dimensions, respectively.On each of these projected versions ofqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional4To illustrate why the dot products get large, assume that the components of q and k are independent randomvariables with mean 0 and variance 1.Then their dot product, q · k = Pdki=1 qiki, has mean 0 and variance dk.4output values.These are concatenated and once again projected, resulting in the final values, asdepicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representationsubspaces at different positions.With a single attention head, averaging inhibits this.MultiHead(Q, K, V ) = Concat(head1, ..., headh)W Owhere headi = Attention(QW Qi , KW Ki , V W Vi )Where the projections are parameter matrices W Q∈Rdmodel×dk, W V∈Rdmodel×dv∈Rdmodel×dk, W Kiiiand W O ∈Rhdv×dmodel.In this work we employ h = 8 parallel attention layers, or heads.For each of these we usedk = dv = dmodel/h = 64.Due to the reduced dimension of each head, the total computational costis similar to that of single-head attention with full dimensionality.3.2.3Applications of Attention in our ModelThe Transformer uses multi-head attention in three different ways:• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,and the memory keys and values come from the output of the encoder.This allows everyposition in the decoder to attend over all positions in the input sequence.This mimics thetypical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38, 2, 9].• The encoder contains self-attention layers.In a self-attention layer all of the keys, valuesand queries come from the same place, in this case, the output of the previous layer in theencoder.Each position in the encoder can attend to all positions in the previous layer of theencoder.• Similarly, self-attention layers in the decoder allow each position in the decoder to attend toall positions in the decoder up to and including that position.We need to prevent leftwardinformation flow in the decoder to preserve the auto-regressive property.We implement thisinside of scaled dot-product attention by masking out (setting to −∞) all values in the inputof the softmax which correspond to illegal connections.See Figure 2.3.3Position-wise Feed-Forward NetworksIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fullyconnected feed-forward network, which is applied to each position separately and identically.This", "page_start": 4, "page_end": 5, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.3494336}, {"chunk_id": "bb77d3cc-59f9-4b97-a21b-132b30215259", "text": "semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised92.1Transformer (4 layers)semi-supervised92.7Luong et al.(2015) [23]multi-task93.0Dyer et al.(2016) [8]generative93.3increased the maximum output length to input length + 300.We used a beam size of 21 and α = 0.3for both WSJ only and the semi-supervised setting.Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of theRecurrent Neural Network Grammar [8].In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.7ConclusionIn this work, we presented the Transformer, the first sequence transduction model based entirely onattention, replacing the recurrent layers most commonly used in encoder-decoder architectures withmulti-headed self-attention.For translation tasks, the Transformer can be trained significantly faster than architectures basedon recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014English-to-French translation tasks, we achieve a new state of the art.In the former task our bestmodel outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.Weplan to extend the Transformer to problems involving input and output modalities other than text andto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputssuch as images, audio and video.Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.AcknowledgementsWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitfulcomments, corrections and inspiration.References[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprintarXiv:1607.06450, 2016.[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointlylearning to align and translate.CoRR, abs/1409.0473, 2014.[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V.Le.Massive exploration of neuralmachine translation architectures.CoRR, abs/1703.03906, 2017.[4] Jianpeng Cheng, Li Dong, and Mirella Lapata.Long short-term memory-networks for machinereading.arXiv preprint arXiv:1601.06733, 2016.10[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,", "page_start": 10, "page_end": 11, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.34135935}], "multilingual_nda_zh": [{"chunk_id": "6c64d139-fd3d-400d-b64b-3ddf0dea3473", "text": "except the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.The English Courts will have non-exclusive jurisdiction to deal with any dispute whichhas arisen or may arise out of, or in connection with, this Agreement.Signed [by [insert name]] OR [on behalf of][insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________PositionSigned [by [insert name]] OR [on behalf of] [insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________Position", "page_start": 2, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.5223676}, {"chunk_id": "94d954fb-950b-4dcf-8049-c72deb8243c9", "text": "Date: 201[ ]Parties:[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]and[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]1.Each of the parties to this Agreement intends to disclose information (the ConfidentialInformation) to the other party for the purpose of [insert details e.g.discussing thepossibility of the parties entering into a joint venture] (the Purpose).2.Each party to this Agreement is referred to as ‘the Recipient’ when it receives or usesthe Confidential Information disclosed by the other party.3.The Recipient undertakes not to use the Confidential Information disclosed by the otherparty for any purpose except the Purpose, without first obtaining the written agreementof the other party.4.The Recipient undertakes to keep the Confidential Information disclosed by the otherparty secure and not to disclose it to any third party [except to its employees [andprofessional advisers] who need to know the same for the Purpose, who know theyowe a duty of confidence to the other party and who are bound by obligationsequivalent to those in clause 3 above and this clause 4.5.The undertakings in clauses 3 and 4 above apply to all of the information disclosed byeach of the parties to the other, regardless of the way or form in which it is disclosed orrecorded but they do not apply to:a) any information which is or in future comes into the public domain (unless as a result ofthe breach of this Agreement); orb) any information which is already known to the Recipient and which was not subject toany obligation of confidence before it was disclosed to the Recipient by the other party.6.Nothing in this Agreement will prevent the Recipient from making any disclosure of theConfidential Information required by law or by any competent authority.7.The Recipient will, on request from the other party, return all copies and records of theConfidential Information disclosed by the other party to the Recipient and will not retainany copies or records of the Confidential Information disclosed by the other party.8.Neither this Agreement nor the supply of any information grants the Recipient anylicence, interest or right in respect of any intellectual property rights of the other partyexcept the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.", "page_start": 1, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.4677302}], "table_attention_01": [{"chunk_id": "914cfb0b-3d4a-4988-8392-851b64c13e57", "text": "The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.Weused beam search with a beam size of 4 and length penalty α = 0.6 [38].These hyperparameterswere chosen after experimentation on the development set.We set the maximum output length duringinference to input length + 50, but terminate early when possible [38].Table 2 summarizes our results and compares our translation quality and training costs to other modelarchitectures from the literature.We estimate the number of floating point operations used to train amodel by multiplying the training time, the number of GPUs used, and an estimate of the sustainedsingle-precision floating-point capacity of each GPU 5.6.2Model VariationsTo evaluate the importance of different components of the Transformer, we varied our base modelin different ways, measuring the change in performance on English-to-German translation on the5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the basemodel.All metrics are on the English-to-German translation development set, newstest2013.Listedperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared toper-word perplexities.trainPPLBLEUparamsNdmodeldffhdkdvPdropϵls×106steps(dev)(dev)base65122048864640.10.1100K4.9225.86515125125.2924.941281285.0025.5(A)1632324.9125.83216165.0125.4165.1625.158(B)325.0125.46026.1123.73645.1925.35084.8825.580(C)25632325.7524.52810241281284.6626.016810245.1225.45340964.7526.2900.05.7724.60.24.9525.5(D)0.04.6725.30.25.4725.7(E)positional embedding instead of sinusoids4.9225.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.", "page_start": 8, "page_end": 9, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.62178993}, {"chunk_id": "160dd6c9-8965-4dcd-8d74-e0dd157c4fd1", "text": "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step number.We usedwarmup_steps = 4000.5.4RegularizationWe employ three types of regularization during training:7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on theEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.BLEUTraining Cost (FLOPs)ModelEN-DEEN-FREN-DEEN-FRByteNet [18]23.751.0 · 1020Deep-Att + PosUnk [39]39.22.3 · 10191.4 · 1020GNMT + RL [38]24.639.929.6 · 10181.5 · 1020ConvS2S [9]25.1640.462.0 · 10191.2 · 1020MoE [32]26.0340.568.0 · 1020Deep-Att + PosUnk Ensemble [39]40.41.8 · 10201.1 · 1021GNMT + RL Ensemble [38]26.3041.167.7 · 10191.2 · 102141.29ConvS2S Ensemble [9]26.363.3 · 1018Transformer (base model)27.338.12.3 · 101928.441.8Transformer (big)Residual DropoutWe apply dropout [33] to the output of each sub-layer, before it is added to thesub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and thepositional encodings in both the encoder and decoder stacks.For the base model, we use a rate ofPdrop = 0.1.During training, we employed label smoothing of value ϵls = 0.1 [36].ThisLabel Smoothinghurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6Results6.1Machine TranslationOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0BLEU, establishing a new state-of-the-art BLEU score of 28.4.The configuration of this model islisted in the bottom line of Table 3.Training took 3.5 days on 8 P100 GPUs.Even our base modelsurpasses all previously published models and ensembles, at a fraction of the training cost of any ofthe competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,outperforming all of the previously published single models, at less than 1/4 the training cost of theprevious state-of-the-art model.The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We", "page_start": 7, "page_end": 8, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.5934064}, {"chunk_id": "8d9a2784-c6bd-4675-a20b-80dd0948329f", "text": "25.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,keeping the amount of computation constant, as described in Section 3.2.2.While single-headattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality.Thissuggests that determining compatibility is not easy and that a more sophisticated compatibilityfunction than dot product may be beneficial.We further observe in rows (C) and (D) that, as expected,bigger models are better, and dropout is very helpful in avoiding over-fitting.In row (E) we replace oursinusoidal positional encoding with learned positional embeddings [9], and observe nearly identicalresults to the base model.6.3English Constituency ParsingTo evaluate if the Transformer can generalize to other tasks we performed experiments on Englishconstituency parsing.This task presents specific challenges: the output is subject to strong structuralconstraints and is significantly longer than the input.Furthermore, RNN sequence-to-sequencemodels have not been able to attain state-of-the-art results in small-data regimes [37].We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of thePenn Treebank [25], about 40K training sentences.We also trained it in a semi-supervised setting,using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences[37].We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokensfor the semi-supervised setting.We performed only a small number of experiments to select the dropout, both attention and residual(section 5.4), learning rates and beam size on the Section 22 development set, all other parametersremained unchanged from the English-to-German base translation model.During inference, we9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23of WSJ)ParserTrainingWSJ 23 F1Vinyals & Kaiser el al.(2014) [37]WSJ only, discriminative88.3Petrov et al.(2006) [29]WSJ only, discriminative90.4Zhu et al.(2013) [40]WSJ only, discriminative90.4Dyer et al.(2016) [8]WSJ only, discriminative91.7Transformer (4 layers)WSJ only, discriminative91.3Zhu et al.(2013) [40]semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised", "page_start": 9, "page_end": 10, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.5208494}, {"chunk_id": "0b373c99-8c0e-44bf-8fc7-f6b6b009590d", "text": "Provided proper attribution is provided, Google hereby grants permission toreproduce the tables and figures in this paper solely for use in journalistic orscholarly works.Ashish Vaswani∗Noam Shazeer∗Niki Parmar∗Jakob Uszkoreit∗Google BrainGoogle BrainGoogle ResearchGoogle Researchavaswani@google.comnoam@google.comnikip@google.comusz@google.comLlion Jones∗Aidan N.Gomez∗†Łukasz Kaiser∗Google ResearchUniversity of TorontoGoogle Brainlukaszkaiser@google.comllion@google.comaidan@cs.toronto.eduIllia Polosukhin∗‡illia.polosukhin@gmail.comAbstractThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder.The bestperforming models also connect the encoder and decoder through an attentionmechanism.We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely.Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring significantlyless time to train.Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU.On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.8 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature.We show that the Transformer generalizes well toother tasks by applying it successfully to English constituency parsing both withlarge and limited training data.∗Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and startedthe effort to evaluate this idea.Ashish, with Illia, designed and implemented the first Transformer models andhas been crucially involved in every aspect of this work.Noam proposed scaled dot-product attention, multi-headattention and the parameter-free position representation and became the other person involved in nearly everydetail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase andtensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, andefficient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of andimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks", "page_start": 1, "page_end": 2, "section_title": "", "score": 0.5122125}, {"chunk_id": "6ecb21e7-c073-4d90-9d8a-41fdfda06afb", "text": "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,textual entailment and learning task-independent sentence representations [4, 27, 28, 22].End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering andlanguage modeling tasks [34].To the best of our knowledge, however, the Transformer is the first transduction model relyingentirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.In the following sections, we will describe the Transformer, motivateself-attention and discuss its advantages over models such as [17, 18] and [9].3Model ArchitectureMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequenceof continuous representations z = (z1, ..., zn).Given z, the decoder then generates an outputsequence (y1, ..., ym) of symbols one element at a time.At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next.2Figure 1: The Transformer - model architecture.The Transformer follows this overall architecture using stacked self-attention and point-wise, fullyconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,respectively.3.1Encoder and Decoder StacksThe encoder is composed of a stack of N = 6 identical layers.Each layer has twoEncoder:sub-layers.The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network.We employ a residual connection [11] around each ofthe two sub-layers, followed by layer normalization [1].That is, the output of each sub-layer isLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layeritself.To facilitate these residual connections, all sub-layers in the model, as well as the embeddinglayers, produce outputs of dimension dmodel = 512.The decoder is also composed of a stack of N = 6 identical layers.In addition to the twoDecoder:sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-headattention over the output of the encoder stack.Similar to the encoder, we employ residual connectionsaround each of the sub-layers, followed by layer normalization.We also modify the self-attentionsub-layer in the decoder stack to prevent positions from attending to subsequent positions.Thismasking, combined with fact that the output embeddings are offset by one position, ensures that the", "page_start": 2, "page_end": 3, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.49205935}, {"chunk_id": "bb77d3cc-59f9-4b97-a21b-132b30215259", "text": "semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised92.1Transformer (4 layers)semi-supervised92.7Luong et al.(2015) [23]multi-task93.0Dyer et al.(2016) [8]generative93.3increased the maximum output length to input length + 300.We used a beam size of 21 and α = 0.3for both WSJ only and the semi-supervised setting.Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of theRecurrent Neural Network Grammar [8].In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.7ConclusionIn this work, we presented the Transformer, the first sequence transduction model based entirely onattention, replacing the recurrent layers most commonly used in encoder-decoder architectures withmulti-headed self-attention.For translation tasks, the Transformer can be trained significantly faster than architectures basedon recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014English-to-French translation tasks, we achieve a new state of the art.In the former task our bestmodel outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.Weplan to extend the Transformer to problems involving input and output modalities other than text andto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputssuch as images, audio and video.Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.AcknowledgementsWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitfulcomments, corrections and inspiration.References[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprintarXiv:1607.06450, 2016.[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointlylearning to align and translate.CoRR, abs/1409.0473, 2014.[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V.Le.Massive exploration of neuralmachine translation architectures.CoRR, abs/1703.03906, 2017.[4] Jianpeng Cheng, Li Dong, and Mirella Lapata.Long short-term memory-networks for machinereading.arXiv preprint arXiv:1601.06733, 2016.10[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,", "page_start": 10, "page_end": 11, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.48599613}, {"chunk_id": "5151352b-ae46-44f3-83b0-8b8748547941", "text": "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networksin particular, have been firmly established as state of the art approaches in sequence modeling andtransduction problems such as language modeling and machine translation [35, 2, 5].Numerousefforts have since continued to push the boundaries of recurrent language models and encoder-decoderarchitectures [38, 24, 15].Recurrent models typically factor computation along the symbol positions of the input and outputsequences.Aligning the positions to steps in computation time, they generate a sequence of hiddenstates ht, as a function of the previous hidden state ht−1 and the input for position t.This inherentlysequential nature precludes parallelization within training examples, which becomes critical at longersequence lengths, as memory constraints limit batching across examples.Recent work has achievedsignificant improvements in computational efficiency through factorization tricks [21] and conditionalcomputation [32], while also improving model performance in case of the latter.The fundamentalconstraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance inthe input or output sequences [2, 19].In all but a few cases [27], however, such attention mechanismsare used in conjunction with a recurrent network.In this work we propose the Transformer, a model architecture eschewing recurrence and insteadrelying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for significantly more parallelization and can reach a new state of the art intranslation quality after being trained for as little as twelve hours on eight P100 GPUs.2BackgroundThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic buildingblock, computing hidden representations in parallel for all input and output positions.In these models,the number of operations required to relate signals from two arbitrary input or output positions growsin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.This makesit more difficult to learn dependencies between distant positions [12].In the Transformer this isreduced to a constant number of operations, albeit at the cost of reduced effective resolution dueto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,", "page_start": 1, "page_end": 2, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.46098182}, {"chunk_id": "92be8a85-80ac-4327-a0b3-f2fff684bda4", "text": "the input sequence centered around the respective output position.This would increase the maximumpath length to O(n/r).We plan to investigate this approach further in future work.A single convolutional layer with kernel width k < n does not connect all pairs of input and outputpositions.Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest pathsbetween any two positions in the network.Convolutional layers are generally more expensive thanrecurrent layers, by a factor of k.Separable convolutions [6], however, decrease the complexityconsiderably, to O(k · n · d + n · d2).Even with k = n, however, the complexity of a separableconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,the approach we take in our model.As side benefit, self-attention could yield more interpretable models.We inspect attention distributionsfrom our models and present and discuss examples in the appendix.Not only do individual attentionheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntacticand semantic structure of the sentences.5TrainingThis section describes the training regime for our models.5.1Training Data and BatchingWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 millionsentence pairs.Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens.For English-French, we used the significantly larger WMT2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piecevocabulary [38].Sentence pairs were batched together by approximate sequence length.Each trainingbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000target tokens.5.2Hardware and ScheduleWe trained our models on one machine with 8 NVIDIA P100 GPUs.For our base models usingthe hyperparameters described throughout the paper, each training step took about 0.4 seconds.Wetrained the base models for a total of 100,000 steps or 12 hours.For our big models,(described on thebottom line of table 3), step time was 1.0 seconds.The big models were trained for 300,000 steps(3.5 days).5.3OptimizerWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9.We varied the learningrate over the course of training, according to the formula:lrate = d−0.5model · min(step_num−0.5, step_num · warmup_steps−1.5)(3)This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step number.We usedwarmup_steps = 4000.5.4RegularizationWe employ three types of regularization during training:7", "page_start": 7, "page_end": 7, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.45276508}], "table_nvidia_01": [{"chunk_id": "823438dd-9ce8-4ae9-924a-70c6bbf5121f", "text": "Form 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38The following table sets forth, for the periods indicated, certain items in our Consolidated Statements of Income expressed as a percentage of revenue.Year EndedJan 28, 2024Jan 29, 2023Revenue100.0 %100.0 %Cost of revenue27.343.1Gross profit72.756.9Operating expensesResearch and development14.227.2Sales, general and administrative4.49.1Acquisition termination cost—5.0Total operating expenses18.641.3Operating income54.115.6Interest income1.41.0Interest expense(0.4)(1.0)0.4(0.1)Other, net1.4(0.1)Other income (expense), netIncome before income tax55.515.5Income tax expense (benefit)6.6(0.7)48.9 %16.2 %Net incomeReportable SegmentsRevenue by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$47,405$15,068$32,337215 %13,51711,9061,611Graphics14 %$60,922$26,974$33,948Total126 %Operating Income by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$32,016$5,083$26,933530 %Graphics5,8464,5521,29428 %(4,890)(5,411)521All Other(10)%$32,972$4,224$28,748Total681 %Compute & Networking revenue – The year-on-year increase was due to higher Data Center revenue.Compute grew 266% due to higher shipments of theNVIDIA Hopper GPU computing platform for the training and inference of LLMs, recommendation engines and generative AI applications.Networking was up133% due to higher shipments of InfiniBand.Graphics revenue – The year-on-year increase was led by growth in Gaming of 15% driven by higher sell-in to partners following the normalization of channelinventory levels.Reportable segment operating income – The year-on-year increase in Compute & Networking and Graphics operating income was driven by higher revenue.39All Other operating loss - The year-on-year decrease was due to the $1.4 billion Arm acquisition termination cost in fiscal year 2023, partially offset by a $839million increase in stock-based compensation expense in fiscal year 2024.Concentration of Revenue", "page_start": 38, "page_end": 40, "section_title": "NVIDIA CORPORATION", "score": 0.6605828}, {"chunk_id": "f2207e14-4c6c-421b-849b-76359737cca1", "text": "Germany, India, Israel, and Taiwan for fiscal years 2005 through 2023.Note 15 - Shareholders’ EquityCapital Return ProgramIn August 2023, our Board of Directors approved an increase to our share repurchase program of an additional $25.0 billion, without expiration.During fiscalyear 2024, we repurchased 21 million shares of our common stock for $9.7 billion.As of January 28, 2024, we were authorized, subject to certain specifications,to repurchase additional shares of our common stock up to $22.5 billion.From January 29, 2024 through February 16, 2024, we repurchased 2.8 million sharesfor $1.9 billion pursuant to a Rule 10b5-1 trading plan.Our share repurchase program aims to offset dilution from shares issued to employees.We may pursueadditional share repurchases as we weigh market factors and other investment opportunities.During fiscal years 2024, 2023, and 2022, we paid $395 million, $398 million, and $399 million in cash dividends to our shareholders, respectively.Our cashdividend program and the payment of future cash dividends under that program are subject to our Board of Directors' continuing determination that the dividendprogram and the declaration of dividends thereunder are in the best interests of our shareholders.In fiscal year 2022, we retired our existing 349 million treasury shares.These shares assumed the status of authorized and unissued shares upon retirement.The excess of repurchase price over par value was allocated between additional paid-in capital and retained earnings, resulting in a reduction in additional paidin capital by $20 million and retained earnings by $12.0 billion.Any future repurchased shares will assume the status of authorized and unissued shares.77NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Note 16 - Employee Retirement PlansWe provide tax-qualified defined contribution plans to eligible employees in the U.S.and certain other countries.Our contribution expense for fiscal years 2024,2023, and 2022 was $255 million, $227 million, and $168 million, respectively.Note 17 - Segment InformationOur Chief Executive Officer, who is considered to be our chief operating decision maker, or CODM, reviews financial information presented on an operatingsegment basis for purposes of making decisions and assessing financial performance.The Compute & Networking segment includes our Data Center accelerated computing platform; networking; automotive artificial intelligence, or AI, Cockpit,autonomous driving development agreements, and autonomous vehicle solutions; electric vehicle computing platforms; Jetson for robotics and other embeddedplatforms; NVIDIA AI Enterprise and other software; and DGX Cloud.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions forgaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across our", "page_start": 77, "page_end": 78, "section_title": "NVIDIA CORPORATION", "score": 0.6274432}, {"chunk_id": "8d7f8306-8d55-40f5-b383-f4914775e7ee", "text": "(10)Restructuring costs and other—(54)—Acquisition termination cost—(1,353)—10(2)—Other$(4,890)$(5,411)$(3,049)TotalRevenue by geographic areas is designated based upon the billing location of the customer.End customer location may be different than our customer’s billinglocation.Revenue by geographic areas was as follows:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Revenue:(In millions)United States$26,966$8,292$4,349Taiwan13,4056,9868,544China (including Hong Kong)10,3065,7857,11110,2455,9116,910Other countries$60,922$26,974$26,914Total revenueRevenue from sales to customers outside of the United States accounted for 56%, 69%, and 84% of total revenue for fiscal years 2024, 2023, and 2022,respectively.The increase in revenue to the United States for fiscal year 2024 was primarily due to higher U.S.-based Compute & Networking segment demand.Sales to one customer represented 13% of total revenue for fiscal year 2024, which was attributable to the Compute & Networking segment.No customerrepresented 10% or more of total revenue for fiscal years 2023 and 2022.The following table summarizes information pertaining to our revenue by each of the specialized markets we serve:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Revenue:(In millions)Data Center$47,525$15,005$10,613Gaming10,4479,06712,462Professional Visualization1,5531,5442,111Automotive1,0919035663064551,162OEM and Other$60,922$26,974$26,914Total revenue79NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)The following table presents summarized information for long-lived assets by country.Long-lived assets consist of property and equipment and exclude otherassets, operating lease assets, goodwill, and intangible assets.Jan 28, 2024Jan 29, 2023Long-lived assets:(In millions)United States$2,595$2,587Taiwan773702Israel325283Other countries221235$3,914$3,807Total long-lived assets80NVIDIA Corporation and SubsidiariesSchedule II – Valuation and Qualifying AccountsBalance atBeginning ofBalance atDescriptionPeriodAdditionsDeductionsEnd of Period(In millions)Fiscal year 2024Allowance for doubtful accounts$4$— (1)$— (1)$4Sales return allowance$26$213 (2)$", "page_start": 79, "page_end": 81, "section_title": "NVIDIA CORPORATION", "score": 0.60525155}, {"chunk_id": "a39adb5c-8c62-4daa-b471-d64ac0bed3a6", "text": "gaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across ourunified architecture and therefore allocated between our two segments.The “All Other” category includes the expenses that our CODM does not assign to either Compute & Networking or Graphics for purposes of making operatingdecisions or assessing financial performance.The expenses include stock-based compensation expense, corporate infrastructure and support costs, acquisitionrelated and other costs, intellectual property related, or IP-related costs, acquisition termination cost, and other non-recurring charges and benefits that ourCODM deems to be enterprise in nature.Our CODM does not review any information regarding total assets on a reportable segment basis.Depreciation and amortization expense directly attributable toeach reportable segment is included in operating results for each segment.However, our CODM does not evaluate depreciation and amortization expense byoperating segment and, therefore, it is not separately presented.There is no intersegment revenue.The accounting policies for segment reporting are the sameas for our consolidated financial statements.The table below presents details of our reportable segments and the “All Other” category.Compute &NetworkingGraphicsAll OtherConsolidated(In millions)Year Ended Jan 28, 2024:Revenue$47,405$13,517$—$60,922Operating income (loss)$32,016$5,846$(4,890)$32,972Year Ended Jan 29, 2023:Revenue$15,068$11,906$—$26,974Operating income (loss)$5,083$4,552$(5,411)$4,224Year Ended Jan 30, 2022:Revenue$11,046$15,868$—$26,914Operating income (loss)$4,598$8,492$(3,049)$10,04178NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022(In millions)Reconciling items included in \"All Other\" category:Stock-based compensation expense$(3,549)$(2,710)$(2,004)Unallocated cost of revenue and operating expenses(728)(595)(399)Acquisition-related and other costs(583)(674)(636)IP-related and legal settlement costs(40)(23)(10)Restructuring costs and other—(54)—Acquisition termination cost—(1,353)—10(2)—Other$(4,890)$(5,411)$(3,049)TotalRevenue by geographic areas is designated based upon the billing location of the customer.", "page_start": 78, "page_end": 79, "section_title": "NVIDIA CORPORATION", "score": 0.6014182}, {"chunk_id": "16846335-f2af-4650-bf60-d4b5485698d4", "text": "Principles of ConsolidationOur consolidated financial statements include the accounts of NVIDIA Corporation and our wholly-owned subsidiaries.All intercompany balances andtransactions have been eliminated in consolidation.Use of EstimatesThe preparation of financial statements in conformity with U.S.GAAP requires management to make estimates and assumptions that affect the reportedamounts of assets and liabilities and disclosures of contingent assets and liabilities at the date of the financial statements and the reported amounts of revenueand expenses during the reporting period.Actual results could differ materially from our estimates.On an on-going basis, we evaluate our estimates, includingthose related to revenue recognition, cash equivalents and marketable securities, accounts receivable, inventories and product purchase commitments, incometaxes, goodwill, stock-based compensation, litigation, investigation and settlement costs, restructuring and other charges, property, plant, and equipment, andother contingencies.These estimates are based on historical facts and various other assumptions that we believe are reasonable.In February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of most of our server, storage, and network equipment from three to four or five years, and our assembly and test equipment from five toseven years.The effect of this change for the fiscal year ended January 28, 2024 was a benefit of $33 million and $102 million for cost of revenue and operatingexpenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or $0.05 per both basic anddiluted share.Revenue RecognitionWe derive our revenue from product sales, including hardware and systems, license and development arrangements, software licensing, and cloud services.Wedetermine revenue recognition through the following steps: (1) identification of the contract with a customer; (2) identification of the performance obligations inthe contract; (3) determination of the transaction price; (4) allocation of the transaction price to the performance obligations in the contract (where revenue isallocated on a relative standalone selling price basis by maximizing the use of observable inputs to determine the standalone selling price for each performanceobligation); and (5) recognition of revenue when, or as, we satisfy a performance obligation.Product Sales RevenueRevenue from product sales is recognized upon transfer of control of products to customers in an amount that reflects the consideration we expect to receive inexchange for those products.Certain products are sold with support or an extended warranty for the incorporated system, hardware, and/or software.Supportand extended warranty revenue are recognized ratably over the service period, or as services are performed.Revenue is recognized net of allowances forreturns, customer programs and any taxes collected from customers.For products sold with a right of return, we record a reduction to revenue by establishing a sales return allowance for estimated product returns at the timerevenue is recognized, based primarily on historical return rates.", "page_start": 55, "page_end": 55, "section_title": "NVIDIA CORPORATION", "score": 0.58349127}, {"chunk_id": "ddc05d6e-79f6-4ed4-a29b-876d9e76fd48", "text": "We did not experience any significant impact or expense toour business; however, if the conflict is further extended, it could impact future product development, operations, and revenue or create other uncertainty for ourbusiness.35Fiscal Year 2024 SummaryYear EndedJan 28, 2024Jan 29, 2023Change($ in millions, except per share data)Revenue$60,922$26,974Up 126%Gross margin72.7 %56.9 %Up 15.8 ptsOperating expenses$11,329$11,132Up 2%Operating income$32,972$4,224Up 681%Net income$29,760$4,368Up 581%Net income per diluted share$11.93$1.74Up 586%We specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Revenue for fiscal year 2024 was $60.9 billion, up 126% from a year ago.Data Center revenue for fiscal year 2024 was up 217%.Strong demand was driven by enterprise software and consumer internet applications, and multipleindustry verticals including automotive, financial services, and healthcare.Customers across industry verticals access NVIDIA AI infrastructure both through thecloud and on-premises.Data Center compute revenue was up 244% in the fiscal year.Networking revenue was up 133% in the fiscal year.Gaming revenue for fiscal year 2024 was up 15%.The increase reflects higher sell-in to partners following the normalization of channel inventory levels andgrowing demand.Professional Visualization revenue for fiscal year 2024 was up 1%.Automotive revenue for the fiscal year 2024 was up 21%.The increase primarily reflected growth in self-driving platforms.Gross margin increased in fiscal year 2024, primarily driven by Data Center revenue growth and lower net inventory provisions as a percentage of revenue.Operating expenses increased for fiscal year 2024, driven by growth in employees and compensation increases.Fiscal year 2023 also included a $1.4 billionacquisition termination charge related to the proposed Arm transaction.Market Platform HighlightsData Center revenue for fiscal year 2024 was $47.5 billion, up 217% from fiscal year 2023.In Data Center, we launched AI inference platforms that combine ourfull-stack inference software with NVIDIA Ada, NVIDIA Hopper and NVIDIA Grace Hopper processors optimized for generative AI, LLMs and other AI workloads.We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.", "page_start": 35, "page_end": 36, "section_title": "NVIDIA CORPORATION", "score": 0.56558025}, {"chunk_id": "c69190e8-be56-40d7-9e3f-a1b1a8224354", "text": "Estimates for customer program accrualsinclude a combination of historical attainment and claim rates and may be adjusted based on relevant internal and external factors.License and Development ArrangementsRevenue from License and Development Arrangements is recognized over the period in which the development services are performed.Each fiscal reportingperiod, we measure progress to completion based on actual cost incurred to date as a percentage of the estimated total cost required to complete each project.Estimated total cost for each project includes a forecast of internal engineer personnel time expected to be incurred and other third-party costs as applicable.Contracts with Multiple Performance ObligationsOur contracts may contain more than one performance obligation.Judgement is required in determining whether each performance obligation within a customercontract is distinct.Except for License and Development Arrangements, NVIDIA products and services function on a standalone basis and do not require asignificant amount of integration or interdependency.Therefore, multiple performance obligations contained within a customer contract are considered distinctand are not combined for revenue recognition purposes.We allocate the total transaction price to each distinct performance obligation in a multiple performance obligations arrangement on a relative standalone sellingprice basis.In certain cases, we can establish standalone selling price based on directly observable prices of products or services sold separately in comparablecircumstances to similar customers.If standalone selling price is not directly observable, such as when we do not sell a product or service separately, wedetermine standalone selling price based on market data and other observable inputs.Change in Accounting EstimateIn February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of a majority of the server, storage, and network equipment from three years to a range of four to five years, and assembly and testequipment from five years to seven years.The estimated effect of this change for fiscal year 2024 was a benefit of $33 million and $102 million for cost ofrevenue and operating expenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or$0.05 per both basic and diluted share.Results of OperationsA discussion regarding our financial condition and results of operations for fiscal year 2024 compared to fiscal year 2023 is presented below.A discussionregarding our financial condition and results of operations for fiscal year 2023 compared to fiscal year 2022 can be found under Item 7 in our Annual Report onForm 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38", "page_start": 38, "page_end": 38, "section_title": "NVIDIA CORPORATION", "score": 0.56005716}, {"chunk_id": "0dd7fcd2-abcb-4469-bfdc-d6d5c6cb8eae", "text": "*$100 invested on 1/27/19 in stock and in indices, including reinvestment of dividends.Source: FactSet financial data and analytics.1/27/20191/26/20201/31/20211/30/20221/29/20231/28/2024NVIDIA Corporation$100.00$157.02$326.26$574.15$512.40$1,536.28S&P 500$100.00$126.17$144.83$175.25$163.63$199.83Nasdaq 100$100.00$136.15$194.20$218.68$185.67$268.13Item 6.[Reserved]33Item 7.Management's Discussion and Analysis of Financial Condition and Results of OperationsThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with “Item 1A.Risk Factors”, ourConsolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described elsewhere in this Annual Report onForm 10-K, before deciding to purchase, hold or sell shares of our common stock.OverviewOur Company and Our BusinessesNVIDIA pioneered accelerated computing to help solve the most challenging computational problems.Since our original focus on PC graphics, we haveexpanded to several other large and important computationally intensive fields.NVIDIA has leveraged its GPU architecture to create platforms for acceleratedcomputing, AI solutions, scientific computing, data science, AV, robotics, metaverse and 3D internet applications.Our two operating segments are \"Compute & Networking\" and \"Graphics.\" Refer to Note 17 of the Notes to the Consolidated Financial Statements in Part IV,Item 15 of this Annual Report on Form 10-K for additional information.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Recent Developments, Future Objectives and ChallengesDemand and Supply, Product Transitions, and New Products and Business ModelsDemand for our data center systems and products surged in fiscal year 2024.Entering fiscal year 2025, we are gathering customer demand indications acrossseveral product transitions.We have demand visibility for our new data center products ramping later in fiscal year 2025.We have increased our supply andcapacity purchases with existing suppliers, added new vendors and entered into prepaid manufacturing and capacity agreements.These increased purchasevolumes, the number of suppliers, and the integration of new vendors into our supply chain may create more complexity and execution risk.Our purchasecommitments and obligations for inventory and manufacturing capacity at the end of fiscal year 2024 were impacted by shortening lead times for certaincomponents.We may continue to enter into new supplier and capacity arrangements.Supply of Hopper architecture products is improving, and demand remainsvery strong.We expect our next-generation products to be supply-constrained based upon demand indications.", "page_start": 33, "page_end": 34, "section_title": "NVIDIA CORPORATION", "score": 0.5550747}], "table_nvidia_02": [{"chunk_id": "823438dd-9ce8-4ae9-924a-70c6bbf5121f", "text": "Form 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38The following table sets forth, for the periods indicated, certain items in our Consolidated Statements of Income expressed as a percentage of revenue.Year EndedJan 28, 2024Jan 29, 2023Revenue100.0 %100.0 %Cost of revenue27.343.1Gross profit72.756.9Operating expensesResearch and development14.227.2Sales, general and administrative4.49.1Acquisition termination cost—5.0Total operating expenses18.641.3Operating income54.115.6Interest income1.41.0Interest expense(0.4)(1.0)0.4(0.1)Other, net1.4(0.1)Other income (expense), netIncome before income tax55.515.5Income tax expense (benefit)6.6(0.7)48.9 %16.2 %Net incomeReportable SegmentsRevenue by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$47,405$15,068$32,337215 %13,51711,9061,611Graphics14 %$60,922$26,974$33,948Total126 %Operating Income by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$32,016$5,083$26,933530 %Graphics5,8464,5521,29428 %(4,890)(5,411)521All Other(10)%$32,972$4,224$28,748Total681 %Compute & Networking revenue – The year-on-year increase was due to higher Data Center revenue.Compute grew 266% due to higher shipments of theNVIDIA Hopper GPU computing platform for the training and inference of LLMs, recommendation engines and generative AI applications.Networking was up133% due to higher shipments of InfiniBand.Graphics revenue – The year-on-year increase was led by growth in Gaming of 15% driven by higher sell-in to partners following the normalization of channelinventory levels.Reportable segment operating income – The year-on-year increase in Compute & Networking and Graphics operating income was driven by higher revenue.39All Other operating loss - The year-on-year decrease was due to the $1.4 billion Arm acquisition termination cost in fiscal year 2023, partially offset by a $839million increase in stock-based compensation expense in fiscal year 2024.Concentration of Revenue", "page_start": 38, "page_end": 40, "section_title": "NVIDIA CORPORATION", "score": 0.743091}, {"chunk_id": "ddc05d6e-79f6-4ed4-a29b-876d9e76fd48", "text": "We did not experience any significant impact or expense toour business; however, if the conflict is further extended, it could impact future product development, operations, and revenue or create other uncertainty for ourbusiness.35Fiscal Year 2024 SummaryYear EndedJan 28, 2024Jan 29, 2023Change($ in millions, except per share data)Revenue$60,922$26,974Up 126%Gross margin72.7 %56.9 %Up 15.8 ptsOperating expenses$11,329$11,132Up 2%Operating income$32,972$4,224Up 681%Net income$29,760$4,368Up 581%Net income per diluted share$11.93$1.74Up 586%We specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Revenue for fiscal year 2024 was $60.9 billion, up 126% from a year ago.Data Center revenue for fiscal year 2024 was up 217%.Strong demand was driven by enterprise software and consumer internet applications, and multipleindustry verticals including automotive, financial services, and healthcare.Customers across industry verticals access NVIDIA AI infrastructure both through thecloud and on-premises.Data Center compute revenue was up 244% in the fiscal year.Networking revenue was up 133% in the fiscal year.Gaming revenue for fiscal year 2024 was up 15%.The increase reflects higher sell-in to partners following the normalization of channel inventory levels andgrowing demand.Professional Visualization revenue for fiscal year 2024 was up 1%.Automotive revenue for the fiscal year 2024 was up 21%.The increase primarily reflected growth in self-driving platforms.Gross margin increased in fiscal year 2024, primarily driven by Data Center revenue growth and lower net inventory provisions as a percentage of revenue.Operating expenses increased for fiscal year 2024, driven by growth in employees and compensation increases.Fiscal year 2023 also included a $1.4 billionacquisition termination charge related to the proposed Arm transaction.Market Platform HighlightsData Center revenue for fiscal year 2024 was $47.5 billion, up 217% from fiscal year 2023.In Data Center, we launched AI inference platforms that combine ourfull-stack inference software with NVIDIA Ada, NVIDIA Hopper and NVIDIA Grace Hopper processors optimized for generative AI, LLMs and other AI workloads.We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.", "page_start": 35, "page_end": 36, "section_title": "NVIDIA CORPORATION", "score": 0.7066891}, {"chunk_id": "16846335-f2af-4650-bf60-d4b5485698d4", "text": "Principles of ConsolidationOur consolidated financial statements include the accounts of NVIDIA Corporation and our wholly-owned subsidiaries.All intercompany balances andtransactions have been eliminated in consolidation.Use of EstimatesThe preparation of financial statements in conformity with U.S.GAAP requires management to make estimates and assumptions that affect the reportedamounts of assets and liabilities and disclosures of contingent assets and liabilities at the date of the financial statements and the reported amounts of revenueand expenses during the reporting period.Actual results could differ materially from our estimates.On an on-going basis, we evaluate our estimates, includingthose related to revenue recognition, cash equivalents and marketable securities, accounts receivable, inventories and product purchase commitments, incometaxes, goodwill, stock-based compensation, litigation, investigation and settlement costs, restructuring and other charges, property, plant, and equipment, andother contingencies.These estimates are based on historical facts and various other assumptions that we believe are reasonable.In February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of most of our server, storage, and network equipment from three to four or five years, and our assembly and test equipment from five toseven years.The effect of this change for the fiscal year ended January 28, 2024 was a benefit of $33 million and $102 million for cost of revenue and operatingexpenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or $0.05 per both basic anddiluted share.Revenue RecognitionWe derive our revenue from product sales, including hardware and systems, license and development arrangements, software licensing, and cloud services.Wedetermine revenue recognition through the following steps: (1) identification of the contract with a customer; (2) identification of the performance obligations inthe contract; (3) determination of the transaction price; (4) allocation of the transaction price to the performance obligations in the contract (where revenue isallocated on a relative standalone selling price basis by maximizing the use of observable inputs to determine the standalone selling price for each performanceobligation); and (5) recognition of revenue when, or as, we satisfy a performance obligation.Product Sales RevenueRevenue from product sales is recognized upon transfer of control of products to customers in an amount that reflects the consideration we expect to receive inexchange for those products.Certain products are sold with support or an extended warranty for the incorporated system, hardware, and/or software.Supportand extended warranty revenue are recognized ratably over the service period, or as services are performed.Revenue is recognized net of allowances forreturns, customer programs and any taxes collected from customers.For products sold with a right of return, we record a reduction to revenue by establishing a sales return allowance for estimated product returns at the timerevenue is recognized, based primarily on historical return rates.", "page_start": 55, "page_end": 55, "section_title": "NVIDIA CORPORATION", "score": 0.6930792}, {"chunk_id": "f2207e14-4c6c-421b-849b-76359737cca1", "text": "Germany, India, Israel, and Taiwan for fiscal years 2005 through 2023.Note 15 - Shareholders’ EquityCapital Return ProgramIn August 2023, our Board of Directors approved an increase to our share repurchase program of an additional $25.0 billion, without expiration.During fiscalyear 2024, we repurchased 21 million shares of our common stock for $9.7 billion.As of January 28, 2024, we were authorized, subject to certain specifications,to repurchase additional shares of our common stock up to $22.5 billion.From January 29, 2024 through February 16, 2024, we repurchased 2.8 million sharesfor $1.9 billion pursuant to a Rule 10b5-1 trading plan.Our share repurchase program aims to offset dilution from shares issued to employees.We may pursueadditional share repurchases as we weigh market factors and other investment opportunities.During fiscal years 2024, 2023, and 2022, we paid $395 million, $398 million, and $399 million in cash dividends to our shareholders, respectively.Our cashdividend program and the payment of future cash dividends under that program are subject to our Board of Directors' continuing determination that the dividendprogram and the declaration of dividends thereunder are in the best interests of our shareholders.In fiscal year 2022, we retired our existing 349 million treasury shares.These shares assumed the status of authorized and unissued shares upon retirement.The excess of repurchase price over par value was allocated between additional paid-in capital and retained earnings, resulting in a reduction in additional paidin capital by $20 million and retained earnings by $12.0 billion.Any future repurchased shares will assume the status of authorized and unissued shares.77NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Note 16 - Employee Retirement PlansWe provide tax-qualified defined contribution plans to eligible employees in the U.S.and certain other countries.Our contribution expense for fiscal years 2024,2023, and 2022 was $255 million, $227 million, and $168 million, respectively.Note 17 - Segment InformationOur Chief Executive Officer, who is considered to be our chief operating decision maker, or CODM, reviews financial information presented on an operatingsegment basis for purposes of making decisions and assessing financial performance.The Compute & Networking segment includes our Data Center accelerated computing platform; networking; automotive artificial intelligence, or AI, Cockpit,autonomous driving development agreements, and autonomous vehicle solutions; electric vehicle computing platforms; Jetson for robotics and other embeddedplatforms; NVIDIA AI Enterprise and other software; and DGX Cloud.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions forgaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across our", "page_start": 77, "page_end": 78, "section_title": "NVIDIA CORPORATION", "score": 0.6801053}, {"chunk_id": "c69190e8-be56-40d7-9e3f-a1b1a8224354", "text": "Estimates for customer program accrualsinclude a combination of historical attainment and claim rates and may be adjusted based on relevant internal and external factors.License and Development ArrangementsRevenue from License and Development Arrangements is recognized over the period in which the development services are performed.Each fiscal reportingperiod, we measure progress to completion based on actual cost incurred to date as a percentage of the estimated total cost required to complete each project.Estimated total cost for each project includes a forecast of internal engineer personnel time expected to be incurred and other third-party costs as applicable.Contracts with Multiple Performance ObligationsOur contracts may contain more than one performance obligation.Judgement is required in determining whether each performance obligation within a customercontract is distinct.Except for License and Development Arrangements, NVIDIA products and services function on a standalone basis and do not require asignificant amount of integration or interdependency.Therefore, multiple performance obligations contained within a customer contract are considered distinctand are not combined for revenue recognition purposes.We allocate the total transaction price to each distinct performance obligation in a multiple performance obligations arrangement on a relative standalone sellingprice basis.In certain cases, we can establish standalone selling price based on directly observable prices of products or services sold separately in comparablecircumstances to similar customers.If standalone selling price is not directly observable, such as when we do not sell a product or service separately, wedetermine standalone selling price based on market data and other observable inputs.Change in Accounting EstimateIn February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of a majority of the server, storage, and network equipment from three years to a range of four to five years, and assembly and testequipment from five years to seven years.The estimated effect of this change for fiscal year 2024 was a benefit of $33 million and $102 million for cost ofrevenue and operating expenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or$0.05 per both basic and diluted share.Results of OperationsA discussion regarding our financial condition and results of operations for fiscal year 2024 compared to fiscal year 2023 is presented below.A discussionregarding our financial condition and results of operations for fiscal year 2023 compared to fiscal year 2022 can be found under Item 7 in our Annual Report onForm 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38", "page_start": 38, "page_end": 38, "section_title": "NVIDIA CORPORATION", "score": 0.6781245}, {"chunk_id": "0dd7fcd2-abcb-4469-bfdc-d6d5c6cb8eae", "text": "*$100 invested on 1/27/19 in stock and in indices, including reinvestment of dividends.Source: FactSet financial data and analytics.1/27/20191/26/20201/31/20211/30/20221/29/20231/28/2024NVIDIA Corporation$100.00$157.02$326.26$574.15$512.40$1,536.28S&P 500$100.00$126.17$144.83$175.25$163.63$199.83Nasdaq 100$100.00$136.15$194.20$218.68$185.67$268.13Item 6.[Reserved]33Item 7.Management's Discussion and Analysis of Financial Condition and Results of OperationsThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with “Item 1A.Risk Factors”, ourConsolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described elsewhere in this Annual Report onForm 10-K, before deciding to purchase, hold or sell shares of our common stock.OverviewOur Company and Our BusinessesNVIDIA pioneered accelerated computing to help solve the most challenging computational problems.Since our original focus on PC graphics, we haveexpanded to several other large and important computationally intensive fields.NVIDIA has leveraged its GPU architecture to create platforms for acceleratedcomputing, AI solutions, scientific computing, data science, AV, robotics, metaverse and 3D internet applications.Our two operating segments are \"Compute & Networking\" and \"Graphics.\" Refer to Note 17 of the Notes to the Consolidated Financial Statements in Part IV,Item 15 of this Annual Report on Form 10-K for additional information.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Recent Developments, Future Objectives and ChallengesDemand and Supply, Product Transitions, and New Products and Business ModelsDemand for our data center systems and products surged in fiscal year 2024.Entering fiscal year 2025, we are gathering customer demand indications acrossseveral product transitions.We have demand visibility for our new data center products ramping later in fiscal year 2025.We have increased our supply andcapacity purchases with existing suppliers, added new vendors and entered into prepaid manufacturing and capacity agreements.These increased purchasevolumes, the number of suppliers, and the integration of new vendors into our supply chain may create more complexity and execution risk.Our purchasecommitments and obligations for inventory and manufacturing capacity at the end of fiscal year 2024 were impacted by shortening lead times for certaincomponents.We may continue to enter into new supplier and capacity arrangements.Supply of Hopper architecture products is improving, and demand remainsvery strong.We expect our next-generation products to be supply-constrained based upon demand indications.", "page_start": 33, "page_end": 34, "section_title": "NVIDIA CORPORATION", "score": 0.6780358}, {"chunk_id": "b54b5cad-85c6-477b-a80b-27989017a705", "text": "Fair value is based upon observable inputs in an inactivemarket and the valuation requires our judgment due to the absence of market prices and inherent lack of liquidity.All gains and losses on these investments,realized and unrealized, are recognized in other income (expense), net on our Consolidated Statements of Income.We assess whether an impairment loss has occurred on our investments in non-marketable equity securities, accounted for under the measurement alternativebased on quantitative and qualitative factors.If any impairment is identified for non-marketable equity securities, we write down the investment to its fair valueand record the corresponding charge through other income (expense), net on our Consolidated Statements of Income.59NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Recently Issued Accounting PronouncementsRecent Accounting Pronouncements Not Yet AdoptedIn November 2023, the Financial Accounting Standards Board, or FASB, issued a new accounting standard to provide for additional disclosures about significantexpenses in operating segments.The standard is effective for our annual reporting for fiscal year 2025 and for interim period reporting starting in fiscal year 2026retrospectively.We are currently evaluating the impact of this standard on our Consolidated Financial Statements.In December 2023, the FASB issued a new accounting standard which provides for new and changes to income tax disclosures including disaggregation of therate reconciliation and income taxes paid disclosures.The amendments in the standard are effective for annual periods beginning after December 15, 2024.Early adoption is permitted and should be applied prospectively, with retrospective application permitted.We expect to adopt this standard in our annual periodbeginning fiscal year 2026.We are currently evaluating the impact of this standard on our Consolidated Financial Statements.Note 2 - Business CombinationTermination of the Arm Share Purchase AgreementIn February 2022, NVIDIA and SoftBank Group Corp, or SoftBank, announced the termination of the Share Purchase Agreement whereby NVIDIA would haveacquired Arm from SoftBank.The parties agreed to terminate it due to significant regulatory challenges preventing the completion of the transaction.Werecorded an acquisition termination cost of $1.4 billion in fiscal year 2023 reflecting the write-off of the prepayment provided at signing.Note 3 - LeasesOur lease obligations primarily consist of operating leases for our headquarters complex, domestic and international office facilities, and data center space, withlease periods expiring between fiscal years 2025 and 2035.Futureminimumleasepaymentsunderournon-cancelableoperatingleasesasofJanuary28,2024,areasfollows:Operating Lease Obligations(In millions)Fiscal Year:2025$29020262702027253202823620292022030 and thereafter288Total1,539192Less imputed interestPresent value of net future minimum lease payments1,347228", "page_start": 59, "page_end": 60, "section_title": "NVIDIA CORPORATION", "score": 0.6521132}, {"chunk_id": "4f638c9d-79d6-4b84-9677-86448558b897", "text": "44,30115,35617,475Operating expensesResearch and development8,6757,3395,268Sales, general and administrative2,6542,4402,166—1,353—Acquisition termination cost11,32911,1327,434Total operating expensesOperating income32,9724,22410,041Interest income86626729Interest expense(257)(262)(236)237(48)107Other, net846(43)(100)Other income (expense), netIncome before income tax33,8184,1819,941Income tax expense (benefit)4,058(187)189$29,760$4,368$9,752Net incomeNet income per share:$12.05$1.76$3.91Basic$11.93$1.74$3.85DilutedWeighted average shares used in per share computation:2,4692,4872,496Basic2,4942,5072,535DilutedSee accompanying notes to the consolidated financial statements.50NVIDIA Corporation and SubsidiariesConsolidated Statements of Comprehensive Income(In millions)Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Net income$29,760$4,368$9,752Other comprehensive income (loss), net of taxAvailable-for-sale securities:Net change in unrealized gain (loss)80(31)(16)Reclassification adjustments for net realized gain included in net income—1—80(30)(16)Net change in unrealized gain (loss)Cash flow hedges:Net change in unrealized gain (loss)3847(43)(48)(49)29Reclassification adjustments for net realized gain (loss) included in net income(10)(2)(14)Net change in unrealized loss70(32)(30)Other comprehensive income (loss), net of tax$29,830$4,336$9,722Total comprehensive incomeSee accompanying notes to the consolidated financial statements.51NVIDIA Corporation and SubsidiariesConsolidated Balance Sheets(In millions, except par value)Jan 28, 2024Jan 29, 2023AssetsCurrent assets:Cash and cash equivalents$7,280$3,389Marketable securities18,7049,907Accounts receivable, net9,9993,827Inventories5,2825,1593,080791Prepaid expenses and other current assetsTotal current assets44,34523,073Property and equipment, net3,9143,807Operating lease assets1,3461,038Goodwill4,4304,372Intangible assets, net1,1121,676Deferred income tax assets6,0813,3964,5003,820Other assets$65,728$41,182Total assetsLiabilities and Shareholders' EquityCurrent liabilities:Accounts payable$2,699$1,193Accrued and other current liabilities6,682", "page_start": 50, "page_end": 52, "section_title": "NVIDIA CORPORATION", "score": 0.6491507}], "adversarial_nvidia_01": [{"chunk_id": "bfbdeb8f-839a-40aa-823c-97d93122c280", "text": "bylaws to the contrary, no Covered Officer shall be entitled to indemnification against the loss of Recoverable Incentive Compensation, nor shall any CoveredOfficer be entitled to advancement of expenses in connection with any enforcement of this Policy by the Company, including payment or reimbursement for theCovered Officer’s insurance premiums to cover potential obligations to the Company under this Policy.All Covered Officers are subject to this Policy pursuant tothe terms as set forth in Exhibit A.Nothing in this Policy nor any recovery shall limit any claims, damages or other legal remedies the Company may have against a Covered Officer.This Policydoes not preclude the Company from taking any other action to enforce a Covered Officer’s obligations to the Company, including, without limitation, terminationof employment and/or institution of civil proceedings.This Policy is in addition to the requirements of Section 304 of the Sarbanes-Oxley Act of 2002 that are applicable to the Company’s Chief Executive Officer andChief Financial Officer and to any other compensation recovery policy and/or similar provisions in any employment, equity plan, equity award or other individualagreement to which the Company is a party or which the Company has adopted or may adopt and maintain from time to time; provided, that any amountsalready recovered by or reimbursed to the Company under those requirements and provisions related to the same Accounting Restatement will be creditedtowards the amount of any Recoverable Incentive Compensation under this Policy, except as may be required by applicable law or listing standards.Any action by the Company to recover or any recovery of Recoverable Incentive Compensation under this Policy from a Covered Officer shall not be deemed (i)“good reason” for resignation or to serve as a basis for a claim of constructive termination under any benefits or compensation arrangements applicable to suchCovered Officer, or (ii) to constitute a breach of any contract or other arrangement to which such Covered Officer is a party.COMPLIANCE, VIOLATIONS and CONSEQUENCESThe Company may be subject to delisting if it does not comply with this Policy.ADMINISTRATION, TERMINATION and AMENDMENTThis Policy shall be administered by the Committee, unless otherwise determined by a majority of the independent members of the Board.Any determination bythe Committee with respect to this Policy shall be final, conclusive and binding on all interested parties and need not be uniform with respect to each individualcovered by this Policy.In carrying out the administration of this Policy, the Committee is authorized and directed to consult with the full Board, individualmembers of the Board, or such other committees of the Board as may be necessary or appropriate as to matters within the scope of such other committee’sresponsibility and authority.The Committee members and any other members of the Board who assist in the administration of this Policy shall not be personally liable for any action,", "page_start": 93, "page_end": 93, "section_title": "COMPENSATION RECOVERY POLICY", "score": 0.08769445}, {"chunk_id": "a3a484d9-d571-4511-a2cf-1d4886fa1165", "text": "payment or reimbursement by the Company for premiums in connection with any insurance I may procure to cover potential obligations to the Company underthis Policy, and I hereby waive any right to advancement of expenses in connection with any enforcement of the Policy by the Company.Agreed and Acknowledged:Name:Title:Date:NVIDIA Corporation Application: GlobalDocument Name: Compensation Recovery Policy - ExternalPage 5 of 5", "page_start": 96, "page_end": 96, "section_title": "COMPENSATION RECOVERY POLICY", "score": 0.07486447}, {"chunk_id": "a2f1ee47-20ba-4328-beab-66f5e79afb12", "text": "this Policy, and I hereby waive any right to advancement of expenses in connection with any enforcement of the Policy by the Company.Agreed and Acknowledged:Name:Title:Date:NVIDIA Corporation Application: GlobalDocument Name: Compensation Recovery Policy - ExternalPage 5 of 5", "page_start": 96, "page_end": 96, "section_title": "COMPENSATION RECOVERY POLICY", "score": 0.0734559}, {"chunk_id": "5bd8f351-2a91-4310-af8b-4a42427301ba", "text": "covered by insurance may be large, which could harm our results of operations and financial condition.Our business and those of our suppliers and customers may also be subject to climate-related laws, regulations and lawsuits.New or proposed regulationsrelating to carbon taxes, fuel or energy taxes, pollution limits, sustainability-related disclosure and governance and supply chain governance could result ingreater direct costs, including costs associated with changes to manufacturing processes or the procurement of raw materials used in manufacturing processes,increased capital expenditures to improve facilities and equipment, and higher compliance and energy costs to reduce emissions, other compliance costs, aswell as greater indirect costs resulting from our customers and/or suppliers incurring additional compliance costs that are passed on to us.These costs andrestrictions could harm our business and results of operations by increasing our expenses or requiring us to alter our operations and product design activities.Stakeholder groups may find us insufficiently responsive to the implications of climate change, and therefore we may face legal action or reputational harm.Wemay not achieve our stated sustainability-related goals, which could harm our reputation, or we may incur additional, unexpected costs to achieve such goals.We may also experience contractual disputes due to supply chain delays arising from climate change-related disruptions, which could result in increasedlitigation and costs.We also face risks related to business trends that may be influenced by climate change concerns.Our business could be negatively impacted by concernsaround the high absolute energy requirements of our GPUs, despite their much more energy efficient design and operation relative to alternative computingplatforms.We may not be able to realize the potential benefits of business investments or acquisitions, and we may not be able to successfully integrateacquired companies, which could hurt our ability to grow our business, develop new products or sell our products.We have acquired and invested and may continue to do so in businesses that offer products, services and technologies that we believe will help expand orenhance our strategic objectives.Acquisitions or investments involve significant challenges and risks and could impair our ability to grow our business, developnew products or sell our products and ultimately could have a negative impact on our financial results.If we pursue a particular transaction, we may limit ourability to enter into other transactions that could help us achieve our other strategic objectives.If we are unable to timely complete acquisitions, including due todelays and challenges in obtaining regulatory approvals, we may be unable to pursue other transactions, we may not be able to retain critical talent from thetarget company, technology may evolve and make the acquisition less attractive, and other changes can take place, which could reduce the anticipated benefitsof the transaction and negatively impact our business.Regulators could also impose conditions that reduce the ultimate value of our acquisitions.In addition, tothe extent that our perceived ability to consummate acquisitions has been22", "page_start": 22, "page_end": 22, "section_title": "NVIDIA CORPORATION", "score": 0.06534668}, {"chunk_id": "d2b5b741-383a-4c6b-aeae-77e43a30069e", "text": "§ 1350 has been provided to NVIDIA Corporation and will beretained by NVIDIA Corporation and furnished to the Securities and Exchange Commission or its staff upon request.This certification accompanies the Form 10-K to which it relates, is not deemed filed with the Securities and Exchange Commission and isnot to be incorporated by reference into any filing of the Company under the Securities Act of 1933, as amended, or the Exchange Act (whethermade before or after the date of the Form 10-K), irrespective of any general incorporation language contained in such filing.Last updated: November 30, 2023This Policy provides for the Company’s recovery, in the event of an Accounting Restatement, of certain compensation received by the Company’s officers.Specifically, this Policy applies to Incentive Compensation Received on or after the Effective Date by a Covered Officer (i) after beginning services as a CoveredOfficer, (ii) who served as a Covered Officer at any time during the performance period for such Incentive Compensation, (iii) while the Company had a class ofsecurities listed on a national securities exchange or a national securities association, and (iv) during the Lookback Period.This Policy is designed to comply, and shall be interpreted to be consistent, with the Compensation Recovery Rules.It shall be binding and enforceable againstall Covered Officers and, to the extent required by the Compensation Recovery Rules, their beneficiaries, heirs, executors, administrators or other legalrepresentatives.This Policy amends and restates the Prior Policy with respect to all Incentive Compensation that is Received by a Covered Officer on and after the EffectiveDate.The Prior Policy shall continue to apply to Incentive Compensation Received by a Covered Officer prior to the Effective Date.In the event of an Accounting Restatement, regardless of whether or when any restated financial statements are filed, the Company must reasonably promptlyrecover the full amount of the Recoverable Incentive Compensation, regardless of whether the Covered Officer engaged in any misconduct and regardless offault.For any compensation plans or programs involving Incentive Compensation, the amount of Recoverable Incentive Compensation shall include the amountcontributed to any notional account based on Recoverable Incentive Compensation and any earnings to date on that notional amount.For any IncentiveCompensation that is based on stock price or TSR, where the amount of Recoverable Incentive Compensation is not subject to mathematical recalculationdirectly from the information in an Accounting Restatement, the amount of Recoverable Incentive Compensation shall be based on a reasonable estimate of theeffect of the Accounting Restatement on the applicable stock price or TSR.The Company shall maintain documentation of the determination of that reasonableestimate and provide it to Nasdaq in accordance with Rule 5608.The Company would not be required to recover Recoverable Incentive Compensation if (I) (A) the direct expense paid to a third party to assist in enforcing this", "page_start": 91, "page_end": 92, "section_title": "NVIDIA CORPORATION", "score": 0.061055902}, {"chunk_id": "d18aa330-0dff-4f13-b004-2f71ef95a8aa", "text": "ReceivedIncentive Compensation is deemed “received” in the Company’s fiscal period in which the relevant Financial ReportingMeasure is attained, even if the payment or grant of such Incentive Compensation occurs after the end of that period.Recoverable IncentiveIncentive Compensation Received by a Covered Officer during the Lookback Period that exceeds the amount of IncentiveCompensationCompensation that otherwise would have been Received had such amount been determined based on the AccountingRestatement, computed without regard to any taxes paid (i.e., on a gross basis without regarding to tax withholdings andother deductions)Rule 5608Nasdaq Listing Rule 5608SECSecurities and Exchange CommissionTSRTotal shareholder returnNVIDIA Corporation Application: GlobalDocument Name: Compensation Recovery Policy - ExternalPage 4 of 5Exhibit ANVIDIA CorporationCompensation Recovery PolicyForm of Executive AcknowledgmentI, the undersigned, agree and acknowledge that I am bound by, and subject to, the NVIDIA Corporation Compensation Recovery Policy dated November 30,2023, as may be amended, restated, supplemented or otherwise modified from time to time (the “Policy”).In the event of any inconsistency between the Policyand the terms of any employment agreement, offer letter, indemnity agreement or other individual agreement with NVIDIA Corporation (the “Company”) to whichI am a party, or the terms of any compensation plan, program or agreement, whether or not written, under which any compensation has been granted, awarded,earned or paid to me, the terms of the Policy shall govern.In the event that the Committee (as defined in the Policy) or other applicable administrator of the Policy determines that any compensation granted, awarded,earned or paid to me must be forfeited or reimbursed to the Company pursuant to the Policy, I will promptly take any action necessary to effectuate suchforfeiture and/or reimbursement.Notwithstanding anything to the contrary in the Company’s bylaws or in any indemnity agreement between the Company andme, I further agree and acknowledge that I am not entitled to indemnification against the loss of any Recoverable Incentive Compensation, I am not entitled topayment or reimbursement by the Company for premiums in connection with any insurance I may procure to cover potential obligations to the Company underthis Policy, and I hereby waive any right to advancement of expenses in connection with any enforcement of the Policy by the Company.Agreed and Acknowledged:Name:Title:Date:NVIDIA Corporation Application: GlobalDocument Name: Compensation Recovery Policy - ExternalPage 5 of 5", "page_start": 95, "page_end": 96, "section_title": "COMPENSATION RECOVERY POLICY", "score": 0.060797583}, {"chunk_id": "ffb980a7-0575-452c-9601-5c1395401380", "text": "networking products has not experienced any significant impact.Further, in connection with the conflict, a substantial number of our employees in the regionhave been called-up for active military duty in Israel.Accordingly, some of our employees in Israel have been absent for an extended period and they or othersmay continue to be absent, which may cause disruption to our product development or operations.We did not experience any significant impact or expense toour business; however, if the conflict is further extended, it could impact future product development, operations, and revenue or create other uncertainty for ourbusiness.Additionally, interruptions or delays in services from CSPs, data center co-location partners, and other third parties on which we rely, including due to the eventsdescribed above or other events such as the insolvency of these parties, could impair our ability to provide our products and services and harm our business.Aswe increase our reliance on these third-party systems and services, our exposure to damage from service interruptions, defects, disruptions, outages, shortagesand other performance and quality problems may increase.Data centers depend on access to clean water and predictable energy.Power or water shortages, orregulations that limit energy or water availability, could impair the ability of our customers to expand their data center capacity and consume our products andservices.Climate change may have a long-term impact on our business.Climate change may have an increasingly adverse impact on our business and on our customers, partners and vendors.Water and energy availability andreliability in the regions where we conduct business is critical, and certain of our facilities may be vulnerable to the impacts of extreme weather events.Extremeheat and wind coupled with dry conditions in Northern California may lead to power safety shut offs due to wildfire risk, which can have adverse implications forour Santa Clara, California headquarter offices and data centers, including impairing the ability of our employees to work effectively.Climate change, its impacton our supply chain and critical infrastructure worldwide and its potential to increase political instability in regions where we, our customers, partners and ourvendors do business, may disrupt our business and cause us to experience higher attrition, losses and costs to maintain or resume operations.Although wemaintain insurance coverage for a variety of property, casualty, and other risks, the types and amounts of insurance we obtain vary depending on availability andcost.Some of our policies have large deductibles and broad exclusions, and our insurance providers may be unable or unwilling to pay a claim.Losses notcovered by insurance may be large, which could harm our results of operations and financial condition.Our business and those of our suppliers and customers may also be subject to climate-related laws, regulations and lawsuits.New or proposed regulationsrelating to carbon taxes, fuel or energy taxes, pollution limits, sustainability-related disclosure and governance and supply chain governance could result in", "page_start": 22, "page_end": 22, "section_title": "NVIDIA CORPORATION", "score": 0.05521099}, {"chunk_id": "96e9c425-d84b-452b-b231-a04873bc80fd", "text": "members of the Board, or such other committees of the Board as may be necessary or appropriate as to matters within the scope of such other committee’sresponsibility and authority.The Committee members and any other members of the Board who assist in the administration of this Policy shall not be personally liable for any action,determination or interpretation made with respect to this Policy and shall be indemnified by the Company to the fullest extent under applicable law and Companypolicy with respect to any such action, determination or interpretation.The foregoing sentence shall not limit any other rights to indemnification of Committeemembers under applicable law or Company policy.Subject to applicable law, the Committee may authorize and empower any officer or employee of the Company to take any and all actions that the Committee, inits sole discretion, deems necessary or appropriate to carry out the purpose and intent of this Policy (other than with respect to any recovery under this Policyinvolving such officer or employee).NVIDIA Corporation Application: GlobalDocument Name: Compensation Recovery Policy - ExternalPage 2 of 5If any provision of this Policy or the application of any provision to a Covered Officer shall be adjudicated to be invalid, illegal or unenforceable in any respect,such invalidity, illegality or unenforceability shall not affect any other provisions of this Policy, and the invalid, illegal or unenforceable provisions shall be deemedamended to the minimum extent necessary to render any such provision or application enforceable.The Committee may amend, terminate or replace this Policy or any portion of this Policy at any time in its sole discretion, and shall amend this Policy as itdeems necessary to comply with applicable law or listing standards.If you are aware of a situation that may violate this Policy, you have a responsibility to report the incident to your immediate supervisor, Corporate Legal or toNVIDIA-Compliance.Reports may also be submitted anonymously by using NVIDIA’s Speak Up system through EthicsPoint at 1-866-295-3993 (for the U.S.) orvia the web here.When reporting a concern, we encourage you to consider revealing your identity so that we can properly follow up and investigate allegedviolations.NVIDIA will ensure that appropriate confidentiality measures are taken and will not retaliate against anyone, who in good faith, reports a concern orcooperates with an investigation, even when allegations are found to be unsubstantiated.If you have any questions about any aspect of this Policy, you are encouraged to contact Corporate Legal.NVIDIA Corporation Application: GlobalDocument Name: Compensation Recovery Policy - ExternalPage 3 of 5DefinitionsAccountingAn accounting restatement that the Company is required to prepare due to the material noncompliance of the Company withRestatementany financial reporting requirement under the securities laws, including any required accounting restatement to correct an", "page_start": 93, "page_end": 95, "section_title": "COMPENSATION RECOVERY POLICY", "score": 0.05485356}], "adversarial_attention_01": [{"chunk_id": "0b373c99-8c0e-44bf-8fc7-f6b6b009590d", "text": "Provided proper attribution is provided, Google hereby grants permission toreproduce the tables and figures in this paper solely for use in journalistic orscholarly works.Ashish Vaswani∗Noam Shazeer∗Niki Parmar∗Jakob Uszkoreit∗Google BrainGoogle BrainGoogle ResearchGoogle Researchavaswani@google.comnoam@google.comnikip@google.comusz@google.comLlion Jones∗Aidan N.Gomez∗†Łukasz Kaiser∗Google ResearchUniversity of TorontoGoogle Brainlukaszkaiser@google.comllion@google.comaidan@cs.toronto.eduIllia Polosukhin∗‡illia.polosukhin@gmail.comAbstractThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder.The bestperforming models also connect the encoder and decoder through an attentionmechanism.We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely.Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring significantlyless time to train.Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU.On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.8 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature.We show that the Transformer generalizes well toother tasks by applying it successfully to English constituency parsing both withlarge and limited training data.∗Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and startedthe effort to evaluate this idea.Ashish, with Illia, designed and implemented the first Transformer models andhas been crucially involved in every aspect of this work.Noam proposed scaled dot-product attention, multi-headattention and the parameter-free position representation and became the other person involved in nearly everydetail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase andtensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, andefficient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of andimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks", "page_start": 1, "page_end": 2, "section_title": "", "score": 0.52178687}, {"chunk_id": "5151352b-ae46-44f3-83b0-8b8748547941", "text": "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networksin particular, have been firmly established as state of the art approaches in sequence modeling andtransduction problems such as language modeling and machine translation [35, 2, 5].Numerousefforts have since continued to push the boundaries of recurrent language models and encoder-decoderarchitectures [38, 24, 15].Recurrent models typically factor computation along the symbol positions of the input and outputsequences.Aligning the positions to steps in computation time, they generate a sequence of hiddenstates ht, as a function of the previous hidden state ht−1 and the input for position t.This inherentlysequential nature precludes parallelization within training examples, which becomes critical at longersequence lengths, as memory constraints limit batching across examples.Recent work has achievedsignificant improvements in computational efficiency through factorization tricks [21] and conditionalcomputation [32], while also improving model performance in case of the latter.The fundamentalconstraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance inthe input or output sequences [2, 19].In all but a few cases [27], however, such attention mechanismsare used in conjunction with a recurrent network.In this work we propose the Transformer, a model architecture eschewing recurrence and insteadrelying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for significantly more parallelization and can reach a new state of the art intranslation quality after being trained for as little as twelve hours on eight P100 GPUs.2BackgroundThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic buildingblock, computing hidden representations in parallel for all input and output positions.In these models,the number of operations required to relate signals from two arbitrary input or output positions growsin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.This makesit more difficult to learn dependencies between distant positions [12].In the Transformer this isreduced to a constant number of operations, albeit at the cost of reduced effective resolution dueto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,", "page_start": 1, "page_end": 2, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.48758602}, {"chunk_id": "bb77d3cc-59f9-4b97-a21b-132b30215259", "text": "semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised92.1Transformer (4 layers)semi-supervised92.7Luong et al.(2015) [23]multi-task93.0Dyer et al.(2016) [8]generative93.3increased the maximum output length to input length + 300.We used a beam size of 21 and α = 0.3for both WSJ only and the semi-supervised setting.Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of theRecurrent Neural Network Grammar [8].In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.7ConclusionIn this work, we presented the Transformer, the first sequence transduction model based entirely onattention, replacing the recurrent layers most commonly used in encoder-decoder architectures withmulti-headed self-attention.For translation tasks, the Transformer can be trained significantly faster than architectures basedon recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014English-to-French translation tasks, we achieve a new state of the art.In the former task our bestmodel outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.Weplan to extend the Transformer to problems involving input and output modalities other than text andto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputssuch as images, audio and video.Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.AcknowledgementsWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitfulcomments, corrections and inspiration.References[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprintarXiv:1607.06450, 2016.[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointlylearning to align and translate.CoRR, abs/1409.0473, 2014.[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V.Le.Massive exploration of neuralmachine translation architectures.CoRR, abs/1703.03906, 2017.[4] Jianpeng Cheng, Li Dong, and Mirella Lapata.Long short-term memory-networks for machinereading.arXiv preprint arXiv:1601.06733, 2016.10[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,", "page_start": 10, "page_end": 11, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.475358}, {"chunk_id": "8d9a2784-c6bd-4675-a20b-80dd0948329f", "text": "25.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,keeping the amount of computation constant, as described in Section 3.2.2.While single-headattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality.Thissuggests that determining compatibility is not easy and that a more sophisticated compatibilityfunction than dot product may be beneficial.We further observe in rows (C) and (D) that, as expected,bigger models are better, and dropout is very helpful in avoiding over-fitting.In row (E) we replace oursinusoidal positional encoding with learned positional embeddings [9], and observe nearly identicalresults to the base model.6.3English Constituency ParsingTo evaluate if the Transformer can generalize to other tasks we performed experiments on Englishconstituency parsing.This task presents specific challenges: the output is subject to strong structuralconstraints and is significantly longer than the input.Furthermore, RNN sequence-to-sequencemodels have not been able to attain state-of-the-art results in small-data regimes [37].We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of thePenn Treebank [25], about 40K training sentences.We also trained it in a semi-supervised setting,using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences[37].We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokensfor the semi-supervised setting.We performed only a small number of experiments to select the dropout, both attention and residual(section 5.4), learning rates and beam size on the Section 22 development set, all other parametersremained unchanged from the English-to-German base translation model.During inference, we9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23of WSJ)ParserTrainingWSJ 23 F1Vinyals & Kaiser el al.(2014) [37]WSJ only, discriminative88.3Petrov et al.(2006) [29]WSJ only, discriminative90.4Zhu et al.(2013) [40]WSJ only, discriminative90.4Dyer et al.(2016) [8]WSJ only, discriminative91.7Transformer (4 layers)WSJ only, discriminative91.3Zhu et al.(2013) [40]semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised", "page_start": 9, "page_end": 10, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.44357026}, {"chunk_id": "160dd6c9-8965-4dcd-8d74-e0dd157c4fd1", "text": "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step number.We usedwarmup_steps = 4000.5.4RegularizationWe employ three types of regularization during training:7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on theEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.BLEUTraining Cost (FLOPs)ModelEN-DEEN-FREN-DEEN-FRByteNet [18]23.751.0 · 1020Deep-Att + PosUnk [39]39.22.3 · 10191.4 · 1020GNMT + RL [38]24.639.929.6 · 10181.5 · 1020ConvS2S [9]25.1640.462.0 · 10191.2 · 1020MoE [32]26.0340.568.0 · 1020Deep-Att + PosUnk Ensemble [39]40.41.8 · 10201.1 · 1021GNMT + RL Ensemble [38]26.3041.167.7 · 10191.2 · 102141.29ConvS2S Ensemble [9]26.363.3 · 1018Transformer (base model)27.338.12.3 · 101928.441.8Transformer (big)Residual DropoutWe apply dropout [33] to the output of each sub-layer, before it is added to thesub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and thepositional encodings in both the encoder and decoder stacks.For the base model, we use a rate ofPdrop = 0.1.During training, we employed label smoothing of value ϵls = 0.1 [36].ThisLabel Smoothinghurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6Results6.1Machine TranslationOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0BLEU, establishing a new state-of-the-art BLEU score of 28.4.The configuration of this model islisted in the bottom line of Table 3.Training took 3.5 days on 8 P100 GPUs.Even our base modelsurpasses all previously published models and ensembles, at a fraction of the training cost of any ofthe competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,outperforming all of the previously published single models, at less than 1/4 the training cost of theprevious state-of-the-art model.The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We", "page_start": 7, "page_end": 8, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.44145027}, {"chunk_id": "6ecb21e7-c073-4d90-9d8a-41fdfda06afb", "text": "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,textual entailment and learning task-independent sentence representations [4, 27, 28, 22].End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering andlanguage modeling tasks [34].To the best of our knowledge, however, the Transformer is the first transduction model relyingentirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.In the following sections, we will describe the Transformer, motivateself-attention and discuss its advantages over models such as [17, 18] and [9].3Model ArchitectureMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequenceof continuous representations z = (z1, ..., zn).Given z, the decoder then generates an outputsequence (y1, ..., ym) of symbols one element at a time.At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next.2Figure 1: The Transformer - model architecture.The Transformer follows this overall architecture using stacked self-attention and point-wise, fullyconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,respectively.3.1Encoder and Decoder StacksThe encoder is composed of a stack of N = 6 identical layers.Each layer has twoEncoder:sub-layers.The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network.We employ a residual connection [11] around each ofthe two sub-layers, followed by layer normalization [1].That is, the output of each sub-layer isLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layeritself.To facilitate these residual connections, all sub-layers in the model, as well as the embeddinglayers, produce outputs of dimension dmodel = 512.The decoder is also composed of a stack of N = 6 identical layers.In addition to the twoDecoder:sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-headattention over the output of the encoder stack.Similar to the encoder, we employ residual connectionsaround each of the sub-layers, followed by layer normalization.We also modify the self-attentionsub-layer in the decoder stack to prevent positions from attending to subsequent positions.Thismasking, combined with fact that the output embeddings are offset by one position, ensures that the", "page_start": 2, "page_end": 3, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4225546}, {"chunk_id": "914cfb0b-3d4a-4988-8392-851b64c13e57", "text": "The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.Weused beam search with a beam size of 4 and length penalty α = 0.6 [38].These hyperparameterswere chosen after experimentation on the development set.We set the maximum output length duringinference to input length + 50, but terminate early when possible [38].Table 2 summarizes our results and compares our translation quality and training costs to other modelarchitectures from the literature.We estimate the number of floating point operations used to train amodel by multiplying the training time, the number of GPUs used, and an estimate of the sustainedsingle-precision floating-point capacity of each GPU 5.6.2Model VariationsTo evaluate the importance of different components of the Transformer, we varied our base modelin different ways, measuring the change in performance on English-to-German translation on the5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the basemodel.All metrics are on the English-to-German translation development set, newstest2013.Listedperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared toper-word perplexities.trainPPLBLEUparamsNdmodeldffhdkdvPdropϵls×106steps(dev)(dev)base65122048864640.10.1100K4.9225.86515125125.2924.941281285.0025.5(A)1632324.9125.83216165.0125.4165.1625.158(B)325.0125.46026.1123.73645.1925.35084.8825.580(C)25632325.7524.52810241281284.6626.016810245.1225.45340964.7526.2900.05.7724.60.24.9525.5(D)0.04.6725.30.25.4725.7(E)positional embedding instead of sinusoids4.9225.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.", "page_start": 8, "page_end": 9, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.3988948}, {"chunk_id": "222d0339-104c-4f96-bec5-f86a9511af81", "text": "[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V.Le.Massive exploration of neuralmachine translation architectures.CoRR, abs/1703.03906, 2017.[4] Jianpeng Cheng, Li Dong, and Mirella Lapata.Long short-term memory-networks for machinereading.arXiv preprint arXiv:1601.06733, 2016.10[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,and Yoshua Bengio.Learning phrase representations using rnn encoder-decoder for statisticalmachine translation.CoRR, abs/1406.1078, 2014.[6] Francois Chollet.Xception: Deep learning with depthwise separable convolutions.arXivpreprint arXiv:1610.02357, 2016.[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio.Empirical evaluationof gated recurrent neural networks on sequence modeling.CoRR, abs/1412.3555, 2014.[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A.Smith.Recurrent neuralnetwork grammars.In Proc.of NAACL, 2016.[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N.Dauphin.Convolutional sequence to sequence learning.arXiv preprint arXiv:1705.03122v2, 2017.arXiv preprint[10] Alex Graves.Generating sequences with recurrent neural networks.arXiv:1308.0850, 2013.[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition.In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pages 770–778, 2016.[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber.Gradient flow inrecurrent nets: the difficulty of learning long-term dependencies, 2001.[13] Sepp Hochreiter and Jürgen Schmidhuber.Long short-term memory.Neural computation,9(8):1735–1780, 1997.[14] Zhongqiang Huang and Mary Harper.Self-training PCFG grammars with latent annotationsacross languages.In Proceedings of the 2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 832–841.ACL, August 2009.[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.Exploringthe limits of language modeling.arXiv preprint arXiv:1602.02410, 2016.[16] Łukasz Kaiser and Samy Bengio.Can active memory replace attention?In Advances in NeuralInformation Processing Systems, (NIPS), 2016.[17] Łukasz Kaiser and Ilya Sutskever.Neural GPUs learn algorithms.In International Conferenceon Learning Representations (ICLR), 2016.[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu.Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,2017.[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M.Rush.Structured attention networks.In International Conference on Learning Representations, 2017.[20] Diederik Kingma and Jimmy Ba.Adam: A method for stochastic optimization.In ICLR, 2015.[21] Oleksii Kuchaiev and Boris Ginsburg.Factorization tricks for LSTM networks.arXiv preprintarXiv:1703.10722, 2017.[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, BowenZhou, and Yoshua Bengio.A structured self-attentive sentence embedding.arXiv preprintarXiv:1703.03130, 2017.[23] Minh-Thang Luong, Quoc V.Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.Multi-tasksequence to sequence learning.arXiv preprint arXiv:1511.", "page_start": 10, "page_end": 11, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.38442785}], "adversarial_nvidia_02": [{"chunk_id": "823438dd-9ce8-4ae9-924a-70c6bbf5121f", "text": "Form 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38The following table sets forth, for the periods indicated, certain items in our Consolidated Statements of Income expressed as a percentage of revenue.Year EndedJan 28, 2024Jan 29, 2023Revenue100.0 %100.0 %Cost of revenue27.343.1Gross profit72.756.9Operating expensesResearch and development14.227.2Sales, general and administrative4.49.1Acquisition termination cost—5.0Total operating expenses18.641.3Operating income54.115.6Interest income1.41.0Interest expense(0.4)(1.0)0.4(0.1)Other, net1.4(0.1)Other income (expense), netIncome before income tax55.515.5Income tax expense (benefit)6.6(0.7)48.9 %16.2 %Net incomeReportable SegmentsRevenue by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$47,405$15,068$32,337215 %13,51711,9061,611Graphics14 %$60,922$26,974$33,948Total126 %Operating Income by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$32,016$5,083$26,933530 %Graphics5,8464,5521,29428 %(4,890)(5,411)521All Other(10)%$32,972$4,224$28,748Total681 %Compute & Networking revenue – The year-on-year increase was due to higher Data Center revenue.Compute grew 266% due to higher shipments of theNVIDIA Hopper GPU computing platform for the training and inference of LLMs, recommendation engines and generative AI applications.Networking was up133% due to higher shipments of InfiniBand.Graphics revenue – The year-on-year increase was led by growth in Gaming of 15% driven by higher sell-in to partners following the normalization of channelinventory levels.Reportable segment operating income – The year-on-year increase in Compute & Networking and Graphics operating income was driven by higher revenue.39All Other operating loss - The year-on-year decrease was due to the $1.4 billion Arm acquisition termination cost in fiscal year 2023, partially offset by a $839million increase in stock-based compensation expense in fiscal year 2024.Concentration of Revenue", "page_start": 38, "page_end": 40, "section_title": "NVIDIA CORPORATION", "score": 0.67217594}, {"chunk_id": "ddc05d6e-79f6-4ed4-a29b-876d9e76fd48", "text": "We did not experience any significant impact or expense toour business; however, if the conflict is further extended, it could impact future product development, operations, and revenue or create other uncertainty for ourbusiness.35Fiscal Year 2024 SummaryYear EndedJan 28, 2024Jan 29, 2023Change($ in millions, except per share data)Revenue$60,922$26,974Up 126%Gross margin72.7 %56.9 %Up 15.8 ptsOperating expenses$11,329$11,132Up 2%Operating income$32,972$4,224Up 681%Net income$29,760$4,368Up 581%Net income per diluted share$11.93$1.74Up 586%We specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Revenue for fiscal year 2024 was $60.9 billion, up 126% from a year ago.Data Center revenue for fiscal year 2024 was up 217%.Strong demand was driven by enterprise software and consumer internet applications, and multipleindustry verticals including automotive, financial services, and healthcare.Customers across industry verticals access NVIDIA AI infrastructure both through thecloud and on-premises.Data Center compute revenue was up 244% in the fiscal year.Networking revenue was up 133% in the fiscal year.Gaming revenue for fiscal year 2024 was up 15%.The increase reflects higher sell-in to partners following the normalization of channel inventory levels andgrowing demand.Professional Visualization revenue for fiscal year 2024 was up 1%.Automotive revenue for the fiscal year 2024 was up 21%.The increase primarily reflected growth in self-driving platforms.Gross margin increased in fiscal year 2024, primarily driven by Data Center revenue growth and lower net inventory provisions as a percentage of revenue.Operating expenses increased for fiscal year 2024, driven by growth in employees and compensation increases.Fiscal year 2023 also included a $1.4 billionacquisition termination charge related to the proposed Arm transaction.Market Platform HighlightsData Center revenue for fiscal year 2024 was $47.5 billion, up 217% from fiscal year 2023.In Data Center, we launched AI inference platforms that combine ourfull-stack inference software with NVIDIA Ada, NVIDIA Hopper and NVIDIA Grace Hopper processors optimized for generative AI, LLMs and other AI workloads.We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.", "page_start": 35, "page_end": 36, "section_title": "NVIDIA CORPORATION", "score": 0.6476306}, {"chunk_id": "0dd7fcd2-abcb-4469-bfdc-d6d5c6cb8eae", "text": "*$100 invested on 1/27/19 in stock and in indices, including reinvestment of dividends.Source: FactSet financial data and analytics.1/27/20191/26/20201/31/20211/30/20221/29/20231/28/2024NVIDIA Corporation$100.00$157.02$326.26$574.15$512.40$1,536.28S&P 500$100.00$126.17$144.83$175.25$163.63$199.83Nasdaq 100$100.00$136.15$194.20$218.68$185.67$268.13Item 6.[Reserved]33Item 7.Management's Discussion and Analysis of Financial Condition and Results of OperationsThe following discussion and analysis of our financial condition and results of operations should be read in conjunction with “Item 1A.Risk Factors”, ourConsolidated Financial Statements and related Notes thereto, as well as other cautionary statements and risks described elsewhere in this Annual Report onForm 10-K, before deciding to purchase, hold or sell shares of our common stock.OverviewOur Company and Our BusinessesNVIDIA pioneered accelerated computing to help solve the most challenging computational problems.Since our original focus on PC graphics, we haveexpanded to several other large and important computationally intensive fields.NVIDIA has leveraged its GPU architecture to create platforms for acceleratedcomputing, AI solutions, scientific computing, data science, AV, robotics, metaverse and 3D internet applications.Our two operating segments are \"Compute & Networking\" and \"Graphics.\" Refer to Note 17 of the Notes to the Consolidated Financial Statements in Part IV,Item 15 of this Annual Report on Form 10-K for additional information.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Recent Developments, Future Objectives and ChallengesDemand and Supply, Product Transitions, and New Products and Business ModelsDemand for our data center systems and products surged in fiscal year 2024.Entering fiscal year 2025, we are gathering customer demand indications acrossseveral product transitions.We have demand visibility for our new data center products ramping later in fiscal year 2025.We have increased our supply andcapacity purchases with existing suppliers, added new vendors and entered into prepaid manufacturing and capacity agreements.These increased purchasevolumes, the number of suppliers, and the integration of new vendors into our supply chain may create more complexity and execution risk.Our purchasecommitments and obligations for inventory and manufacturing capacity at the end of fiscal year 2024 were impacted by shortening lead times for certaincomponents.We may continue to enter into new supplier and capacity arrangements.Supply of Hopper architecture products is improving, and demand remainsvery strong.We expect our next-generation products to be supply-constrained based upon demand indications.", "page_start": 33, "page_end": 34, "section_title": "NVIDIA CORPORATION", "score": 0.6156594}, {"chunk_id": "096b49f9-ef3d-4e1e-a07d-95303cb2ec48", "text": "not be able to reduce our inventory or other contractual purchase commitments.In the past, we have experienced a reduction in average selling prices, includingdue to channel pricing programs that we have implemented and may continue to implement, as a result of our overestimation of future demand, and we mayneed to continue these reductions.We have had to increase prices for certain of our products as a result of our suppliers’ increase in prices, and we may need tocontinue to do so for other products in the future.We have also written down our inventory, incurred cancellation penalties, and recorded impairments and mayhave to do so in the future.These impacts were amplified by our placement of non-cancellable and non-returnable purchasing terms well in advance of ourhistorical lead times and could be exacerbated if we need to make changes to the design of future products.The risk of these impacts has increased and maycontinue to increase as our purchase obligations and prepaids have grown and are expected to continue to grow and become a greater portion of our totalsupply.All of these factors may negatively impact our gross margins and financial results.We build technology and introduce products for new and innovative use cases and applications, such as NVIDIA DGX Cloud services, NVIDIA AI Foundations,Omniverse platform, LLMs, and generative AI models.Our demand estimates for new use cases, applications, and services can be incorrect and create volatilityin our revenue or supply levels, and we may not be able to generate significant revenue from these use cases, applications, and services.Recent technologies,such as generative AI models, have emerged, and while they have driven increased demand for Data Center, the long-term trajectory is unknown.Because ourproducts may be used in multiple use cases and applications, it is difficult for us to estimate with any reasonable degree of precision the impact of generative AImodels on our reported revenue or forecasted demand.Additionally, we started shipping our CPU product offerings, the Grace CPU and Grace HopperSuperchips, in the third quarter of fiscal year 2024.Our ability to adequately predict our CPU demand may create volatility in our revenue or supply levels.Challenges in estimating demand could become more pronounced or volatile in the future on both a global and regional basis.Extended lead times may occur ifwe experience other supply constraints caused by natural disasters, pandemics or other events.In addition, geopolitical tensions, such as those involving Taiwanand China, which comprise a significant portion of our revenue and where we have suppliers, contract manufacturers, and assembly partners who are critical toour supply continuity, could have a material adverse impact on us.The use of our GPUs other than that for which they were designed and marketed, including new and unexpected use cases, has impacted and can in the futureimpact demand for our products, including by leading to inconsistent spikes and drops in demand.", "page_start": 17, "page_end": 17, "section_title": "NVIDIA CORPORATION", "score": 0.5911896}, {"chunk_id": "f2207e14-4c6c-421b-849b-76359737cca1", "text": "Germany, India, Israel, and Taiwan for fiscal years 2005 through 2023.Note 15 - Shareholders’ EquityCapital Return ProgramIn August 2023, our Board of Directors approved an increase to our share repurchase program of an additional $25.0 billion, without expiration.During fiscalyear 2024, we repurchased 21 million shares of our common stock for $9.7 billion.As of January 28, 2024, we were authorized, subject to certain specifications,to repurchase additional shares of our common stock up to $22.5 billion.From January 29, 2024 through February 16, 2024, we repurchased 2.8 million sharesfor $1.9 billion pursuant to a Rule 10b5-1 trading plan.Our share repurchase program aims to offset dilution from shares issued to employees.We may pursueadditional share repurchases as we weigh market factors and other investment opportunities.During fiscal years 2024, 2023, and 2022, we paid $395 million, $398 million, and $399 million in cash dividends to our shareholders, respectively.Our cashdividend program and the payment of future cash dividends under that program are subject to our Board of Directors' continuing determination that the dividendprogram and the declaration of dividends thereunder are in the best interests of our shareholders.In fiscal year 2022, we retired our existing 349 million treasury shares.These shares assumed the status of authorized and unissued shares upon retirement.The excess of repurchase price over par value was allocated between additional paid-in capital and retained earnings, resulting in a reduction in additional paidin capital by $20 million and retained earnings by $12.0 billion.Any future repurchased shares will assume the status of authorized and unissued shares.77NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Note 16 - Employee Retirement PlansWe provide tax-qualified defined contribution plans to eligible employees in the U.S.and certain other countries.Our contribution expense for fiscal years 2024,2023, and 2022 was $255 million, $227 million, and $168 million, respectively.Note 17 - Segment InformationOur Chief Executive Officer, who is considered to be our chief operating decision maker, or CODM, reviews financial information presented on an operatingsegment basis for purposes of making decisions and assessing financial performance.The Compute & Networking segment includes our Data Center accelerated computing platform; networking; automotive artificial intelligence, or AI, Cockpit,autonomous driving development agreements, and autonomous vehicle solutions; electric vehicle computing platforms; Jetson for robotics and other embeddedplatforms; NVIDIA AI Enterprise and other software; and DGX Cloud.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions forgaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across our", "page_start": 77, "page_end": 78, "section_title": "NVIDIA CORPORATION", "score": 0.58939457}, {"chunk_id": "8d7f8306-8d55-40f5-b383-f4914775e7ee", "text": "(10)Restructuring costs and other—(54)—Acquisition termination cost—(1,353)—10(2)—Other$(4,890)$(5,411)$(3,049)TotalRevenue by geographic areas is designated based upon the billing location of the customer.End customer location may be different than our customer’s billinglocation.Revenue by geographic areas was as follows:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Revenue:(In millions)United States$26,966$8,292$4,349Taiwan13,4056,9868,544China (including Hong Kong)10,3065,7857,11110,2455,9116,910Other countries$60,922$26,974$26,914Total revenueRevenue from sales to customers outside of the United States accounted for 56%, 69%, and 84% of total revenue for fiscal years 2024, 2023, and 2022,respectively.The increase in revenue to the United States for fiscal year 2024 was primarily due to higher U.S.-based Compute & Networking segment demand.Sales to one customer represented 13% of total revenue for fiscal year 2024, which was attributable to the Compute & Networking segment.No customerrepresented 10% or more of total revenue for fiscal years 2023 and 2022.The following table summarizes information pertaining to our revenue by each of the specialized markets we serve:Year EndedJan 28, 2024Jan 29, 2023Jan 30, 2022Revenue:(In millions)Data Center$47,525$15,005$10,613Gaming10,4479,06712,462Professional Visualization1,5531,5442,111Automotive1,0919035663064551,162OEM and Other$60,922$26,974$26,914Total revenue79NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)The following table presents summarized information for long-lived assets by country.Long-lived assets consist of property and equipment and exclude otherassets, operating lease assets, goodwill, and intangible assets.Jan 28, 2024Jan 29, 2023Long-lived assets:(In millions)United States$2,595$2,587Taiwan773702Israel325283Other countries221235$3,914$3,807Total long-lived assets80NVIDIA Corporation and SubsidiariesSchedule II – Valuation and Qualifying AccountsBalance atBeginning ofBalance atDescriptionPeriodAdditionsDeductionsEnd of Period(In millions)Fiscal year 2024Allowance for doubtful accounts$4$— (1)$— (1)$4Sales return allowance$26$213 (2)$", "page_start": 79, "page_end": 81, "section_title": "NVIDIA CORPORATION", "score": 0.58766127}, {"chunk_id": "c69190e8-be56-40d7-9e3f-a1b1a8224354", "text": "Estimates for customer program accrualsinclude a combination of historical attainment and claim rates and may be adjusted based on relevant internal and external factors.License and Development ArrangementsRevenue from License and Development Arrangements is recognized over the period in which the development services are performed.Each fiscal reportingperiod, we measure progress to completion based on actual cost incurred to date as a percentage of the estimated total cost required to complete each project.Estimated total cost for each project includes a forecast of internal engineer personnel time expected to be incurred and other third-party costs as applicable.Contracts with Multiple Performance ObligationsOur contracts may contain more than one performance obligation.Judgement is required in determining whether each performance obligation within a customercontract is distinct.Except for License and Development Arrangements, NVIDIA products and services function on a standalone basis and do not require asignificant amount of integration or interdependency.Therefore, multiple performance obligations contained within a customer contract are considered distinctand are not combined for revenue recognition purposes.We allocate the total transaction price to each distinct performance obligation in a multiple performance obligations arrangement on a relative standalone sellingprice basis.In certain cases, we can establish standalone selling price based on directly observable prices of products or services sold separately in comparablecircumstances to similar customers.If standalone selling price is not directly observable, such as when we do not sell a product or service separately, wedetermine standalone selling price based on market data and other observable inputs.Change in Accounting EstimateIn February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of a majority of the server, storage, and network equipment from three years to a range of four to five years, and assembly and testequipment from five years to seven years.The estimated effect of this change for fiscal year 2024 was a benefit of $33 million and $102 million for cost ofrevenue and operating expenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or$0.05 per both basic and diluted share.Results of OperationsA discussion regarding our financial condition and results of operations for fiscal year 2024 compared to fiscal year 2023 is presented below.A discussionregarding our financial condition and results of operations for fiscal year 2023 compared to fiscal year 2022 can be found under Item 7 in our Annual Report onForm 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38", "page_start": 38, "page_end": 38, "section_title": "NVIDIA CORPORATION", "score": 0.57723856}, {"chunk_id": "16846335-f2af-4650-bf60-d4b5485698d4", "text": "Principles of ConsolidationOur consolidated financial statements include the accounts of NVIDIA Corporation and our wholly-owned subsidiaries.All intercompany balances andtransactions have been eliminated in consolidation.Use of EstimatesThe preparation of financial statements in conformity with U.S.GAAP requires management to make estimates and assumptions that affect the reportedamounts of assets and liabilities and disclosures of contingent assets and liabilities at the date of the financial statements and the reported amounts of revenueand expenses during the reporting period.Actual results could differ materially from our estimates.On an on-going basis, we evaluate our estimates, includingthose related to revenue recognition, cash equivalents and marketable securities, accounts receivable, inventories and product purchase commitments, incometaxes, goodwill, stock-based compensation, litigation, investigation and settlement costs, restructuring and other charges, property, plant, and equipment, andother contingencies.These estimates are based on historical facts and various other assumptions that we believe are reasonable.In February 2023, we assessed the useful lives of our property, plant, and equipment.Based on advances in technology and usage rate, we increased theestimated useful life of most of our server, storage, and network equipment from three to four or five years, and our assembly and test equipment from five toseven years.The effect of this change for the fiscal year ended January 28, 2024 was a benefit of $33 million and $102 million for cost of revenue and operatingexpenses, respectively, which resulted in an increase in operating income of $135 million and net income of $114 million after tax, or $0.05 per both basic anddiluted share.Revenue RecognitionWe derive our revenue from product sales, including hardware and systems, license and development arrangements, software licensing, and cloud services.Wedetermine revenue recognition through the following steps: (1) identification of the contract with a customer; (2) identification of the performance obligations inthe contract; (3) determination of the transaction price; (4) allocation of the transaction price to the performance obligations in the contract (where revenue isallocated on a relative standalone selling price basis by maximizing the use of observable inputs to determine the standalone selling price for each performanceobligation); and (5) recognition of revenue when, or as, we satisfy a performance obligation.Product Sales RevenueRevenue from product sales is recognized upon transfer of control of products to customers in an amount that reflects the consideration we expect to receive inexchange for those products.Certain products are sold with support or an extended warranty for the incorporated system, hardware, and/or software.Supportand extended warranty revenue are recognized ratably over the service period, or as services are performed.Revenue is recognized net of allowances forreturns, customer programs and any taxes collected from customers.For products sold with a right of return, we record a reduction to revenue by establishing a sales return allowance for estimated product returns at the timerevenue is recognized, based primarily on historical return rates.", "page_start": 55, "page_end": 55, "section_title": "NVIDIA CORPORATION", "score": 0.56978095}], "adversarial_nda_01": [{"chunk_id": "6c64d139-fd3d-400d-b64b-3ddf0dea3473", "text": "except the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.The English Courts will have non-exclusive jurisdiction to deal with any dispute whichhas arisen or may arise out of, or in connection with, this Agreement.Signed [by [insert name]] OR [on behalf of][insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________PositionSigned [by [insert name]] OR [on behalf of] [insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________Position", "page_start": 2, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.44240564}, {"chunk_id": "94d954fb-950b-4dcf-8049-c72deb8243c9", "text": "Date: 201[ ]Parties:[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]and[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]1.Each of the parties to this Agreement intends to disclose information (the ConfidentialInformation) to the other party for the purpose of [insert details e.g.discussing thepossibility of the parties entering into a joint venture] (the Purpose).2.Each party to this Agreement is referred to as ‘the Recipient’ when it receives or usesthe Confidential Information disclosed by the other party.3.The Recipient undertakes not to use the Confidential Information disclosed by the otherparty for any purpose except the Purpose, without first obtaining the written agreementof the other party.4.The Recipient undertakes to keep the Confidential Information disclosed by the otherparty secure and not to disclose it to any third party [except to its employees [andprofessional advisers] who need to know the same for the Purpose, who know theyowe a duty of confidence to the other party and who are bound by obligationsequivalent to those in clause 3 above and this clause 4.5.The undertakings in clauses 3 and 4 above apply to all of the information disclosed byeach of the parties to the other, regardless of the way or form in which it is disclosed orrecorded but they do not apply to:a) any information which is or in future comes into the public domain (unless as a result ofthe breach of this Agreement); orb) any information which is already known to the Recipient and which was not subject toany obligation of confidence before it was disclosed to the Recipient by the other party.6.Nothing in this Agreement will prevent the Recipient from making any disclosure of theConfidential Information required by law or by any competent authority.7.The Recipient will, on request from the other party, return all copies and records of theConfidential Information disclosed by the other party to the Recipient and will not retainany copies or records of the Confidential Information disclosed by the other party.8.Neither this Agreement nor the supply of any information grants the Recipient anylicence, interest or right in respect of any intellectual property rights of the other partyexcept the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.", "page_start": 1, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.43813846}], "ambiguous_attention_01": [{"chunk_id": "0b373c99-8c0e-44bf-8fc7-f6b6b009590d", "text": "Provided proper attribution is provided, Google hereby grants permission toreproduce the tables and figures in this paper solely for use in journalistic orscholarly works.Ashish Vaswani∗Noam Shazeer∗Niki Parmar∗Jakob Uszkoreit∗Google BrainGoogle BrainGoogle ResearchGoogle Researchavaswani@google.comnoam@google.comnikip@google.comusz@google.comLlion Jones∗Aidan N.Gomez∗†Łukasz Kaiser∗Google ResearchUniversity of TorontoGoogle Brainlukaszkaiser@google.comllion@google.comaidan@cs.toronto.eduIllia Polosukhin∗‡illia.polosukhin@gmail.comAbstractThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder.The bestperforming models also connect the encoder and decoder through an attentionmechanism.We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely.Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring significantlyless time to train.Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU.On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.8 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature.We show that the Transformer generalizes well toother tasks by applying it successfully to English constituency parsing both withlarge and limited training data.∗Equal contribution.Listing order is random.Jakob proposed replacing RNNs with self-attention and startedthe effort to evaluate this idea.Ashish, with Illia, designed and implemented the first Transformer models andhas been crucially involved in every aspect of this work.Noam proposed scaled dot-product attention, multi-headattention and the parameter-free position representation and became the other person involved in nearly everydetail.Niki designed, implemented, tuned and evaluated countless model variants in our original codebase andtensor2tensor.Llion also experimented with novel model variants, was responsible for our initial codebase, andefficient inference and visualizations.Lukasz and Aidan spent countless long days designing various parts of andimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks", "page_start": 1, "page_end": 2, "section_title": "", "score": 0.52887523}, {"chunk_id": "5151352b-ae46-44f3-83b0-8b8748547941", "text": "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.1IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networksin particular, have been firmly established as state of the art approaches in sequence modeling andtransduction problems such as language modeling and machine translation [35, 2, 5].Numerousefforts have since continued to push the boundaries of recurrent language models and encoder-decoderarchitectures [38, 24, 15].Recurrent models typically factor computation along the symbol positions of the input and outputsequences.Aligning the positions to steps in computation time, they generate a sequence of hiddenstates ht, as a function of the previous hidden state ht−1 and the input for position t.This inherentlysequential nature precludes parallelization within training examples, which becomes critical at longersequence lengths, as memory constraints limit batching across examples.Recent work has achievedsignificant improvements in computational efficiency through factorization tricks [21] and conditionalcomputation [32], while also improving model performance in case of the latter.The fundamentalconstraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance inthe input or output sequences [2, 19].In all but a few cases [27], however, such attention mechanismsare used in conjunction with a recurrent network.In this work we propose the Transformer, a model architecture eschewing recurrence and insteadrelying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for significantly more parallelization and can reach a new state of the art intranslation quality after being trained for as little as twelve hours on eight P100 GPUs.2BackgroundThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic buildingblock, computing hidden representations in parallel for all input and output positions.In these models,the number of operations required to relate signals from two arbitrary input or output positions growsin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.This makesit more difficult to learn dependencies between distant positions [12].In the Transformer this isreduced to a constant number of operations, albeit at the cost of reduced effective resolution dueto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,", "page_start": 1, "page_end": 2, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.5176841}, {"chunk_id": "6ecb21e7-c073-4d90-9d8a-41fdfda06afb", "text": "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence.Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,textual entailment and learning task-independent sentence representations [4, 27, 28, 22].End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering andlanguage modeling tasks [34].To the best of our knowledge, however, the Transformer is the first transduction model relyingentirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.In the following sections, we will describe the Transformer, motivateself-attention and discuss its advantages over models such as [17, 18] and [9].3Model ArchitectureMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequenceof continuous representations z = (z1, ..., zn).Given z, the decoder then generates an outputsequence (y1, ..., ym) of symbols one element at a time.At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next.2Figure 1: The Transformer - model architecture.The Transformer follows this overall architecture using stacked self-attention and point-wise, fullyconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,respectively.3.1Encoder and Decoder StacksThe encoder is composed of a stack of N = 6 identical layers.Each layer has twoEncoder:sub-layers.The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network.We employ a residual connection [11] around each ofthe two sub-layers, followed by layer normalization [1].That is, the output of each sub-layer isLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layeritself.To facilitate these residual connections, all sub-layers in the model, as well as the embeddinglayers, produce outputs of dimension dmodel = 512.The decoder is also composed of a stack of N = 6 identical layers.In addition to the twoDecoder:sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-headattention over the output of the encoder stack.Similar to the encoder, we employ residual connectionsaround each of the sub-layers, followed by layer normalization.We also modify the self-attentionsub-layer in the decoder stack to prevent positions from attending to subsequent positions.Thismasking, combined with fact that the output embeddings are offset by one position, ensures that the", "page_start": 2, "page_end": 3, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.5156296}, {"chunk_id": "914cfb0b-3d4a-4988-8392-851b64c13e57", "text": "The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.Weused beam search with a beam size of 4 and length penalty α = 0.6 [38].These hyperparameterswere chosen after experimentation on the development set.We set the maximum output length duringinference to input length + 50, but terminate early when possible [38].Table 2 summarizes our results and compares our translation quality and training costs to other modelarchitectures from the literature.We estimate the number of floating point operations used to train amodel by multiplying the training time, the number of GPUs used, and an estimate of the sustainedsingle-precision floating-point capacity of each GPU 5.6.2Model VariationsTo evaluate the importance of different components of the Transformer, we varied our base modelin different ways, measuring the change in performance on English-to-German translation on the5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.8Table 3: Variations on the Transformer architecture.Unlisted values are identical to those of the basemodel.All metrics are on the English-to-German translation development set, newstest2013.Listedperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared toper-word perplexities.trainPPLBLEUparamsNdmodeldffhdkdvPdropϵls×106steps(dev)(dev)base65122048864640.10.1100K4.9225.86515125125.2924.941281285.0025.5(A)1632324.9125.83216165.0125.4165.1625.158(B)325.0125.46026.1123.73645.1925.35084.8825.580(C)25632325.7524.52810241281284.6626.016810245.1225.45340964.7526.2900.05.7724.60.24.9525.5(D)0.04.6725.30.25.4725.7(E)positional embedding instead of sinusoids4.9225.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.", "page_start": 8, "page_end": 9, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.46965012}, {"chunk_id": "bb77d3cc-59f9-4b97-a21b-132b30215259", "text": "semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised92.1Transformer (4 layers)semi-supervised92.7Luong et al.(2015) [23]multi-task93.0Dyer et al.(2016) [8]generative93.3increased the maximum output length to input length + 300.We used a beam size of 21 and α = 0.3for both WSJ only and the semi-supervised setting.Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of theRecurrent Neural Network Grammar [8].In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.7ConclusionIn this work, we presented the Transformer, the first sequence transduction model based entirely onattention, replacing the recurrent layers most commonly used in encoder-decoder architectures withmulti-headed self-attention.For translation tasks, the Transformer can be trained significantly faster than architectures basedon recurrent or convolutional layers.On both WMT 2014 English-to-German and WMT 2014English-to-French translation tasks, we achieve a new state of the art.In the former task our bestmodel outperforms even all previously reported ensembles.We are excited about the future of attention-based models and plan to apply them to other tasks.Weplan to extend the Transformer to problems involving input and output modalities other than text andto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputssuch as images, audio and video.Making generation less sequential is another research goals of ours.The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.AcknowledgementsWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitfulcomments, corrections and inspiration.References[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprintarXiv:1607.06450, 2016.[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.Neural machine translation by jointlylearning to align and translate.CoRR, abs/1409.0473, 2014.[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V.Le.Massive exploration of neuralmachine translation architectures.CoRR, abs/1703.03906, 2017.[4] Jianpeng Cheng, Li Dong, and Mirella Lapata.Long short-term memory-networks for machinereading.arXiv preprint arXiv:1601.06733, 2016.10[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,", "page_start": 10, "page_end": 11, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.45866358}, {"chunk_id": "160dd6c9-8965-4dcd-8d74-e0dd157c4fd1", "text": "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,and decreasing it thereafter proportionally to the inverse square root of the step number.We usedwarmup_steps = 4000.5.4RegularizationWe employ three types of regularization during training:7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on theEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.BLEUTraining Cost (FLOPs)ModelEN-DEEN-FREN-DEEN-FRByteNet [18]23.751.0 · 1020Deep-Att + PosUnk [39]39.22.3 · 10191.4 · 1020GNMT + RL [38]24.639.929.6 · 10181.5 · 1020ConvS2S [9]25.1640.462.0 · 10191.2 · 1020MoE [32]26.0340.568.0 · 1020Deep-Att + PosUnk Ensemble [39]40.41.8 · 10201.1 · 1021GNMT + RL Ensemble [38]26.3041.167.7 · 10191.2 · 102141.29ConvS2S Ensemble [9]26.363.3 · 1018Transformer (base model)27.338.12.3 · 101928.441.8Transformer (big)Residual DropoutWe apply dropout [33] to the output of each sub-layer, before it is added to thesub-layer input and normalized.In addition, we apply dropout to the sums of the embeddings and thepositional encodings in both the encoder and decoder stacks.For the base model, we use a rate ofPdrop = 0.1.During training, we employed label smoothing of value ϵls = 0.1 [36].ThisLabel Smoothinghurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.6Results6.1Machine TranslationOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0BLEU, establishing a new state-of-the-art BLEU score of 28.4.The configuration of this model islisted in the bottom line of Table 3.Training took 3.5 days on 8 P100 GPUs.Even our base modelsurpasses all previously published models and ensembles, at a fraction of the training cost of any ofthe competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,outperforming all of the previously published single models, at less than 1/4 the training cost of theprevious state-of-the-art model.The Transformer (big) model trained for English-to-French useddropout rate Pdrop = 0.1, instead of 0.3.For the base models, we used a single model obtained by averaging the last 5 checkpoints, whichwere written at 10-minute intervals.For the big models, we averaged the last 20 checkpoints.We", "page_start": 7, "page_end": 8, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.44788492}, {"chunk_id": "8d9a2784-c6bd-4675-a20b-80dd0948329f", "text": "25.74.3326.4big610244096160.3300K213development set, newstest2013.We used beam search as described in the previous section, but nocheckpoint averaging.We present these results in Table 3.In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,keeping the amount of computation constant, as described in Section 3.2.2.While single-headattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality.Thissuggests that determining compatibility is not easy and that a more sophisticated compatibilityfunction than dot product may be beneficial.We further observe in rows (C) and (D) that, as expected,bigger models are better, and dropout is very helpful in avoiding over-fitting.In row (E) we replace oursinusoidal positional encoding with learned positional embeddings [9], and observe nearly identicalresults to the base model.6.3English Constituency ParsingTo evaluate if the Transformer can generalize to other tasks we performed experiments on Englishconstituency parsing.This task presents specific challenges: the output is subject to strong structuralconstraints and is significantly longer than the input.Furthermore, RNN sequence-to-sequencemodels have not been able to attain state-of-the-art results in small-data regimes [37].We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of thePenn Treebank [25], about 40K training sentences.We also trained it in a semi-supervised setting,using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences[37].We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokensfor the semi-supervised setting.We performed only a small number of experiments to select the dropout, both attention and residual(section 5.4), learning rates and beam size on the Section 22 development set, all other parametersremained unchanged from the English-to-German base translation model.During inference, we9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23of WSJ)ParserTrainingWSJ 23 F1Vinyals & Kaiser el al.(2014) [37]WSJ only, discriminative88.3Petrov et al.(2006) [29]WSJ only, discriminative90.4Zhu et al.(2013) [40]WSJ only, discriminative90.4Dyer et al.(2016) [8]WSJ only, discriminative91.7Transformer (4 layers)WSJ only, discriminative91.3Zhu et al.(2013) [40]semi-supervised91.3Huang & Harper (2009) [14]semi-supervised91.3McClosky et al.(2006) [26]semi-supervised92.1Vinyals & Kaiser el al.(2014) [37]semi-supervised", "page_start": 9, "page_end": 10, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.4296245}, {"chunk_id": "58f1c685-b5d0-4ba4-8711-0ba5768f729c", "text": "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,we found it beneficial to linearly project the queries, keys and values h times with different, learnedlinear projections to dk, dk and dv dimensions, respectively.On each of these projected versions ofqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional4To illustrate why the dot products get large, assume that the components of q and k are independent randomvariables with mean 0 and variance 1.Then their dot product, q · k = Pdki=1 qiki, has mean 0 and variance dk.4output values.These are concatenated and once again projected, resulting in the final values, asdepicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representationsubspaces at different positions.With a single attention head, averaging inhibits this.MultiHead(Q, K, V ) = Concat(head1, ..., headh)W Owhere headi = Attention(QW Qi , KW Ki , V W Vi )Where the projections are parameter matrices W Q∈Rdmodel×dk, W V∈Rdmodel×dv∈Rdmodel×dk, W Kiiiand W O ∈Rhdv×dmodel.In this work we employ h = 8 parallel attention layers, or heads.For each of these we usedk = dv = dmodel/h = 64.Due to the reduced dimension of each head, the total computational costis similar to that of single-head attention with full dimensionality.3.2.3Applications of Attention in our ModelThe Transformer uses multi-head attention in three different ways:• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,and the memory keys and values come from the output of the encoder.This allows everyposition in the decoder to attend over all positions in the input sequence.This mimics thetypical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38, 2, 9].• The encoder contains self-attention layers.In a self-attention layer all of the keys, valuesand queries come from the same place, in this case, the output of the previous layer in theencoder.Each position in the encoder can attend to all positions in the previous layer of theencoder.• Similarly, self-attention layers in the decoder allow each position in the decoder to attend toall positions in the decoder up to and including that position.We need to prevent leftwardinformation flow in the decoder to preserve the auto-regressive property.We implement thisinside of scaled dot-product attention by masking out (setting to −∞) all values in the inputof the softmax which correspond to illegal connections.See Figure 2.3.3Position-wise Feed-Forward NetworksIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fullyconnected feed-forward network, which is applied to each position separately and identically.This", "page_start": 4, "page_end": 5, "section_title": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023", "score": 0.42439267}], "ambiguous_nvidia_01": [{"chunk_id": "af665b9d-5bc9-4f58-8c86-b860d78f48e8", "text": "TOP500 list, including 24 of the top 30 systems on the Green500 list.Gamers choose NVIDIA GPUs to enjoy immersive, increasingly cinematic virtual worlds.In addition to serving the growing number of gamers, the market for PCGPUs is expanding because of the burgeoning population of live streamers, broadcasters, artists, and creators.With the advent of generative AI, we expect abroader set of PC users to choose NVIDIA GPUs for running generative AI applications locally on their PC, which is critical for privacy, latency, and cost-sensitiveAI applications.Professional artists, architects and designers use NVIDIA partner products accelerated with our GPUs and software platform for a range of creative and designuse cases, such as creating visual effects in movies or designing buildings and products.In addition, generative AI is expanding the market for our workstationclass GPUs, as more enterprise customers develop and deploy AI applications with their data on-premises.Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.Our BusinessesWe report our business results in two segments.The Compute & Networking segment is comprised of our Data Center accelerated computing platforms and end-to-end networking platforms including Quantumfor InfiniBand and Spectrum for Ethernet; our NVIDIA DRIVE automated-driving platform and automotive development agreements; Jetson robotics and otherembedded platforms; NVIDIA AI Enterprise and other software; and DGX Cloud software and services.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure; Quadro/NVIDIARTX GPUs for enterprise workstation graphics; virtual GPU, or vGPU, software for cloud-based visual and virtual computing; automotive platforms forinfotainment systems; and Omniverse Enterprise software for building and operating metaverse and 3D internet applications.Our MarketsWe specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Data CenterThe NVIDIA Data Center platform is focused on accelerating the most compute-intensive workloads, such as AI, data analytics, graphics and scientificcomputing, delivering significantly better performance and power efficiency relative to conventional CPU-only approaches.It is deployed in cloud, hyperscale,on-premises and edge data centers.The platform consists of compute and networking offerings typically delivered to customers as systems, subsystems, ormodules, along with software and services.Our compute offerings include supercomputing platforms and servers, bringing together our energy efficient GPUs, DPUs, interconnects, and fully optimized AIand high-performance computing, or HPC, software stacks.In addition, they include NVIDIA AI Enterprise software; our DGX Cloud service; and a growing bodyof acceleration libraries, APIs, SDKs, and domain-specific application frameworks.Our networking offerings include end-to-end platforms for InfiniBand and Ethernet, consisting of network adapters, cables, DPUs, and switch systems, as well asa full software stack.", "page_start": 5, "page_end": 5, "section_title": "NVIDIA CORPORATION", "score": 0.66951466}, {"chunk_id": "f2207e14-4c6c-421b-849b-76359737cca1", "text": "Germany, India, Israel, and Taiwan for fiscal years 2005 through 2023.Note 15 - Shareholders’ EquityCapital Return ProgramIn August 2023, our Board of Directors approved an increase to our share repurchase program of an additional $25.0 billion, without expiration.During fiscalyear 2024, we repurchased 21 million shares of our common stock for $9.7 billion.As of January 28, 2024, we were authorized, subject to certain specifications,to repurchase additional shares of our common stock up to $22.5 billion.From January 29, 2024 through February 16, 2024, we repurchased 2.8 million sharesfor $1.9 billion pursuant to a Rule 10b5-1 trading plan.Our share repurchase program aims to offset dilution from shares issued to employees.We may pursueadditional share repurchases as we weigh market factors and other investment opportunities.During fiscal years 2024, 2023, and 2022, we paid $395 million, $398 million, and $399 million in cash dividends to our shareholders, respectively.Our cashdividend program and the payment of future cash dividends under that program are subject to our Board of Directors' continuing determination that the dividendprogram and the declaration of dividends thereunder are in the best interests of our shareholders.In fiscal year 2022, we retired our existing 349 million treasury shares.These shares assumed the status of authorized and unissued shares upon retirement.The excess of repurchase price over par value was allocated between additional paid-in capital and retained earnings, resulting in a reduction in additional paidin capital by $20 million and retained earnings by $12.0 billion.Any future repurchased shares will assume the status of authorized and unissued shares.77NVIDIA Corporation and SubsidiariesNotes to the Consolidated Financial Statements(Continued)Note 16 - Employee Retirement PlansWe provide tax-qualified defined contribution plans to eligible employees in the U.S.and certain other countries.Our contribution expense for fiscal years 2024,2023, and 2022 was $255 million, $227 million, and $168 million, respectively.Note 17 - Segment InformationOur Chief Executive Officer, who is considered to be our chief operating decision maker, or CODM, reviews financial information presented on an operatingsegment basis for purposes of making decisions and assessing financial performance.The Compute & Networking segment includes our Data Center accelerated computing platform; networking; automotive artificial intelligence, or AI, Cockpit,autonomous driving development agreements, and autonomous vehicle solutions; electric vehicle computing platforms; Jetson for robotics and other embeddedplatforms; NVIDIA AI Enterprise and other software; and DGX Cloud.The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions forgaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU software for cloud-based visual and virtual computing; automotiveplatforms for infotainment systems; and Omniverse Enterprise software for building and operating 3D internet applications.Operating results by segment include costs or expenses that are directly attributable to each segment, and costs or expenses that are leveraged across our", "page_start": 77, "page_end": 78, "section_title": "NVIDIA CORPORATION", "score": 0.6448556}, {"chunk_id": "a012dfff-19d8-49a4-b8a0-ec4462727f0c", "text": "markets with shared underlying technology by using a variety of software stacks developed either internally or by third-party developers and partners.We utilizethis platform approach in each of our target markets.Extending our technology and platform leadership in AI.We provide a complete, end-to-end accelerated computing platform for AI, addressing both trainingand inferencing.This includes full-stack data center-scale compute and networking solutions across processing units, interconnects, systems, and software.Ourcompute solutions include all three major processing units in AI servers – GPUs, CPUs, and DPUs.GPUs are uniquely suited to AI, and we will continue to addAI-specific features to our GPU architecture to further extend our leadership position.In addition, we offer DGX Cloud, an AI-training-as-a-service platform, andNeMo – a complete solution for building enterprise-ready Large Language Models, or LLMs, using open source and proprietary LLMs created by NVIDIA andthird parties.Our AI technology leadership is reinforced by our large and expanding ecosystem in a virtuous cycle.Our computing platforms are available fromvirtually every major server maker and CSP, as well as on our own AI supercomputers.There are over 4.7 million developers worldwide using CUDA and ourother software tools to help deploy our technology in our target markets.We evangelize AI through partnerships with hundreds of universities and thousands ofstartups through our Inception program.Additionally, our Deep Learning Institute provides instruction on the latest techniques on how to design, train, and deployneural networks in applications using our accelerated computing platform.Extending our technology and platform leadership in computer graphics.We believe that computer graphics infused with AI is fundamental to thecontinued expansion and evolution of computing.We apply our research and development resources to enhance the user experience for consumerentertainment and professional visualization applications and create new virtual world and simulation capabilities.Our technologies are instrumental in drivingthe gaming, design, and creative industries forward, as developers leverage our libraries and algorithms to deliver an optimized experience on our GeForce andNVIDIA RTX platforms.Our computer graphics platforms leverage AI end-to-end, from the developer tools and cloud services to the Tensor Cores included in allRTX-class GPUs.For example, NVIDIA Avatar Cloud Engine, or ACE, is a suite of technologies that help developers bring digital avatars to life with generativeAI, running in the cloud or locally on the PC.GeForce Experience enhances each gamer’s experience by optimizing their PC’s settings, as well as enabling therecording and sharing of gameplay.Our Studio drivers enhance and accelerate a number of popular creative applications.Omniverse is real-time 3D designcollaboration and virtual world simulation software that empowers artists, designers, and creators to connect and collaborate in leading design applications.Wealso enable interactive graphics applications - such as games, movie and photo editing and design software - to be accessed by almost any device, almostanywhere, through our cloud platforms such as vGPU for enterprise and GeForce NOW for gaming.Advancing the leading autonomous vehicle platform.", "page_start": 7, "page_end": 7, "section_title": "NVIDIA CORPORATION", "score": 0.6423798}, {"chunk_id": "90af9f4f-085c-4a86-89fa-ee7b46a3f3b4", "text": "Forward-Looking StatementsThis Annual Report on Form 10-K contains forward-looking statements which are based on our management's beliefs and assumptions and on informationcurrently available to our management.In some cases, you can identify forward-looking statements by terms such as “may,” “will,” “should,” “could,” “goal,”“would,” “expect,” “plan,” “anticipate,” “believe,” “estimate,” “project,” “predict,” “potential,” and similar expressions intended to identify forward-lookingstatements.These statements involve known and unknown risks, uncertainties and other factors, which may cause our actual results, performance, time framesor achievements to be materially different from any future results, performance, time frames or achievements expressed or implied by the forward-lookingstatements.We discuss many of these risks, uncertainties, and other factors in this Annual Report on Form 10-K in greater detail under the heading “RiskFactors.” Given these risks, uncertainties, and other factors, you should not place undue reliance on these forward-looking statements.Also, these forwardlooking statements represent our estimates and assumptions only as of the date of this filing.You should read this Annual Report on Form 10-K completely andwith the understanding that our actual future results may be materially different from what we expect.We hereby qualify our forward-looking statements by thesecautionary statements.Except as required by law, we assume no obligation to update these forward-looking statements publicly, or to update the reasons actualresults could differ materially from those anticipated in these forward-looking statements, even if new information becomes available in the future.All references to “NVIDIA,” “we,” “us,” “our,” or the “Company” mean NVIDIA Corporation and its subsidiaries.In addition, statements that “we believe” and similar statements reflect our beliefs and opinions on the relevant subject.These statements are based uponinformation available to us as of the filing date of this Annual Report on Form 10-K, and while we believe such information forms a reasonable basis for suchstatements, such information may be limited or incomplete, and our statements should not be read to indicate that we have conducted an exhaustive inquiry into,or review of, all potentially available relevant information.These statements are inherently uncertain and investors are cautioned not to unduly rely upon thesestatements.© 2024 NVIDIA Corporation.All rights reserved.3Part IItem 1.BusinessOur CompanyNVIDIA pioneered accelerated computing to help solve the most challenging computational problems.NVIDIA is now a full-stack computing infrastructurecompany with data-center-scale offerings that are reshaping industry.Our full-stack includes the foundational CUDA programming model that runs on all NVIDIA GPUs, as well as hundreds of domain-specific software libraries,software development kits, or SDKs, and Application Programming Interfaces, or APIs.This deep and broad software stack accelerates the performance and", "page_start": 3, "page_end": 4, "section_title": "NVIDIA CORPORATION", "score": 0.6229981}, {"chunk_id": "f9f46931-b66e-4235-8451-f372d6442a23", "text": "In addition, they include NVIDIA AI Enterprise software; our DGX Cloud service; and a growing bodyof acceleration libraries, APIs, SDKs, and domain-specific application frameworks.Our networking offerings include end-to-end platforms for InfiniBand and Ethernet, consisting of network adapters, cables, DPUs, and switch systems, as well asa full software stack.This has enabled us to architect data center-scale computing platforms that can interconnect thousands of compute nodes with highperformance networking.While historically the server was the unit of computing, as AI and HPC workloads have become extremely large spanning thousands ofcompute nodes, the data center has become the new unit of computing, with networking as an integral part.Our end customers include the world’s leading public cloud and consumer internet companies, thousands of enterprises and startups, and public sector entities.We work with industry leaders to help build or transform their applications and data center infrastructure.Our direct customers include original equipmentmanufacturers, or OEMs, original device manufacturers, or ODMs, system integrators and distributors which we partner with to help bring our products tomarket.We also have partnerships in automotive, healthcare, financial services, manufacturing, and retail among others, to accelerate the adoption of AI.5At the foundation of the NVIDIA accelerated computing platform are our GPUs, which excel at parallel workloads such as the training and inferencing of neuralnetworks.They are available in the NVIDIA accelerated computing platform and in industry standard servers from every major cloud provider and server maker.Beyond GPUs, our data center platform expanded to include DPUs in fiscal year 2022 and CPUs in fiscal year 2024.We can optimize across the entirecomputing, networking and storage stack to deliver data center-scale computing solutions.While our approach starts with powerful chips, what makes it a full-stack computing platform is our large body of software, including the CUDA parallelprogramming model, the CUDA-X collection of acceleration libraries, APIs, SDKs, and domain-specific application frameworks.In addition to software delivered to customers as an integral part of our data center computing platform, we offer paid licenses to NVIDIA AI Enterprise, acomprehensive suite of enterprise-grade AI software and NVIDIA vGPU software for graphics-rich virtual desktops and workstations.In fiscal year 2024, we launched the NVIDIA DGX Cloud, an AI-training-as-a-service platform which includes cloud-based infrastructure and software for AI,customizable pretrained AI models, and access to NVIDIA experts.We have partnered with leading cloud service providers to host this service in their datacenters.GamingGaming is the largest entertainment industry, with PC gaming as the predominant platform.Many factors propel its growth, including new high production valuegames and franchises, the continued rise of competitive gaming, or eSports, social connectivity and the increasing popularity of game streamers, modders, orgamers who remaster games, and creators.Our gaming platforms leverage our GPUs and sophisticated software to enhance the gaming experience with smoother, higher quality graphics.", "page_start": 5, "page_end": 6, "section_title": "NVIDIA CORPORATION", "score": 0.6194633}, {"chunk_id": "f1afdb9e-f35c-4c22-a847-c7d9a35572e3", "text": "Many factors propel its growth, including new high production valuegames and franchises, the continued rise of competitive gaming, or eSports, social connectivity and the increasing popularity of game streamers, modders, orgamers who remaster games, and creators.Our gaming platforms leverage our GPUs and sophisticated software to enhance the gaming experience with smoother, higher quality graphics.We developedNVIDIA RTX to bring next generation graphics and AI to games.NVIDIA RTX features ray tracing technology for real-time, cinematic-quality rendering.Raytracing, which has long been used for special effects in the movie industry, is a computationally intensive technique that simulates the physical behavior of lightto achieve greater realism in computer-generated scenes.NVIDIA RTX also features deep learning super sampling, or NVIDIA DLSS, our AI technology thatboosts frame rates while generating beautiful, sharp images for games.RTX GPUs will also accelerate a new generation of AI applications.With an installedbase of over 100 million AI capable PCs, more than 500 RTX AI-enabled applications and games, and a robust suite of development tools, RTX is already the AIPC leader.Our products for the gaming market include GeForce RTX and GeForce GTX GPUs for gaming desktop and laptop PCs, GeForce NOW cloud gaming forplaying PC games on underpowered devices, as well as SoCs and development services for game consoles.Professional VisualizationWe serve the Professional Visualization market by working closely with independent software vendors, or ISVs, to optimize their offerings for NVIDIA GPUs.OurGPU computing platform enhances productivity and introduces new capabilities for critical workflows in many fields, such as design and manufacturing anddigital content creation.Design and manufacturing encompass computer-aided design, architectural design, consumer-products manufacturing, medicalinstrumentation, and aerospace.Digital content creation includes professional video editing and post-production, special effects for films, and broadcasttelevision graphics.The NVIDIA RTX platform makes it possible to render film-quality, photorealistic objects and environments with physically accurate shadows, reflections andrefractions using ray tracing in real-time.Many leading 3D design and content creation applications developed by our ecosystem partners now support RTX,allowing professionals to accelerate and transform their workflows with NVIDIA RTX GPUs and software.We offer NVIDIA Omniverse as a development platform and operating system for building virtual world simulation applications, available as a softwaresubscription for enterprise use and free for individual use.Industrial enterprises are adopting Omniverse’s 3D and simulation technologies to digitalize theircomplex physical assets, processes, and environments – building digital twins of factories, real time 3D product configurators, testing and validating autonomousrobots and vehicles, powered by NVIDIA accelerated computing infrastructure on-premises and in the cloud.AutomotiveAutomotive market is comprised of platform solutions for automated driving and in-vehicle cockpit computing.Leveraging our technology leadership in AI andbuilding on our long-standing automotive relationships, we are delivering a complete end-to-end solution for the AV market under the DRIVE Hyperion brand.We", "page_start": 6, "page_end": 6, "section_title": "NVIDIA CORPORATION", "score": 0.6180104}, {"chunk_id": "823438dd-9ce8-4ae9-924a-70c6bbf5121f", "text": "Form 10-K for the fiscal year ended January 29, 2023, filed with the SEC on February 24, 2023, which is available free of charge on the SEC’s website athttp://www.sec.gov and at our investor relations website, http://investor.nvidia.com.38The following table sets forth, for the periods indicated, certain items in our Consolidated Statements of Income expressed as a percentage of revenue.Year EndedJan 28, 2024Jan 29, 2023Revenue100.0 %100.0 %Cost of revenue27.343.1Gross profit72.756.9Operating expensesResearch and development14.227.2Sales, general and administrative4.49.1Acquisition termination cost—5.0Total operating expenses18.641.3Operating income54.115.6Interest income1.41.0Interest expense(0.4)(1.0)0.4(0.1)Other, net1.4(0.1)Other income (expense), netIncome before income tax55.515.5Income tax expense (benefit)6.6(0.7)48.9 %16.2 %Net incomeReportable SegmentsRevenue by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$47,405$15,068$32,337215 %13,51711,9061,611Graphics14 %$60,922$26,974$33,948Total126 %Operating Income by Reportable SegmentsYear Ended$%Jan 28, 2024Jan 29, 2023ChangeChange($ in millions)Compute & Networking$32,016$5,083$26,933530 %Graphics5,8464,5521,29428 %(4,890)(5,411)521All Other(10)%$32,972$4,224$28,748Total681 %Compute & Networking revenue – The year-on-year increase was due to higher Data Center revenue.Compute grew 266% due to higher shipments of theNVIDIA Hopper GPU computing platform for the training and inference of LLMs, recommendation engines and generative AI applications.Networking was up133% due to higher shipments of InfiniBand.Graphics revenue – The year-on-year increase was led by growth in Gaming of 15% driven by higher sell-in to partners following the normalization of channelinventory levels.Reportable segment operating income – The year-on-year increase in Compute & Networking and Graphics operating income was driven by higher revenue.39All Other operating loss - The year-on-year decrease was due to the $1.4 billion Arm acquisition termination cost in fiscal year 2023, partially offset by a $839million increase in stock-based compensation expense in fiscal year 2024.Concentration of Revenue", "page_start": 38, "page_end": 40, "section_title": "NVIDIA CORPORATION", "score": 0.61289287}, {"chunk_id": "ddc05d6e-79f6-4ed4-a29b-876d9e76fd48", "text": "We did not experience any significant impact or expense toour business; however, if the conflict is further extended, it could impact future product development, operations, and revenue or create other uncertainty for ourbusiness.35Fiscal Year 2024 SummaryYear EndedJan 28, 2024Jan 29, 2023Change($ in millions, except per share data)Revenue$60,922$26,974Up 126%Gross margin72.7 %56.9 %Up 15.8 ptsOperating expenses$11,329$11,132Up 2%Operating income$32,972$4,224Up 681%Net income$29,760$4,368Up 581%Net income per diluted share$11.93$1.74Up 586%We specialize in markets where our computing platforms can provide tremendous acceleration for applications.These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value.Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive.Revenue for fiscal year 2024 was $60.9 billion, up 126% from a year ago.Data Center revenue for fiscal year 2024 was up 217%.Strong demand was driven by enterprise software and consumer internet applications, and multipleindustry verticals including automotive, financial services, and healthcare.Customers across industry verticals access NVIDIA AI infrastructure both through thecloud and on-premises.Data Center compute revenue was up 244% in the fiscal year.Networking revenue was up 133% in the fiscal year.Gaming revenue for fiscal year 2024 was up 15%.The increase reflects higher sell-in to partners following the normalization of channel inventory levels andgrowing demand.Professional Visualization revenue for fiscal year 2024 was up 1%.Automotive revenue for the fiscal year 2024 was up 21%.The increase primarily reflected growth in self-driving platforms.Gross margin increased in fiscal year 2024, primarily driven by Data Center revenue growth and lower net inventory provisions as a percentage of revenue.Operating expenses increased for fiscal year 2024, driven by growth in employees and compensation increases.Fiscal year 2023 also included a $1.4 billionacquisition termination charge related to the proposed Arm transaction.Market Platform HighlightsData Center revenue for fiscal year 2024 was $47.5 billion, up 217% from fiscal year 2023.In Data Center, we launched AI inference platforms that combine ourfull-stack inference software with NVIDIA Ada, NVIDIA Hopper and NVIDIA Grace Hopper processors optimized for generative AI, LLMs and other AI workloads.We introduced NVIDIA DGX Cloud and AI Foundations to help businesses create and operate custom large language models and generative AI models.As AValgorithms move to video transformers, and more cars are equipped with cameras, we expect NVIDIA’s automotive data center processing demand to growsignificantly.We estimate that in fiscal year 2024, approximately 40% of Data Center revenue was for AI inference.", "page_start": 35, "page_end": 36, "section_title": "NVIDIA CORPORATION", "score": 0.61249936}], "ambiguous_nda_01": [{"chunk_id": "6c64d139-fd3d-400d-b64b-3ddf0dea3473", "text": "except the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.The English Courts will have non-exclusive jurisdiction to deal with any dispute whichhas arisen or may arise out of, or in connection with, this Agreement.Signed [by [insert name]] OR [on behalf of][insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________PositionSigned [by [insert name]] OR [on behalf of] [insert name] by its duly authorisedrepresentative]:_____________________________Signature_____________________________Name_____________________________Position", "page_start": 2, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.43471622}, {"chunk_id": "94d954fb-950b-4dcf-8049-c72deb8243c9", "text": "Date: 201[ ]Parties:[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]and[NAME OF INDIVIDUAL] of [address of individual]OR[NAME OF COMPANY], a company registered in [England] under company number[number on Register of Companies] whose registered office is at [address of office onthe Register of Companies]1.Each of the parties to this Agreement intends to disclose information (the ConfidentialInformation) to the other party for the purpose of [insert details e.g.discussing thepossibility of the parties entering into a joint venture] (the Purpose).2.Each party to this Agreement is referred to as ‘the Recipient’ when it receives or usesthe Confidential Information disclosed by the other party.3.The Recipient undertakes not to use the Confidential Information disclosed by the otherparty for any purpose except the Purpose, without first obtaining the written agreementof the other party.4.The Recipient undertakes to keep the Confidential Information disclosed by the otherparty secure and not to disclose it to any third party [except to its employees [andprofessional advisers] who need to know the same for the Purpose, who know theyowe a duty of confidence to the other party and who are bound by obligationsequivalent to those in clause 3 above and this clause 4.5.The undertakings in clauses 3 and 4 above apply to all of the information disclosed byeach of the parties to the other, regardless of the way or form in which it is disclosed orrecorded but they do not apply to:a) any information which is or in future comes into the public domain (unless as a result ofthe breach of this Agreement); orb) any information which is already known to the Recipient and which was not subject toany obligation of confidence before it was disclosed to the Recipient by the other party.6.Nothing in this Agreement will prevent the Recipient from making any disclosure of theConfidential Information required by law or by any competent authority.7.The Recipient will, on request from the other party, return all copies and records of theConfidential Information disclosed by the other party to the Recipient and will not retainany copies or records of the Confidential Information disclosed by the other party.8.Neither this Agreement nor the supply of any information grants the Recipient anylicence, interest or right in respect of any intellectual property rights of the other partyexcept the right to copy the Confidential Information disclosed by the other party solelyfor the Purpose.9.The undertakings in clauses 3 and 4 will continue in force [indefinitely][for [insertnumber] years from the date of this Agreement].10.This Agreement is governed by, and is to be construed in accordance with, English law.", "page_start": 1, "page_end": 2, "section_title": "Mutual Non-Disclosure Agreement", "score": 0.38528544}]}